* 
* ==> Audit <==
* |----------------|---------------------|----------|-------|---------|---------------------|---------------------|
|    Command     |        Args         | Profile  | User  | Version |     Start Time      |      End Time       |
|----------------|---------------------|----------|-------|---------|---------------------|---------------------|
| start          | --driver=docker     | minikube | nouha | v1.26.1 | 28 Aug 22 14:39 +01 |                     |
| start          | --driver=docker     | minikube | nouha | v1.26.1 | 28 Aug 22 14:45 +01 |                     |
| start          | --driver=docker     | minikube | nouha | v1.26.1 | 28 Aug 22 14:46 +01 |                     |
| update-context |                     | minikube | nouha | v1.26.1 | 28 Aug 22 15:45 +01 |                     |
| delete         |                     | minikube | nouha | v1.26.1 | 28 Aug 22 15:47 +01 | 28 Aug 22 15:47 +01 |
| start          |                     | minikube | nouha | v1.26.1 | 28 Aug 22 15:48 +01 | 28 Aug 22 15:51 +01 |
| kubectl        | -- get pods -A      | minikube | nouha | v1.26.1 | 28 Aug 22 17:50 +01 | 28 Aug 22 17:51 +01 |
| kubectl        | -- get pods -A      | minikube | nouha | v1.26.1 | 28 Aug 22 17:51 +01 | 28 Aug 22 17:51 +01 |
| start          |                     | minikube | nouha | v1.26.1 | 28 Aug 22 17:53 +01 | 28 Aug 22 17:54 +01 |
| start          |                     | minikube | nouha | v1.26.1 | 28 Aug 22 20:20 +01 |                     |
| config         | set driver docker   | minikube | nouha | v1.26.1 | 28 Aug 22 20:22 +01 | 28 Aug 22 20:22 +01 |
| delete         |                     | minikube | nouha | v1.26.1 | 28 Aug 22 20:22 +01 | 28 Aug 22 20:23 +01 |
| start          | --driver=docker     | minikube | nouha | v1.26.1 | 28 Aug 22 20:23 +01 | 28 Aug 22 20:34 +01 |
| docker-env     |                     | minikube | nouha | v1.26.1 | 28 Aug 22 20:58 +01 | 28 Aug 22 20:58 +01 |
| docker-env     | minikube docker-env | minikube | nouha | v1.26.1 | 28 Aug 22 20:59 +01 | 28 Aug 22 20:59 +01 |
| docker-env     |                     | minikube | nouha | v1.26.1 | 02 Sep 22 15:43 +01 |                     |
| docker-env     |                     | minikube | nouha | v1.26.1 | 02 Sep 22 15:44 +01 |                     |
| start          |                     | minikube | nouha | v1.26.1 | 02 Sep 22 15:44 +01 |                     |
| start          |                     | minikube | nouha | v1.26.1 | 02 Sep 22 15:51 +01 | 02 Sep 22 16:13 +01 |
| docker-env     |                     | minikube | nouha | v1.26.1 | 02 Sep 22 16:29 +01 |                     |
|----------------|---------------------|----------|-------|---------|---------------------|---------------------|

* 
* ==> Dernier d√©marrage <==
* Log file created at: 2022/09/02 15:51:10
Running on machine: nouha-VirtualBox
Binary: Built with gc go1.18.3 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0902 15:51:10.502290   10427 out.go:296] Setting OutFile to fd 1 ...
I0902 15:51:10.502511   10427 out.go:348] isatty.IsTerminal(1) = true
I0902 15:51:10.502519   10427 out.go:309] Setting ErrFile to fd 2...
I0902 15:51:10.502545   10427 out.go:348] isatty.IsTerminal(2) = true
I0902 15:51:10.503035   10427 root.go:333] Updating PATH: /home/nouha/.minikube/bin
I0902 15:51:10.595039   10427 out.go:303] Setting JSON to false
I0902 15:51:10.621219   10427 start.go:115] hostinfo: {"hostname":"nouha-VirtualBox","uptime":26649,"bootTime":1662103621,"procs":265,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"20.04","kernelVersion":"5.15.0-46-generic","kernelArch":"x86_64","virtualizationSystem":"vbox","virtualizationRole":"guest","hostId":"a7f71466-3d6c-47fe-a658-949d517d98c3"}
I0902 15:51:10.621370   10427 start.go:125] virtualization: vbox guest
I0902 15:51:10.790557   10427 out.go:177] üòÑ  minikube v1.26.1 sur Ubuntu 20.04 (vbox/amd64)
I0902 15:51:10.811837   10427 notify.go:193] Checking for updates...
I0902 15:51:10.814796   10427 config.go:180] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.24.3
I0902 15:51:11.064647   10427 driver.go:365] Setting default libvirt URI to qemu:///system
I0902 15:51:16.615845   10427 docker.go:137] docker version: linux-20.10.17
I0902 15:51:16.616078   10427 cli_runner.go:164] Run: docker system info --format "{{json .}}"
W0902 15:51:22.040435   10427 notify.go:58] Error getting json from minikube version url: error with http GET for endpoint https://storage.googleapis.com/minikube/releases-v2.json: Get "https://storage.googleapis.com/minikube/releases-v2.json": net/http: TLS handshake timeout
I0902 15:51:55.520994   10427 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (38.904843756s)
I0902 15:51:55.613557   10427 info.go:265] docker info: {ID:AGKB:HJL2:66KB:HYQW:SIH7:27EV:LC5A:OZRQ:ILNU:DTID:R7KP:VOGR Containers:2 ContainersRunning:0 ContainersPaused:0 ContainersStopped:2 Images:2 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:true KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:24 OomKillDisable:true NGoroutines:34 SystemTime:2022-09-02 15:51:16.698081134 +0100 +01 LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:0 KernelVersion:5.15.0-46-generic OperatingSystem:Ubuntu 20.04.5 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:2 MemTotal:4636856320 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:nouha-VirtualBox Labels:[] ExperimentalBuild:false ServerVersion:20.10.17 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:9cd3357b7fd7218e4aec3eae239db1f68a5a6ec6 Expected:9cd3357b7fd7218e4aec3eae239db1f68a5a6ec6} RuncCommit:{ID:v1.1.4-0-g5fd4c4d Expected:v1.1.4-0-g5fd4c4d} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=apparmor name=seccomp,profile=default] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Experimental:true Name:app Path:/usr/libexec/docker/cli-plugins/docker-app SchemaVersion:0.1.0 ShortDescription:Docker App Vendor:Docker Inc. Version:v0.9.1-beta3] map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.8.2-docker] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.6.0] map[Name:scan Path:/usr/libexec/docker/cli-plugins/docker-scan SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.17.0]] Warnings:<nil>}}
I0902 15:51:55.613940   10427 docker.go:254] overlay module found
I0902 15:51:55.680753   10427 out.go:177] ‚ú®  Utilisation du pilote docker bas√© sur le profil existant
I0902 15:51:55.685399   10427 start.go:284] selected driver: docker
I0902 15:51:55.685432   10427 start.go:808] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.33@sha256:73b259e144d926189cf169ae5b46bbec4e08e4e2f2bd87296054c3244f70feb8 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.24.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.24.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/nouha:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath:}
I0902 15:51:55.685988   10427 start.go:819] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0902 15:51:55.686404   10427 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0902 15:51:56.122389   10427 info.go:265] docker info: {ID:AGKB:HJL2:66KB:HYQW:SIH7:27EV:LC5A:OZRQ:ILNU:DTID:R7KP:VOGR Containers:2 ContainersRunning:0 ContainersPaused:0 ContainersStopped:2 Images:2 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:true KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:24 OomKillDisable:true NGoroutines:33 SystemTime:2022-09-02 15:51:55.795565761 +0100 +01 LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:0 KernelVersion:5.15.0-46-generic OperatingSystem:Ubuntu 20.04.5 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:2 MemTotal:4636856320 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:nouha-VirtualBox Labels:[] ExperimentalBuild:false ServerVersion:20.10.17 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:9cd3357b7fd7218e4aec3eae239db1f68a5a6ec6 Expected:9cd3357b7fd7218e4aec3eae239db1f68a5a6ec6} RuncCommit:{ID:v1.1.4-0-g5fd4c4d Expected:v1.1.4-0-g5fd4c4d} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=apparmor name=seccomp,profile=default] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Experimental:true Name:app Path:/usr/libexec/docker/cli-plugins/docker-app SchemaVersion:0.1.0 ShortDescription:Docker App Vendor:Docker Inc. Version:v0.9.1-beta3] map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.8.2-docker] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.6.0] map[Name:scan Path:/usr/libexec/docker/cli-plugins/docker-scan SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.17.0]] Warnings:<nil>}}
I0902 15:51:56.632659   10427 cni.go:95] Creating CNI manager for ""
I0902 15:51:56.632731   10427 cni.go:169] CNI unnecessary in this configuration, recommending no CNI
I0902 15:51:56.694401   10427 start_flags.go:310] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.33@sha256:73b259e144d926189cf169ae5b46bbec4e08e4e2f2bd87296054c3244f70feb8 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.24.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.24.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/nouha:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath:}
I0902 15:51:57.051845   10427 out.go:177] üëç  D√©marrage du noeud de plan de contr√¥le minikube dans le cluster minikube
I0902 15:51:57.127570   10427 cache.go:120] Beginning downloading kic base image for docker with docker
I0902 15:51:57.183964   10427 out.go:177] üöú  Extraction de l'image de base...
I0902 15:51:57.208759   10427 image.go:75] Checking for gcr.io/k8s-minikube/kicbase:v0.0.33@sha256:73b259e144d926189cf169ae5b46bbec4e08e4e2f2bd87296054c3244f70feb8 in local docker daemon
I0902 15:51:57.209138   10427 preload.go:132] Checking if preload exists for k8s version v1.24.3 and runtime docker
I0902 15:51:57.209223   10427 preload.go:148] Found local preload: /home/nouha/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.24.3-docker-overlay2-amd64.tar.lz4
I0902 15:51:57.209234   10427 cache.go:57] Caching tarball of preloaded images
I0902 15:51:57.209574   10427 preload.go:174] Found /home/nouha/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.24.3-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0902 15:51:57.209609   10427 cache.go:60] Finished verifying existence of preloaded tar for  v1.24.3 on docker
I0902 15:51:57.209899   10427 profile.go:148] Saving config to /home/nouha/.minikube/profiles/minikube/config.json ...
I0902 15:51:57.340944   10427 image.go:79] Found gcr.io/k8s-minikube/kicbase:v0.0.33@sha256:73b259e144d926189cf169ae5b46bbec4e08e4e2f2bd87296054c3244f70feb8 in local docker daemon, skipping pull
I0902 15:51:57.340973   10427 cache.go:142] gcr.io/k8s-minikube/kicbase:v0.0.33@sha256:73b259e144d926189cf169ae5b46bbec4e08e4e2f2bd87296054c3244f70feb8 exists in daemon, skipping load
I0902 15:51:57.340990   10427 cache.go:208] Successfully downloaded all kic artifacts
I0902 15:51:57.341037   10427 start.go:371] acquiring machines lock for minikube: {Name:mk8cc2abdc6cbf432caf31b051afc3e6e4940923 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0902 15:51:57.403418   10427 start.go:375] acquired machines lock for "minikube" in 62.329624ms
I0902 15:51:57.403460   10427 start.go:95] Skipping create...Using existing machine configuration
I0902 15:51:57.403470   10427 fix.go:55] fixHost starting: 
I0902 15:51:57.404275   10427 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0902 15:51:57.757158   10427 fix.go:103] recreateIfNeeded on minikube: state=Stopped err=<nil>
W0902 15:51:57.757307   10427 fix.go:129] unexpected machine state, will restart: <nil>
I0902 15:51:57.781658   10427 out.go:177] üîÑ  Red√©marrage du docker container existant pour "minikube" ...
I0902 15:51:57.784912   10427 cli_runner.go:164] Run: docker start minikube
I0902 15:52:53.551691   10427 cli_runner.go:217] Completed: docker start minikube: (55.766710211s)
I0902 15:52:53.551874   10427 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0902 15:52:53.748093   10427 kic.go:415] container "minikube" state is running.
I0902 15:52:54.089252   10427 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0902 15:52:54.203623   10427 profile.go:148] Saving config to /home/nouha/.minikube/profiles/minikube/config.json ...
I0902 15:52:54.204026   10427 machine.go:88] provisioning docker machine ...
I0902 15:52:54.204051   10427 ubuntu.go:169] provisioning hostname "minikube"
I0902 15:52:54.204167   10427 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0902 15:52:54.389409   10427 main.go:134] libmachine: Using SSH client type: native
I0902 15:52:54.442476   10427 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x7daec0] 0x7ddf20 <nil>  [] 0s} 127.0.0.1 49157 <nil> <nil>}
I0902 15:52:54.442512   10427 main.go:134] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0902 15:52:54.443509   10427 main.go:134] libmachine: Error dialing TCP: ssh: handshake failed: EOF
I0902 15:52:57.444451   10427 main.go:134] libmachine: Error dialing TCP: ssh: handshake failed: EOF
I0902 15:53:00.445572   10427 main.go:134] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:54870->127.0.0.1:49157: read: connection reset by peer
I0902 15:53:03.446661   10427 main.go:134] libmachine: Error dialing TCP: ssh: handshake failed: EOF
I0902 15:53:06.448187   10427 main.go:134] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:54874->127.0.0.1:49157: read: connection reset by peer
I0902 15:53:09.450178   10427 main.go:134] libmachine: Error dialing TCP: ssh: handshake failed: EOF
I0902 15:53:12.451940   10427 main.go:134] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:54878->127.0.0.1:49157: read: connection reset by peer
I0902 15:53:15.453398   10427 main.go:134] libmachine: Error dialing TCP: ssh: handshake failed: EOF
I0902 15:53:18.454156   10427 main.go:134] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:54884->127.0.0.1:49157: read: connection reset by peer
I0902 15:53:21.455279   10427 main.go:134] libmachine: Error dialing TCP: ssh: handshake failed: EOF
I0902 15:53:24.457101   10427 main.go:134] libmachine: Error dialing TCP: ssh: handshake failed: EOF
I0902 15:53:27.458462   10427 main.go:134] libmachine: Error dialing TCP: ssh: handshake failed: EOF
I0902 15:53:30.459654   10427 main.go:134] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:54892->127.0.0.1:49157: read: connection reset by peer
I0902 15:53:33.461712   10427 main.go:134] libmachine: Error dialing TCP: ssh: handshake failed: EOF
I0902 15:53:36.463886   10427 main.go:134] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:54896->127.0.0.1:49157: read: connection reset by peer
I0902 15:53:39.466666   10427 main.go:134] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:54898->127.0.0.1:49157: read: connection reset by peer
I0902 15:53:42.468344   10427 main.go:134] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:54900->127.0.0.1:49157: read: connection reset by peer
I0902 15:53:53.335080   10427 main.go:134] libmachine: SSH cmd err, output: <nil>: minikube

I0902 15:53:53.335167   10427 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0902 15:53:53.853236   10427 main.go:134] libmachine: Using SSH client type: native
I0902 15:53:53.853547   10427 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x7daec0] 0x7ddf20 <nil>  [] 0s} 127.0.0.1 49157 <nil> <nil>}
I0902 15:53:53.853608   10427 main.go:134] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0902 15:53:54.730089   10427 main.go:134] libmachine: SSH cmd err, output: <nil>: 
I0902 15:53:54.730111   10427 ubuntu.go:175] set auth options {CertDir:/home/nouha/.minikube CaCertPath:/home/nouha/.minikube/certs/ca.pem CaPrivateKeyPath:/home/nouha/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/nouha/.minikube/machines/server.pem ServerKeyPath:/home/nouha/.minikube/machines/server-key.pem ClientKeyPath:/home/nouha/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/nouha/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/nouha/.minikube}
I0902 15:53:54.730146   10427 ubuntu.go:177] setting up certificates
I0902 15:53:54.730158   10427 provision.go:83] configureAuth start
I0902 15:53:54.730238   10427 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0902 15:53:54.828653   10427 provision.go:138] copyHostCerts
I0902 15:53:54.870009   10427 exec_runner.go:144] found /home/nouha/.minikube/ca.pem, removing ...
I0902 15:53:54.870028   10427 exec_runner.go:207] rm: /home/nouha/.minikube/ca.pem
I0902 15:53:54.954203   10427 exec_runner.go:151] cp: /home/nouha/.minikube/certs/ca.pem --> /home/nouha/.minikube/ca.pem (1074 bytes)
I0902 15:53:55.048173   10427 exec_runner.go:144] found /home/nouha/.minikube/cert.pem, removing ...
I0902 15:53:55.048221   10427 exec_runner.go:207] rm: /home/nouha/.minikube/cert.pem
I0902 15:53:55.048685   10427 exec_runner.go:151] cp: /home/nouha/.minikube/certs/cert.pem --> /home/nouha/.minikube/cert.pem (1119 bytes)
I0902 15:53:55.115662   10427 exec_runner.go:144] found /home/nouha/.minikube/key.pem, removing ...
I0902 15:53:55.115691   10427 exec_runner.go:207] rm: /home/nouha/.minikube/key.pem
I0902 15:53:55.115914   10427 exec_runner.go:151] cp: /home/nouha/.minikube/certs/key.pem --> /home/nouha/.minikube/key.pem (1679 bytes)
I0902 15:53:55.191754   10427 provision.go:112] generating server cert: /home/nouha/.minikube/machines/server.pem ca-key=/home/nouha/.minikube/certs/ca.pem private-key=/home/nouha/.minikube/certs/ca-key.pem org=nouha.minikube san=[192.168.49.2 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I0902 15:53:56.818111   10427 provision.go:172] copyRemoteCerts
I0902 15:53:56.818260   10427 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0902 15:53:56.818505   10427 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0902 15:53:56.964383   10427 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49157 SSHKeyPath:/home/nouha/.minikube/machines/minikube/id_rsa Username:docker}
I0902 15:53:57.188716   10427 ssh_runner.go:362] scp /home/nouha/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1074 bytes)
I0902 15:53:58.018963   10427 ssh_runner.go:362] scp /home/nouha/.minikube/machines/server.pem --> /etc/docker/server.pem (1196 bytes)
I0902 15:53:58.204160   10427 ssh_runner.go:362] scp /home/nouha/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0902 15:53:58.299240   10427 provision.go:86] duration metric: configureAuth took 3.569066571s
I0902 15:53:58.299259   10427 ubuntu.go:193] setting minikube options for container-runtime
I0902 15:53:58.299457   10427 config.go:180] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.24.3
I0902 15:53:58.299510   10427 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0902 15:53:58.404804   10427 main.go:134] libmachine: Using SSH client type: native
I0902 15:53:58.405101   10427 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x7daec0] 0x7ddf20 <nil>  [] 0s} 127.0.0.1 49157 <nil> <nil>}
I0902 15:53:58.405131   10427 main.go:134] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0902 15:53:58.884998   10427 main.go:134] libmachine: SSH cmd err, output: <nil>: overlay

I0902 15:53:58.885039   10427 ubuntu.go:71] root file system type: overlay
I0902 15:53:58.887083   10427 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I0902 15:53:58.887352   10427 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0902 15:53:59.019096   10427 main.go:134] libmachine: Using SSH client type: native
I0902 15:53:59.019426   10427 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x7daec0] 0x7ddf20 <nil>  [] 0s} 127.0.0.1 49157 <nil> <nil>}
I0902 15:53:59.019802   10427 main.go:134] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0902 15:53:59.317300   10427 main.go:134] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0902 15:53:59.317494   10427 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0902 15:53:59.483728   10427 main.go:134] libmachine: Using SSH client type: native
I0902 15:53:59.484033   10427 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x7daec0] 0x7ddf20 <nil>  [] 0s} 127.0.0.1 49157 <nil> <nil>}
I0902 15:53:59.484105   10427 main.go:134] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0902 15:53:59.926756   10427 main.go:134] libmachine: SSH cmd err, output: <nil>: 
I0902 15:53:59.926791   10427 machine.go:91] provisioned docker machine in 1m5.722746009s
I0902 15:53:59.926823   10427 start.go:307] post-start starting for "minikube" (driver="docker")
I0902 15:53:59.926867   10427 start.go:335] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0902 15:53:59.927066   10427 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0902 15:53:59.927269   10427 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0902 15:54:00.043425   10427 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49157 SSHKeyPath:/home/nouha/.minikube/machines/minikube/id_rsa Username:docker}
I0902 15:54:01.350995   10427 ssh_runner.go:235] Completed: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs: (1.423654925s)
I0902 15:54:01.418198   10427 ssh_runner.go:195] Run: cat /etc/os-release
I0902 15:54:01.480477   10427 main.go:134] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0902 15:54:01.480521   10427 main.go:134] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0902 15:54:01.480561   10427 main.go:134] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0902 15:54:01.480573   10427 info.go:137] Remote host: Ubuntu 20.04.4 LTS
I0902 15:54:01.480589   10427 filesync.go:126] Scanning /home/nouha/.minikube/addons for local assets ...
I0902 15:54:01.577116   10427 filesync.go:126] Scanning /home/nouha/.minikube/files for local assets ...
I0902 15:54:01.608316   10427 start.go:310] post-start completed in 1.681448905s
I0902 15:54:01.609258   10427 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0902 15:54:01.609492   10427 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0902 15:54:01.741976   10427 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49157 SSHKeyPath:/home/nouha/.minikube/machines/minikube/id_rsa Username:docker}
I0902 15:54:02.310603   10427 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0902 15:54:02.322914   10427 fix.go:57] fixHost completed within 2m4.919433681s
I0902 15:54:02.322934   10427 start.go:82] releasing machines lock for "minikube", held for 2m4.919493323s
I0902 15:54:02.323047   10427 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0902 15:54:02.416601   10427 ssh_runner.go:195] Run: systemctl --version
I0902 15:54:02.416672   10427 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0902 15:54:02.417291   10427 ssh_runner.go:195] Run: curl -sS -m 2 https://k8s.gcr.io/
I0902 15:54:02.417389   10427 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0902 15:54:02.545562   10427 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49157 SSHKeyPath:/home/nouha/.minikube/machines/minikube/id_rsa Username:docker}
I0902 15:54:02.654618   10427 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49157 SSHKeyPath:/home/nouha/.minikube/machines/minikube/id_rsa Username:docker}
I0902 15:54:03.460440   10427 ssh_runner.go:235] Completed: systemctl --version: (1.043784032s)
I0902 15:54:03.460597   10427 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0902 15:54:12.426614   10427 ssh_runner.go:235] Completed: sudo systemctl cat docker.service: (8.965978324s)
I0902 15:54:12.426643   10427 cruntime.go:273] skipping containerd shutdown because we are bound to it
I0902 15:54:12.426747   10427 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0902 15:54:12.427582   10427 ssh_runner.go:235] Completed: curl -sS -m 2 https://k8s.gcr.io/: (10.010259575s)
I0902 15:54:12.910475   10427 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
image-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0902 15:54:12.946106   10427 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0902 15:54:13.940054   10427 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0902 15:54:15.395205   10427 ssh_runner.go:235] Completed: sudo systemctl enable docker.socket: (1.455112665s)
I0902 15:54:15.395270   10427 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0902 15:54:15.901729   10427 ssh_runner.go:195] Run: sudo systemctl restart docker
I0902 15:56:19.068027   10427 ssh_runner.go:235] Completed: sudo systemctl restart docker: (2m3.040250819s)
I0902 15:56:19.068125   10427 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0902 15:56:20.376560   10427 ssh_runner.go:235] Completed: sudo systemctl enable cri-docker.socket: (1.308365622s)
I0902 15:56:20.376659   10427 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0902 15:56:20.719545   10427 ssh_runner.go:195] Run: sudo systemctl start cri-docker.socket
I0902 15:56:20.907732   10427 start.go:450] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0902 15:56:20.907848   10427 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0902 15:56:21.025149   10427 start.go:471] Will wait 60s for crictl version
I0902 15:56:21.025688   10427 ssh_runner.go:195] Run: sudo crictl version
I0902 15:56:40.672679   10427 ssh_runner.go:235] Completed: sudo crictl version: (19.646763443s)
I0902 15:56:40.701270   10427 retry.go:31] will retry after 11.04660288s: Temporary Error: sudo crictl version: Process exited with status 1
stdout:

stderr:
time="2022-09-02T14:56:40Z" level=fatal msg="connect: connect endpoint 'unix:///var/run/cri-dockerd.sock', make sure you are running as root and the endpoint has been started: context deadline exceeded"
I0902 15:56:51.749639   10427 ssh_runner.go:195] Run: sudo crictl version
I0902 15:56:53.851401   10427 ssh_runner.go:235] Completed: sudo crictl version: (2.101716683s)
I0902 15:56:53.851441   10427 retry.go:31] will retry after 21.607636321s: Temporary Error: sudo crictl version: Process exited with status 1
stdout:

stderr:
time="2022-09-02T14:56:53Z" level=fatal msg="connect: connect endpoint 'unix:///var/run/cri-dockerd.sock', make sure you are running as root and the endpoint has been started: context deadline exceeded"
I0902 15:57:15.462758   10427 ssh_runner.go:195] Run: sudo crictl version
I0902 15:57:15.895643   10427 start.go:480] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  20.10.17
RuntimeApiVersion:  1.41.0
I0902 15:57:15.895747   10427 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0902 15:57:48.602840   10427 ssh_runner.go:235] Completed: docker version --format {{.Server.Version}}: (32.707036276s)
I0902 15:57:48.602948   10427 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0902 15:57:50.152925   10427 out.go:204] üê≥  Pr√©paration de Kubernetes v1.24.3 sur Docker 20.10.17...
I0902 15:57:50.244257   10427 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0902 15:57:59.499751   10427 cli_runner.go:217] Completed: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}": (9.255413453s)
I0902 15:57:59.500001   10427 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I0902 15:57:59.668955   10427 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0902 15:58:00.229009   10427 preload.go:132] Checking if preload exists for k8s version v1.24.3 and runtime docker
I0902 15:58:00.229281   10427 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0902 15:58:00.390415   10427 docker.go:611] Got preloaded images: -- stdout --
test-img:1.1.0
test-img:1.0.0
openjdk:18
openjdk:11
k8s.gcr.io/kube-apiserver:v1.24.3
k8s.gcr.io/kube-proxy:v1.24.3
k8s.gcr.io/kube-controller-manager:v1.24.3
k8s.gcr.io/kube-scheduler:v1.24.3
k8s.gcr.io/etcd:3.5.3-0
k8s.gcr.io/pause:3.7
k8s.gcr.io/coredns/coredns:v1.8.6
k8s.gcr.io/pause:3.6
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0902 15:58:00.390443   10427 docker.go:542] Images already preloaded, skipping extraction
I0902 15:58:00.390522   10427 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0902 15:58:00.584390   10427 docker.go:611] Got preloaded images: -- stdout --
test-img:1.1.0
test-img:1.0.0
openjdk:18
openjdk:11
k8s.gcr.io/kube-apiserver:v1.24.3
k8s.gcr.io/kube-controller-manager:v1.24.3
k8s.gcr.io/kube-proxy:v1.24.3
k8s.gcr.io/kube-scheduler:v1.24.3
k8s.gcr.io/etcd:3.5.3-0
k8s.gcr.io/pause:3.7
k8s.gcr.io/coredns/coredns:v1.8.6
k8s.gcr.io/pause:3.6
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0902 15:58:00.948515   10427 cache_images.go:84] Images are preloaded, skipping loading
I0902 15:58:00.948871   10427 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0902 15:58:52.707070   10427 ssh_runner.go:235] Completed: docker info --format {{.CgroupDriver}}: (51.758077008s)
I0902 15:58:53.058893   10427 cni.go:95] Creating CNI manager for ""
I0902 15:58:53.058926   10427 cni.go:169] CNI unnecessary in this configuration, recommending no CNI
I0902 15:58:53.227583   10427 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I0902 15:58:53.227637   10427 kubeadm.go:158] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.24.3 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NoTaintMaster:true NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[]}
I0902 15:58:53.997226   10427 kubeadm.go:162] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: /var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.24.3
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0902 15:58:54.338223   10427 kubeadm.go:961] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.24.3/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime=remote --container-runtime-endpoint=/var/run/cri-dockerd.sock --hostname-override=minikube --image-service-endpoint=/var/run/cri-dockerd.sock --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2 --runtime-request-timeout=15m

[Install]
 config:
{KubernetesVersion:v1.24.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I0902 15:58:54.405361   10427 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.24.3
I0902 15:58:55.201651   10427 binaries.go:44] Found k8s binaries, skipping transfer
I0902 15:58:55.327799   10427 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0902 15:58:55.481109   10427 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (470 bytes)
I0902 15:58:56.582315   10427 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0902 15:58:56.658321   10427 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2031 bytes)
I0902 15:58:56.780123   10427 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0902 15:58:56.787900   10427 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0902 15:58:56.854192   10427 certs.go:54] Setting up /home/nouha/.minikube/profiles/minikube for IP: 192.168.49.2
I0902 15:58:57.141611   10427 certs.go:182] skipping minikubeCA CA generation: /home/nouha/.minikube/ca.key
I0902 15:58:57.202258   10427 certs.go:182] skipping proxyClientCA CA generation: /home/nouha/.minikube/proxy-client-ca.key
I0902 15:58:57.203325   10427 certs.go:298] skipping minikube-user signed cert generation: /home/nouha/.minikube/profiles/minikube/client.key
I0902 15:58:57.275848   10427 certs.go:298] skipping minikube signed cert generation: /home/nouha/.minikube/profiles/minikube/apiserver.key.dd3b5fb2
I0902 15:58:57.307437   10427 certs.go:298] skipping aggregator signed cert generation: /home/nouha/.minikube/profiles/minikube/proxy-client.key
I0902 15:58:57.307785   10427 certs.go:388] found cert: /home/nouha/.minikube/certs/home/nouha/.minikube/certs/ca-key.pem (1679 bytes)
I0902 15:58:57.357207   10427 certs.go:388] found cert: /home/nouha/.minikube/certs/home/nouha/.minikube/certs/ca.pem (1074 bytes)
I0902 15:58:57.357290   10427 certs.go:388] found cert: /home/nouha/.minikube/certs/home/nouha/.minikube/certs/cert.pem (1119 bytes)
I0902 15:58:57.364950   10427 certs.go:388] found cert: /home/nouha/.minikube/certs/home/nouha/.minikube/certs/key.pem (1679 bytes)
I0902 15:58:59.569942   10427 ssh_runner.go:362] scp /home/nouha/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I0902 15:58:59.865545   10427 ssh_runner.go:362] scp /home/nouha/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I0902 15:58:59.963466   10427 ssh_runner.go:362] scp /home/nouha/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0902 15:59:00.205987   10427 ssh_runner.go:362] scp /home/nouha/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0902 15:59:00.285727   10427 ssh_runner.go:362] scp /home/nouha/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0902 15:59:00.352253   10427 ssh_runner.go:362] scp /home/nouha/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0902 15:59:00.467958   10427 ssh_runner.go:362] scp /home/nouha/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0902 15:59:00.650720   10427 ssh_runner.go:362] scp /home/nouha/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0902 15:59:00.765450   10427 ssh_runner.go:362] scp /home/nouha/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0902 15:59:01.000804   10427 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0902 15:59:01.265085   10427 ssh_runner.go:195] Run: openssl version
I0902 15:59:02.158578   10427 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0902 15:59:02.414528   10427 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0902 15:59:02.422706   10427 certs.go:431] hashing: -rw-r--r-- 1 root root 1111 Aug 28 14:50 /usr/share/ca-certificates/minikubeCA.pem
I0902 15:59:02.422816   10427 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0902 15:59:03.109407   10427 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0902 15:59:03.138433   10427 kubeadm.go:395] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.33@sha256:73b259e144d926189cf169ae5b46bbec4e08e4e2f2bd87296054c3244f70feb8 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.24.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.24.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/nouha:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath:}
I0902 15:59:03.138635   10427 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0902 15:59:07.809481   10427 ssh_runner.go:235] Completed: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}: (4.67070801s)
I0902 15:59:07.809603   10427 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0902 15:59:08.114069   10427 kubeadm.go:410] found existing configuration files, will attempt cluster restart
I0902 15:59:08.114120   10427 kubeadm.go:626] restartCluster start
I0902 15:59:08.114216   10427 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0902 15:59:08.139002   10427 kubeadm.go:127] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0902 15:59:08.274479   10427 kubeconfig.go:92] found "minikube" server: "https://192.168.49.2:8443"
I0902 15:59:10.749539   10427 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0902 15:59:10.879235   10427 api_server.go:165] Checking apiserver status ...
I0902 15:59:10.879315   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0902 15:59:11.249102   10427 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0902 15:59:11.449327   10427 api_server.go:165] Checking apiserver status ...
I0902 15:59:11.449401   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0902 15:59:11.473599   10427 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0902 15:59:11.649418   10427 api_server.go:165] Checking apiserver status ...
I0902 15:59:11.649489   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0902 15:59:11.671728   10427 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0902 15:59:11.849588   10427 api_server.go:165] Checking apiserver status ...
I0902 15:59:11.849667   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0902 15:59:11.877499   10427 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0902 15:59:12.049815   10427 api_server.go:165] Checking apiserver status ...
I0902 15:59:12.049896   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0902 15:59:12.082885   10427 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0902 15:59:12.249864   10427 api_server.go:165] Checking apiserver status ...
I0902 15:59:12.249939   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0902 15:59:12.273520   10427 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0902 15:59:12.450090   10427 api_server.go:165] Checking apiserver status ...
I0902 15:59:12.450168   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0902 15:59:12.491494   10427 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0902 15:59:12.650202   10427 api_server.go:165] Checking apiserver status ...
I0902 15:59:12.650273   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0902 15:59:12.673406   10427 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0902 15:59:12.849335   10427 api_server.go:165] Checking apiserver status ...
I0902 15:59:12.849410   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0902 15:59:12.874216   10427 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0902 15:59:13.049525   10427 api_server.go:165] Checking apiserver status ...
I0902 15:59:13.049604   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0902 15:59:13.078028   10427 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0902 15:59:13.249851   10427 api_server.go:165] Checking apiserver status ...
I0902 15:59:13.249924   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0902 15:59:13.273677   10427 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0902 15:59:13.450736   10427 api_server.go:165] Checking apiserver status ...
I0902 15:59:13.450818   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0902 15:59:13.477074   10427 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0902 15:59:13.650065   10427 api_server.go:165] Checking apiserver status ...
I0902 15:59:13.650143   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0902 15:59:13.676963   10427 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0902 15:59:13.850951   10427 api_server.go:165] Checking apiserver status ...
I0902 15:59:13.851024   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0902 15:59:13.873901   10427 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0902 15:59:14.049326   10427 api_server.go:165] Checking apiserver status ...
I0902 15:59:14.049398   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0902 15:59:14.080579   10427 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0902 15:59:14.250390   10427 api_server.go:165] Checking apiserver status ...
I0902 15:59:14.250481   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0902 15:59:14.281696   10427 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0902 15:59:14.281711   10427 api_server.go:165] Checking apiserver status ...
I0902 15:59:14.281772   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0902 15:59:14.310208   10427 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0902 15:59:14.310240   10427 kubeadm.go:601] needs reconfigure: apiserver error: timed out waiting for the condition
I0902 15:59:14.352794   10427 kubeadm.go:1092] stopping kube-system containers ...
I0902 15:59:14.352889   10427 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0902 15:59:14.608267   10427 docker.go:443] Stopping containers: [53572edb6ee7 522c61a969ae d2c2d1eaffce 1b8f30850abc 7bebaa7f7d56 b293a2985b5b c3adefbcd9e8 6dac02c0a6e7 98f8c28a35b6 28ea8642eb2f 505b95de9f6d 15338fd71886 a3eb960c4fbb 8e138f55c0f5 02caf3c5c617]
I0902 15:59:14.608363   10427 ssh_runner.go:195] Run: docker stop 53572edb6ee7 522c61a969ae d2c2d1eaffce 1b8f30850abc 7bebaa7f7d56 b293a2985b5b c3adefbcd9e8 6dac02c0a6e7 98f8c28a35b6 28ea8642eb2f 505b95de9f6d 15338fd71886 a3eb960c4fbb 8e138f55c0f5 02caf3c5c617
I0902 15:59:15.078722   10427 ssh_runner.go:195] Run: sudo systemctl stop kubelet
I0902 15:59:15.650690   10427 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0902 15:59:15.801199   10427 kubeadm.go:155] found existing configuration files:
-rw------- 1 root root 5639 Aug 28 19:31 /etc/kubernetes/admin.conf
-rw------- 1 root root 5656 Aug 28 19:31 /etc/kubernetes/controller-manager.conf
-rw------- 1 root root 1971 Aug 28 19:33 /etc/kubernetes/kubelet.conf
-rw------- 1 root root 5604 Aug 28 19:31 /etc/kubernetes/scheduler.conf

I0902 15:59:15.801297   10427 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0902 15:59:15.904047   10427 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0902 15:59:16.011933   10427 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0902 15:59:16.094611   10427 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 1
stdout:

stderr:
I0902 15:59:16.094680   10427 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0902 15:59:16.252696   10427 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0902 15:59:16.288492   10427 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 1
stdout:

stderr:
I0902 15:59:16.288560   10427 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0902 15:59:16.309649   10427 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0902 15:59:16.385469   10427 kubeadm.go:703] reconfiguring cluster from /var/tmp/minikube/kubeadm.yaml
I0902 15:59:16.385489   10427 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.24.3:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml"
I0902 15:59:33.302372   10427 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.24.3:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml": (16.916838821s)
I0902 15:59:33.302410   10427 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.24.3:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml"
I0902 15:59:39.797245   10427 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.24.3:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml": (6.494800531s)
I0902 15:59:39.797272   10427 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.24.3:$PATH" kubeadm init phase kubelet-start --config /var/tmp/minikube/kubeadm.yaml"
I0902 16:00:28.245865   10427 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.24.3:$PATH" kubeadm init phase kubelet-start --config /var/tmp/minikube/kubeadm.yaml": (48.448523633s)
I0902 16:00:28.245894   10427 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.24.3:$PATH" kubeadm init phase control-plane all --config /var/tmp/minikube/kubeadm.yaml"
I0902 16:00:28.927209   10427 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.24.3:$PATH" kubeadm init phase etcd local --config /var/tmp/minikube/kubeadm.yaml"
I0902 16:00:29.141517   10427 api_server.go:51] waiting for apiserver process to appear ...
I0902 16:00:29.167097   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:00:29.728396   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:00:30.228260   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:00:30.727095   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:00:31.227118   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:00:31.726916   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:00:32.227675   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:00:32.726968   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:00:33.227967   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:00:33.726834   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:00:34.227158   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:00:34.726973   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:00:35.227604   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:00:35.727218   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:00:36.227161   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:00:36.726899   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:00:37.227205   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:00:37.727724   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:00:38.227585   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:00:38.727634   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:00:39.227297   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:00:39.727674   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:00:40.227623   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:00:40.727291   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:00:41.727039   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:00:42.227678   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:00:42.728133   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:00:43.226974   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:00:43.726797   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:00:44.232045   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:00:44.728133   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:00:45.228186   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:00:45.728558   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:00:47.228123   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:00:47.728042   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:00:48.227567   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:00:48.727710   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:00:49.227092   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:00:49.726839   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:00:50.227624   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:00:50.727734   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:00:51.726941   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:00:52.226924   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:00:52.727464   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:00:53.228498   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:00:53.729662   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:00:54.228339   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:00:54.728209   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:00:55.227675   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:00:55.728722   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:00:56.227182   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:00:56.727755   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:00:57.227304   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:00:57.729927   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:00:58.228396   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:00:58.727860   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:00:59.227321   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:00:59.727864   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:01:00.226990   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:01:00.735614   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:01:01.227238   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:01:01.727040   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:01:02.226775   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:01:02.728735   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:01:03.226856   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:01:03.727208   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:01:04.228893   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:01:04.727062   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:01:05.227084   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:01:05.727672   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:01:06.226914   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:01:06.726893   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:01:07.227052   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:01:07.726724   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:01:08.227877   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:01:08.726842   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:01:09.228007   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:01:09.732318   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:01:10.227324   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:01:10.726993   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:01:11.228404   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:01:11.726906   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:01:12.229193   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:01:12.728241   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:01:13.227796   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:01:13.726824   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:01:14.227800   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:01:14.727656   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:01:15.226920   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:01:15.726806   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:01:16.228179   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:01:16.727606   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:01:17.227373   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:01:17.726765   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:01:18.229149   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:01:18.727321   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:01:19.227556   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:01:20.337299   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:01:20.727046   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:01:21.727640   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:01:22.226833   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:01:22.731119   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:01:23.233661   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:01:23.728857   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:01:24.226730   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:01:24.726875   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:01:25.227516   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:01:25.727450   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:01:26.227512   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:01:26.727357   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:01:27.227116   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:01:28.226906   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:01:28.728929   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:01:29.500313   10427 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I0902 16:01:31.648005   10427 logs.go:274] 2 containers: [5b62549995e3 d2c2d1eaffce]
I0902 16:01:31.683173   10427 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I0902 16:01:31.903737   10427 logs.go:274] 2 containers: [7f57b049beb9 28ea8642eb2f]
I0902 16:01:31.903823   10427 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I0902 16:01:31.996861   10427 logs.go:274] 1 containers: [7bebaa7f7d56]
I0902 16:01:31.996958   10427 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I0902 16:01:32.104842   10427 logs.go:274] 2 containers: [eaeba0744d77 505b95de9f6d]
I0902 16:01:32.104932   10427 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I0902 16:01:32.510384   10427 logs.go:274] 1 containers: [b293a2985b5b]
I0902 16:01:32.510466   10427 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kubernetes-dashboard --format={{.ID}}
I0902 16:01:32.657576   10427 logs.go:274] 0 containers: []
W0902 16:01:32.657594   10427 logs.go:276] No container was found matching "kubernetes-dashboard"
I0902 16:01:32.657675   10427 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I0902 16:01:32.903371   10427 logs.go:274] 1 containers: [53572edb6ee7]
I0902 16:01:32.903462   10427 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I0902 16:01:33.009672   10427 logs.go:274] 2 containers: [b4cb9c339fb6 98f8c28a35b6]
I0902 16:01:33.157400   10427 logs.go:123] Gathering logs for dmesg ...
I0902 16:01:33.157435   10427 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I0902 16:01:34.339126   10427 ssh_runner.go:235] Completed: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400": (1.181646294s)
I0902 16:01:35.129581   10427 logs.go:123] Gathering logs for kube-apiserver [5b62549995e3] ...
I0902 16:01:35.129607   10427 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5b62549995e3"
I0902 16:01:35.417429   10427 logs.go:123] Gathering logs for kube-apiserver [d2c2d1eaffce] ...
I0902 16:01:35.417462   10427 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 d2c2d1eaffce"
I0902 16:01:38.103777   10427 ssh_runner.go:235] Completed: /bin/bash -c "docker logs --tail 400 d2c2d1eaffce": (2.686279782s)
I0902 16:01:38.283661   10427 logs.go:123] Gathering logs for etcd [28ea8642eb2f] ...
I0902 16:01:38.283717   10427 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 28ea8642eb2f"
I0902 16:01:43.796983   10427 ssh_runner.go:235] Completed: /bin/bash -c "docker logs --tail 400 28ea8642eb2f": (5.513193119s)
I0902 16:01:44.525862   10427 logs.go:123] Gathering logs for coredns [7bebaa7f7d56] ...
I0902 16:01:44.525885   10427 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7bebaa7f7d56"
I0902 16:01:46.886737   10427 ssh_runner.go:235] Completed: /bin/bash -c "docker logs --tail 400 7bebaa7f7d56": (2.360818675s)
I0902 16:01:47.012508   10427 logs.go:123] Gathering logs for kube-controller-manager [98f8c28a35b6] ...
I0902 16:01:47.012533   10427 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 98f8c28a35b6"
I0902 16:01:49.755749   10427 ssh_runner.go:235] Completed: /bin/bash -c "docker logs --tail 400 98f8c28a35b6": (2.743180995s)
I0902 16:01:49.997329   10427 logs.go:123] Gathering logs for storage-provisioner [53572edb6ee7] ...
I0902 16:01:49.997405   10427 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 53572edb6ee7"
I0902 16:01:54.407010   10427 ssh_runner.go:235] Completed: /bin/bash -c "docker logs --tail 400 53572edb6ee7": (4.409569718s)
I0902 16:01:54.508479   10427 logs.go:123] Gathering logs for describe nodes ...
I0902 16:01:54.508502   10427 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.24.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
I0902 16:02:43.976152   10427 ssh_runner.go:235] Completed: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.24.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": (49.467612668s)
W0902 16:02:43.976195   10427 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.24.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.24.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I0902 16:02:43.976219   10427 logs.go:123] Gathering logs for etcd [7f57b049beb9] ...
I0902 16:02:43.976235   10427 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7f57b049beb9"
I0902 16:02:44.132465   10427 logs.go:123] Gathering logs for kube-scheduler [eaeba0744d77] ...
I0902 16:02:44.132492   10427 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 eaeba0744d77"
I0902 16:02:44.453451   10427 logs.go:123] Gathering logs for kube-proxy [b293a2985b5b] ...
I0902 16:02:44.453477   10427 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 b293a2985b5b"
I0902 16:02:47.346057   10427 ssh_runner.go:235] Completed: /bin/bash -c "docker logs --tail 400 b293a2985b5b": (2.89250948s)
I0902 16:02:47.521026   10427 logs.go:123] Gathering logs for kube-controller-manager [b4cb9c339fb6] ...
I0902 16:02:47.521050   10427 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 b4cb9c339fb6"
I0902 16:02:47.656301   10427 logs.go:123] Gathering logs for Docker ...
I0902 16:02:47.656331   10427 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -n 400"
I0902 16:02:50.456149   10427 ssh_runner.go:235] Completed: /bin/bash -c "sudo journalctl -u docker -n 400": (2.799784112s)
I0902 16:02:50.486017   10427 logs.go:123] Gathering logs for kubelet ...
I0902 16:02:50.486043   10427 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I0902 16:02:50.848397   10427 logs.go:123] Gathering logs for kube-scheduler [505b95de9f6d] ...
I0902 16:02:51.623945   10427 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 505b95de9f6d"
I0902 16:02:54.544655   10427 ssh_runner.go:235] Completed: /bin/bash -c "docker logs --tail 400 505b95de9f6d": (2.920611638s)
I0902 16:02:54.813563   10427 logs.go:123] Gathering logs for container status ...
I0902 16:02:54.813588   10427 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I0902 16:03:29.278772   10427 ssh_runner.go:235] Completed: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a": (34.465150541s)
I0902 16:03:31.785694   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:03:31.814272   10427 api_server.go:71] duration metric: took 3m2.672755865s to wait for apiserver process to appear ...
I0902 16:03:32.035935   10427 api_server.go:87] waiting for apiserver healthz status ...
I0902 16:03:32.710187   10427 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0902 16:03:40.134029   10427 api_server.go:256] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I0902 16:03:40.694953   10427 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0902 16:03:45.882993   10427 api_server.go:256] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I0902 16:03:46.195103   10427 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0902 16:03:47.704587   10427 api_server.go:266] https://192.168.49.2:8443/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0902 16:03:47.800948   10427 api_server.go:102] status: https://192.168.49.2:8443/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0902 16:03:48.195001   10427 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0902 16:03:48.686680   10427 api_server.go:266] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[-]poststarthook/priority-and-fairness-config-producer failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0902 16:03:48.686707   10427 api_server.go:102] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[-]poststarthook/priority-and-fairness-config-producer failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0902 16:03:48.695835   10427 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0902 16:03:49.110944   10427 api_server.go:266] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0902 16:03:49.110971   10427 api_server.go:102] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0902 16:03:49.195209   10427 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0902 16:03:51.005304   10427 api_server.go:266] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0902 16:03:51.005345   10427 api_server.go:102] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0902 16:03:51.195922   10427 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0902 16:03:51.209177   10427 api_server.go:266] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0902 16:03:51.209200   10427 api_server.go:102] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0902 16:03:51.695308   10427 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0902 16:03:51.933891   10427 api_server.go:266] https://192.168.49.2:8443/healthz returned 200:
ok
I0902 16:03:53.033721   10427 api_server.go:140] control plane version: v1.24.3
I0902 16:03:53.033744   10427 api_server.go:130] duration metric: took 20.997795878s to wait for apiserver health ...
I0902 16:03:53.033761   10427 cni.go:95] Creating CNI manager for ""
I0902 16:03:53.033779   10427 cni.go:169] CNI unnecessary in this configuration, recommending no CNI
I0902 16:03:53.142953   10427 system_pods.go:43] waiting for kube-system pods to appear ...
I0902 16:03:55.936108   10427 system_pods.go:59] 7 kube-system pods found
I0902 16:03:55.949165   10427 system_pods.go:61] "coredns-6d4b75cb6d-kqd2l" [a0a0084e-9037-47e7-8e2b-7a14f9dac658] Running
I0902 16:03:55.949202   10427 system_pods.go:61] "etcd-minikube" [5b78c728-f166-40f0-89a9-c27de553f759] Running
I0902 16:03:55.949241   10427 system_pods.go:61] "kube-apiserver-minikube" [48e30474-3b82-416a-90cb-7d80d3e7d2ab] Running
I0902 16:03:55.949276   10427 system_pods.go:61] "kube-controller-manager-minikube" [66231bfa-e30f-4145-85d6-5cbcb3fa6450] Running
I0902 16:03:55.949308   10427 system_pods.go:61] "kube-proxy-rvfpd" [7b0cc691-8210-4e00-a40e-5b85fa5519ae] Running
I0902 16:03:55.949341   10427 system_pods.go:61] "kube-scheduler-minikube" [9cb1245e-bc90-4d4c-ad37-75a265f69596] Running
I0902 16:03:55.949373   10427 system_pods.go:61] "storage-provisioner" [a2b4d595-eb3a-4754-9dfa-0ec9a28076c0] Running
I0902 16:03:55.949391   10427 system_pods.go:74] duration metric: took 2.661843278s to wait for pod list to return data ...
I0902 16:03:55.949413   10427 node_conditions.go:102] verifying NodePressure condition ...
I0902 16:03:56.037272   10427 node_conditions.go:122] node storage ephemeral capacity is 33597096Ki
I0902 16:03:56.037337   10427 node_conditions.go:123] node cpu capacity is 2
I0902 16:03:56.569137   10427 node_conditions.go:105] duration metric: took 619.693637ms to run NodePressure ...
I0902 16:03:57.428924   10427 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.24.3:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml"
I0902 16:06:02.257917   10427 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.24.3:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml": (2m4.828954124s)
I0902 16:06:02.257947   10427 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0902 16:06:03.324595   10427 ssh_runner.go:235] Completed: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj": (1.066614688s)
I0902 16:06:03.474352   10427 ops.go:34] apiserver oom_adj: -16
I0902 16:06:03.474371   10427 kubeadm.go:630] restartCluster took 6m55.360213504s
I0902 16:06:03.474383   10427 kubeadm.go:397] StartCluster complete in 7m0.335960406s
I0902 16:06:04.192707   10427 settings.go:142] acquiring lock: {Name:mk7d090e9afe253c954acfbd9efd03850beb4d12 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0902 16:06:06.858484   10427 settings.go:150] Updating kubeconfig:  /home/nouha/.kube/config
I0902 16:06:11.720695   10427 lock.go:35] WriteFile acquiring /home/nouha/.kube/config: {Name:mkca2f6ec1142b6bcc315c61e169649787f4b0fe Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0902 16:06:16.046504   10427 kapi.go:244] deployment "coredns" in namespace "kube-system" and context "minikube" rescaled to 1
I0902 16:06:16.576648   10427 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.24.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0902 16:06:17.082862   10427 start.go:211] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.24.3 ContainerRuntime:docker ControlPlane:true Worker:true}
I0902 16:06:19.119595   10427 addons.go:412] enableAddons start: toEnable=map[default-storageclass:true storage-provisioner:true], additional=[]
I0902 16:06:19.608910   10427 config.go:180] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.24.3
I0902 16:06:20.135730   10427 out.go:177] üîé  V√©rification des composants Kubernetes...
I0902 16:06:19.940019   10427 addons.go:65] Setting default-storageclass=true in profile "minikube"
I0902 16:06:19.940061   10427 addons.go:65] Setting storage-provisioner=true in profile "minikube"
I0902 16:06:21.885950   10427 addons.go:153] Setting addon storage-provisioner=true in "minikube"
I0902 16:06:21.900996   10427 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
W0902 16:06:21.885984   10427 addons.go:162] addon storage-provisioner should already be in state true
I0902 16:06:23.067680   10427 host.go:66] Checking if "minikube" exists ...
I0902 16:06:23.366069   10427 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I0902 16:06:25.803502   10427 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0902 16:06:25.807116   10427 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0902 16:06:46.169796   10427 host.go:54] host status for "minikube" returned error: state: unknown state "minikube": context deadline exceeded
W0902 16:06:46.367619   10427 addons.go:199] "minikube" is not running, setting storage-provisioner=true and skipping enablement (err=<nil>)
W0902 16:06:48.033067   10427 out.go:239] ‚ùó  L'ex√©cution de "docker container inspect minikube --format={{.State.Status}}" a pris un temps inhabituellement long : 16.951738631s
W0902 16:06:48.033315   10427 out.go:239] üí°  Le red√©marrage du service docker peut am√©liorer les performances.
W0902 16:06:48.033371   10427 host.go:54] host status for "minikube" returned error: state: unknown state "minikube": context deadline exceeded
W0902 16:06:48.033394   10427 addons_storage_classes.go:55] "minikube" is not running, writing default-storageclass=true to disk and skipping enablement
I0902 16:06:48.033404   10427 addons.go:153] Setting addon default-storageclass=true in "minikube"
W0902 16:06:48.033418   10427 addons.go:162] addon default-storageclass should already be in state true
I0902 16:06:48.033457   10427 host.go:66] Checking if "minikube" exists ...
I0902 16:06:48.034847   10427 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0902 16:07:07.041909   10427 host.go:54] host status for "minikube" returned error: state: unknown state "minikube": context deadline exceeded
W0902 16:07:07.041959   10427 addons.go:199] "minikube" is not running, setting default-storageclass=true and skipping enablement (err=<nil>)
I0902 16:07:07.732351   10427 out.go:177] üåü  Modules activ√©s: storage-provisioner, default-storageclass
I0902 16:07:08.416638   10427 addons.go:414] enableAddons completed in 50.022595034s
I0902 16:08:44.005106   10427 ssh_runner.go:235] Completed: sudo systemctl is-active --quiet service kubelet: (2m20.638993651s)
I0902 16:08:44.005306   10427 ssh_runner.go:235] Completed: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.24.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml": (2m27.428625631s)
I0902 16:08:44.357271   10427 start.go:789] CoreDNS already contains "host.minikube.internal" host record, skipping...
I0902 16:08:44.758930   10427 api_server.go:51] waiting for apiserver process to appear ...
I0902 16:08:45.281152   10427 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I0902 16:09:55.279465   10427 ssh_runner.go:235] Completed: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}: (1m9.998258506s)
I0902 16:09:55.279496   10427 logs.go:274] 2 containers: [5b62549995e3 d2c2d1eaffce]
I0902 16:09:55.390415   10427 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I0902 16:09:56.563101   10427 ssh_runner.go:235] Completed: docker ps -a --filter=name=k8s_etcd --format={{.ID}}: (1.172644863s)
I0902 16:09:56.563156   10427 logs.go:274] 2 containers: [7f57b049beb9 28ea8642eb2f]
I0902 16:09:56.563264   10427 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I0902 16:09:56.692722   10427 logs.go:274] 1 containers: [20b1a59d4608]
I0902 16:09:56.692979   10427 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I0902 16:09:56.807566   10427 logs.go:274] 2 containers: [eaeba0744d77 505b95de9f6d]
I0902 16:09:56.807661   10427 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I0902 16:09:57.005096   10427 logs.go:274] 1 containers: [818e5a214c30]
I0902 16:09:57.044839   10427 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kubernetes-dashboard --format={{.ID}}
I0902 16:09:57.375335   10427 logs.go:274] 0 containers: []
W0902 16:09:57.375354   10427 logs.go:276] No container was found matching "kubernetes-dashboard"
I0902 16:09:57.469250   10427 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I0902 16:09:57.567577   10427 logs.go:274] 2 containers: [9375486e7ba7 f87719350d90]
I0902 16:09:57.567665   10427 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I0902 16:09:57.890155   10427 logs.go:274] 2 containers: [6ff11fb7bd95 b4cb9c339fb6]
I0902 16:09:58.068036   10427 logs.go:123] Gathering logs for Docker ...
I0902 16:09:58.068075   10427 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -n 400"
I0902 16:10:09.574145   10427 ssh_runner.go:235] Completed: /bin/bash -c "sudo journalctl -u docker -n 400": (11.506036788s)
I0902 16:10:09.808666   10427 logs.go:123] Gathering logs for kubelet ...
I0902 16:10:09.808691   10427 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I0902 16:10:10.369229   10427 logs.go:123] Gathering logs for dmesg ...
I0902 16:10:10.369255   10427 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I0902 16:10:10.762474   10427 logs.go:123] Gathering logs for etcd [28ea8642eb2f] ...
I0902 16:10:10.827411   10427 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 28ea8642eb2f"
I0902 16:10:14.507923   10427 ssh_runner.go:235] Completed: /bin/bash -c "docker logs --tail 400 28ea8642eb2f": (3.680466302s)
I0902 16:10:14.818931   10427 logs.go:123] Gathering logs for coredns [20b1a59d4608] ...
I0902 16:10:14.818959   10427 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 20b1a59d4608"
I0902 16:10:15.192741   10427 logs.go:123] Gathering logs for kube-proxy [818e5a214c30] ...
I0902 16:10:15.192764   10427 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 818e5a214c30"
I0902 16:10:15.313838   10427 logs.go:123] Gathering logs for kube-controller-manager [6ff11fb7bd95] ...
I0902 16:10:15.313864   10427 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 6ff11fb7bd95"
I0902 16:10:16.744007   10427 ssh_runner.go:235] Completed: /bin/bash -c "docker logs --tail 400 6ff11fb7bd95": (1.430110122s)
I0902 16:10:16.822042   10427 logs.go:123] Gathering logs for storage-provisioner [9375486e7ba7] ...
I0902 16:10:16.822074   10427 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 9375486e7ba7"
I0902 16:10:17.000930   10427 logs.go:123] Gathering logs for kube-controller-manager [b4cb9c339fb6] ...
I0902 16:10:17.000955   10427 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 b4cb9c339fb6"
I0902 16:10:18.196346   10427 ssh_runner.go:235] Completed: /bin/bash -c "docker logs --tail 400 b4cb9c339fb6": (1.195187494s)
I0902 16:10:18.279363   10427 logs.go:123] Gathering logs for describe nodes ...
I0902 16:10:18.279387   10427 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.24.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
I0902 16:10:33.406491   10427 ssh_runner.go:235] Completed: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.24.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": (15.127071788s)
I0902 16:10:33.427178   10427 logs.go:123] Gathering logs for kube-apiserver [5b62549995e3] ...
I0902 16:10:33.427201   10427 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5b62549995e3"
I0902 16:10:35.970755   10427 ssh_runner.go:235] Completed: /bin/bash -c "docker logs --tail 400 5b62549995e3": (2.543519873s)
I0902 16:10:36.331415   10427 logs.go:123] Gathering logs for etcd [7f57b049beb9] ...
I0902 16:10:36.331439   10427 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7f57b049beb9"
I0902 16:10:37.300445   10427 logs.go:123] Gathering logs for kube-scheduler [505b95de9f6d] ...
I0902 16:10:37.300500   10427 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 505b95de9f6d"
I0902 16:10:41.434355   10427 ssh_runner.go:235] Completed: /bin/bash -c "docker logs --tail 400 505b95de9f6d": (4.133818843s)
I0902 16:10:41.682195   10427 logs.go:123] Gathering logs for kube-apiserver [d2c2d1eaffce] ...
I0902 16:10:41.682222   10427 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 d2c2d1eaffce"
I0902 16:10:49.346034   10427 ssh_runner.go:235] Completed: /bin/bash -c "docker logs --tail 400 d2c2d1eaffce": (7.663775141s)
I0902 16:10:49.521319   10427 logs.go:123] Gathering logs for kube-scheduler [eaeba0744d77] ...
I0902 16:10:49.521342   10427 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 eaeba0744d77"
I0902 16:10:53.054152   10427 ssh_runner.go:235] Completed: /bin/bash -c "docker logs --tail 400 eaeba0744d77": (3.532772643s)
I0902 16:10:53.304495   10427 logs.go:123] Gathering logs for storage-provisioner [f87719350d90] ...
I0902 16:10:53.304518   10427 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 f87719350d90"
I0902 16:10:53.589294   10427 logs.go:123] Gathering logs for container status ...
I0902 16:10:53.589324   10427 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I0902 16:11:36.410636   10427 ssh_runner.go:235] Completed: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a": (42.82127597s)
I0902 16:11:39.143675   10427 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0902 16:11:40.059199   10427 api_server.go:71] duration metric: took 5m22.852630265s to wait for apiserver process to appear ...
I0902 16:11:40.146177   10427 api_server.go:87] waiting for apiserver healthz status ...
I0902 16:11:40.810078   10427 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I0902 16:11:40.935496   10427 logs.go:274] 2 containers: [5b62549995e3 d2c2d1eaffce]
I0902 16:11:40.935591   10427 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I0902 16:11:41.042773   10427 logs.go:274] 2 containers: [7f57b049beb9 28ea8642eb2f]
I0902 16:11:41.042873   10427 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I0902 16:11:41.164578   10427 logs.go:274] 1 containers: [20b1a59d4608]
I0902 16:11:41.164658   10427 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I0902 16:11:41.305298   10427 logs.go:274] 2 containers: [eaeba0744d77 505b95de9f6d]
I0902 16:11:41.305392   10427 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I0902 16:11:41.485053   10427 logs.go:274] 1 containers: [818e5a214c30]
I0902 16:11:41.485163   10427 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kubernetes-dashboard --format={{.ID}}
I0902 16:11:41.585866   10427 logs.go:274] 0 containers: []
W0902 16:11:41.585886   10427 logs.go:276] No container was found matching "kubernetes-dashboard"
I0902 16:11:41.585973   10427 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I0902 16:11:41.704326   10427 logs.go:274] 2 containers: [9375486e7ba7 f87719350d90]
I0902 16:11:41.704417   10427 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I0902 16:11:41.807148   10427 logs.go:274] 2 containers: [6ff11fb7bd95 b4cb9c339fb6]
I0902 16:11:41.807186   10427 logs.go:123] Gathering logs for coredns [20b1a59d4608] ...
I0902 16:11:41.807201   10427 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 20b1a59d4608"
I0902 16:11:42.623554   10427 logs.go:123] Gathering logs for Docker ...
I0902 16:11:42.623575   10427 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -n 400"
I0902 16:11:42.784112   10427 logs.go:123] Gathering logs for container status ...
I0902 16:11:42.784141   10427 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I0902 16:11:44.094246   10427 ssh_runner.go:235] Completed: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a": (1.31007312s)
I0902 16:11:44.100806   10427 logs.go:123] Gathering logs for dmesg ...
I0902 16:11:44.100828   10427 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I0902 16:11:44.140687   10427 logs.go:123] Gathering logs for kube-scheduler [505b95de9f6d] ...
I0902 16:11:44.140710   10427 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 505b95de9f6d"
I0902 16:11:44.934598   10427 logs.go:123] Gathering logs for kube-proxy [818e5a214c30] ...
I0902 16:11:44.934623   10427 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 818e5a214c30"
I0902 16:11:45.048371   10427 logs.go:123] Gathering logs for kube-controller-manager [6ff11fb7bd95] ...
I0902 16:11:45.048392   10427 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 6ff11fb7bd95"
I0902 16:11:45.279457   10427 logs.go:123] Gathering logs for kube-controller-manager [b4cb9c339fb6] ...
I0902 16:11:45.279483   10427 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 b4cb9c339fb6"
I0902 16:11:45.505812   10427 logs.go:123] Gathering logs for kube-apiserver [5b62549995e3] ...
I0902 16:11:45.778171   10427 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5b62549995e3"
I0902 16:11:46.281548   10427 logs.go:123] Gathering logs for kube-apiserver [d2c2d1eaffce] ...
I0902 16:11:46.281574   10427 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 d2c2d1eaffce"
I0902 16:11:46.623677   10427 logs.go:123] Gathering logs for etcd [7f57b049beb9] ...
I0902 16:11:46.623701   10427 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7f57b049beb9"
I0902 16:11:47.383765   10427 logs.go:123] Gathering logs for storage-provisioner [f87719350d90] ...
I0902 16:11:47.383788   10427 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 f87719350d90"
I0902 16:11:47.515287   10427 logs.go:123] Gathering logs for storage-provisioner [9375486e7ba7] ...
I0902 16:11:47.515309   10427 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 9375486e7ba7"
I0902 16:11:48.007860   10427 logs.go:123] Gathering logs for kubelet ...
I0902 16:11:48.007881   10427 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I0902 16:11:48.505353   10427 logs.go:123] Gathering logs for describe nodes ...
I0902 16:11:48.505377   10427 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.24.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
I0902 16:11:51.398393   10427 ssh_runner.go:235] Completed: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.24.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": (2.892967708s)
I0902 16:11:51.417987   10427 logs.go:123] Gathering logs for etcd [28ea8642eb2f] ...
I0902 16:11:51.418005   10427 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 28ea8642eb2f"
I0902 16:11:51.912108   10427 logs.go:123] Gathering logs for kube-scheduler [eaeba0744d77] ...
I0902 16:11:51.912132   10427 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 eaeba0744d77"
I0902 16:11:54.829741   10427 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0902 16:12:00.357755   10427 api_server.go:266] https://192.168.49.2:8443/healthz returned 200:
ok
I0902 16:12:02.839141   10427 api_server.go:140] control plane version: v1.24.3
I0902 16:12:02.839164   10427 api_server.go:130] duration metric: took 22.6179965s to wait for apiserver health ...
I0902 16:12:02.996973   10427 system_pods.go:43] waiting for kube-system pods to appear ...
I0902 16:12:02.997176   10427 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I0902 16:12:03.094467   10427 logs.go:274] 2 containers: [5b62549995e3 d2c2d1eaffce]
I0902 16:12:03.094550   10427 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I0902 16:12:03.214923   10427 logs.go:274] 2 containers: [7f57b049beb9 28ea8642eb2f]
I0902 16:12:03.215020   10427 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I0902 16:12:03.321596   10427 logs.go:274] 1 containers: [20b1a59d4608]
I0902 16:12:03.321737   10427 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I0902 16:12:03.436407   10427 logs.go:274] 2 containers: [eaeba0744d77 505b95de9f6d]
I0902 16:12:03.436495   10427 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I0902 16:12:03.587218   10427 logs.go:274] 1 containers: [818e5a214c30]
I0902 16:12:03.587298   10427 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kubernetes-dashboard --format={{.ID}}
I0902 16:12:03.696509   10427 logs.go:274] 0 containers: []
W0902 16:12:03.696531   10427 logs.go:276] No container was found matching "kubernetes-dashboard"
I0902 16:12:03.696600   10427 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I0902 16:12:03.826089   10427 logs.go:274] 2 containers: [9375486e7ba7 f87719350d90]
I0902 16:12:03.826176   10427 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I0902 16:12:03.937244   10427 logs.go:274] 2 containers: [6ff11fb7bd95 b4cb9c339fb6]
I0902 16:12:03.937282   10427 logs.go:123] Gathering logs for dmesg ...
I0902 16:12:03.937297   10427 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I0902 16:12:03.975119   10427 logs.go:123] Gathering logs for kube-apiserver [5b62549995e3] ...
I0902 16:12:03.975141   10427 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5b62549995e3"
I0902 16:12:05.126396   10427 logs.go:123] Gathering logs for kube-scheduler [eaeba0744d77] ...
I0902 16:12:05.126420   10427 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 eaeba0744d77"
I0902 16:12:06.382707   10427 ssh_runner.go:235] Completed: /bin/bash -c "docker logs --tail 400 eaeba0744d77": (1.256250229s)
I0902 16:12:07.019933   10427 logs.go:123] Gathering logs for kube-proxy [818e5a214c30] ...
I0902 16:12:07.019957   10427 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 818e5a214c30"
I0902 16:12:07.136261   10427 logs.go:123] Gathering logs for kubelet ...
I0902 16:12:07.136304   10427 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I0902 16:12:07.540508   10427 logs.go:123] Gathering logs for etcd [28ea8642eb2f] ...
I0902 16:12:07.540544   10427 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 28ea8642eb2f"
I0902 16:12:08.591256   10427 logs.go:123] Gathering logs for kube-controller-manager [b4cb9c339fb6] ...
I0902 16:12:08.591285   10427 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 b4cb9c339fb6"
I0902 16:12:09.082938   10427 logs.go:123] Gathering logs for kube-apiserver [d2c2d1eaffce] ...
I0902 16:12:09.082964   10427 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 d2c2d1eaffce"
I0902 16:12:09.630005   10427 logs.go:123] Gathering logs for coredns [20b1a59d4608] ...
I0902 16:12:09.630032   10427 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 20b1a59d4608"
I0902 16:12:09.788842   10427 logs.go:123] Gathering logs for kube-scheduler [505b95de9f6d] ...
I0902 16:12:09.788863   10427 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 505b95de9f6d"
I0902 16:12:10.268917   10427 logs.go:123] Gathering logs for storage-provisioner [9375486e7ba7] ...
I0902 16:12:10.268942   10427 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 9375486e7ba7"
I0902 16:12:10.373642   10427 logs.go:123] Gathering logs for storage-provisioner [f87719350d90] ...
I0902 16:12:10.373663   10427 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 f87719350d90"
I0902 16:12:10.483046   10427 logs.go:123] Gathering logs for Docker ...
I0902 16:12:10.483069   10427 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -n 400"
I0902 16:12:10.559338   10427 logs.go:123] Gathering logs for container status ...
I0902 16:12:10.724621   10427 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I0902 16:12:10.886715   10427 logs.go:123] Gathering logs for describe nodes ...
I0902 16:12:10.886736   10427 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.24.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
I0902 16:12:13.565146   10427 ssh_runner.go:235] Completed: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.24.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": (2.67837685s)
I0902 16:12:13.583028   10427 logs.go:123] Gathering logs for kube-controller-manager [6ff11fb7bd95] ...
I0902 16:12:13.583050   10427 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 6ff11fb7bd95"
I0902 16:12:13.855139   10427 logs.go:123] Gathering logs for etcd [7f57b049beb9] ...
I0902 16:12:13.855164   10427 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7f57b049beb9"
I0902 16:12:16.484915   10427 ssh_runner.go:235] Completed: /bin/bash -c "docker logs --tail 400 7f57b049beb9": (2.629608541s)
I0902 16:12:22.702783   10427 system_pods.go:59] 7 kube-system pods found
I0902 16:12:22.702876   10427 system_pods.go:61] "coredns-6d4b75cb6d-kqd2l" [a0a0084e-9037-47e7-8e2b-7a14f9dac658] Running
I0902 16:12:22.702932   10427 system_pods.go:61] "etcd-minikube" [5b78c728-f166-40f0-89a9-c27de553f759] Running
I0902 16:12:22.702990   10427 system_pods.go:61] "kube-apiserver-minikube" [48e30474-3b82-416a-90cb-7d80d3e7d2ab] Running
I0902 16:12:22.703051   10427 system_pods.go:61] "kube-controller-manager-minikube" [66231bfa-e30f-4145-85d6-5cbcb3fa6450] Running
I0902 16:12:22.703106   10427 system_pods.go:61] "kube-proxy-rvfpd" [7b0cc691-8210-4e00-a40e-5b85fa5519ae] Running
I0902 16:12:22.703164   10427 system_pods.go:61] "kube-scheduler-minikube" [9cb1245e-bc90-4d4c-ad37-75a265f69596] Running
I0902 16:12:22.703328   10427 system_pods.go:61] "storage-provisioner" [a2b4d595-eb3a-4754-9dfa-0ec9a28076c0] Running
I0902 16:12:22.703363   10427 system_pods.go:74] duration metric: took 19.706355541s to wait for pod list to return data ...
I0902 16:12:23.149617   10427 kubeadm.go:572] duration metric: took 6m5.496898016s to wait for : map[apiserver:true system_pods:true] ...
I0902 16:12:23.469302   10427 node_conditions.go:102] verifying NodePressure condition ...
I0902 16:12:23.483155   10427 node_conditions.go:122] node storage ephemeral capacity is 33597096Ki
I0902 16:12:23.483181   10427 node_conditions.go:123] node cpu capacity is 2
I0902 16:12:23.623086   10427 node_conditions.go:105] duration metric: took 153.753842ms to run NodePressure ...
I0902 16:12:23.663641   10427 start.go:216] waiting for startup goroutines ...
I0902 16:13:13.095056   10427 start.go:506] kubectl: 1.24.4, cluster: 1.24.3 (minor skew: 0)
I0902 16:13:14.329061   10427 out.go:177] üèÑ  Termin√© ! kubectl est maintenant configur√© pour utiliser "minikube" cluster et espace de noms "default" par d√©faut.

* 
* ==> Docker <==
* -- Logs begin at Fri 2022-09-02 14:53:37 UTC, end at Fri 2022-09-02 15:32:31 UTC. --
Sep 02 14:54:59 minikube dockerd[400]: time="2022-09-02T14:54:59.678765270Z" level=info msg="ClientConn switching balancer to \"pick_first\"" module=grpc
Sep 02 14:55:00 minikube dockerd[400]: time="2022-09-02T14:55:00.238831438Z" level=info msg="parsed scheme: \"unix\"" module=grpc
Sep 02 14:55:00 minikube dockerd[400]: time="2022-09-02T14:55:00.239584276Z" level=info msg="scheme \"unix\" not registered, fallback to default scheme" module=grpc
Sep 02 14:55:00 minikube dockerd[400]: time="2022-09-02T14:55:00.240112259Z" level=info msg="ccResolverWrapper: sending update to cc: {[{unix:///run/containerd/containerd.sock  <nil> 0 <nil>}] <nil> <nil>}" module=grpc
Sep 02 14:55:00 minikube dockerd[400]: time="2022-09-02T14:55:00.240708676Z" level=info msg="ClientConn switching balancer to \"pick_first\"" module=grpc
Sep 02 14:55:01 minikube dockerd[400]: time="2022-09-02T14:55:01.776855180Z" level=info msg="[graphdriver] using prior storage driver: overlay2"
Sep 02 14:55:24 minikube dockerd[400]: time="2022-09-02T14:55:24.836199177Z" level=warning msg="Your kernel does not support CPU realtime scheduler"
Sep 02 14:55:24 minikube dockerd[400]: time="2022-09-02T14:55:24.837173918Z" level=warning msg="Your kernel does not support cgroup blkio weight"
Sep 02 14:55:24 minikube dockerd[400]: time="2022-09-02T14:55:24.837293588Z" level=warning msg="Your kernel does not support cgroup blkio weight_device"
Sep 02 14:55:24 minikube dockerd[400]: time="2022-09-02T14:55:24.874783648Z" level=info msg="Loading containers: start."
Sep 02 14:55:47 minikube dockerd[400]: time="2022-09-02T14:55:47.862464287Z" level=info msg="Removing stale sandbox a98a867b0a5087882ef7c9f49cc09a0f22d67e7bb4953d98e0d5ba912ba1ccb4 (02caf3c5c617b5e1b9fe3497b3b21530f3740b2548933dcaabb25e88e175a07c)"
Sep 02 14:55:48 minikube dockerd[400]: time="2022-09-02T14:55:48.378279598Z" level=warning msg="Error (Unable to complete atomic operation, key modified) deleting object [endpoint abfaaecb223a10629a94ddc08730ddfe913bcef9f8703abf1977f670f97b043a d9f201843a50f403997fdeb7617dd06fa994546583361672195d59472d98cbfd], retrying...."
Sep 02 14:55:51 minikube dockerd[400]: time="2022-09-02T14:55:51.032225310Z" level=info msg="Removing stale sandbox dc2cafbd16c9ac705bc39f84b6474cd72948d29e03444acdf68dff2ceb5f1df3 (1b8f30850abcfe17a113cb868f0de451d882b23a03fe60fe044f1d785ab9d455)"
Sep 02 14:55:51 minikube dockerd[400]: time="2022-09-02T14:55:51.534426536Z" level=warning msg="Error (Unable to complete atomic operation, key modified) deleting object [endpoint abfaaecb223a10629a94ddc08730ddfe913bcef9f8703abf1977f670f97b043a 36d9991ff48c8823742acd4c9a77a30dab67e3119f1d567712f81c4fc82cfbac], retrying...."
Sep 02 14:55:53 minikube dockerd[400]: time="2022-09-02T14:55:53.451883334Z" level=info msg="Removing stale sandbox e87fc8b442699f683ed198c7b6e6d5cf66450ef2ebcae5717989e2188b5a6364 (15338fd7188693ab6c88ad3c421bcd6f8b40822a9b7ddb56d7e8ffd00f36bae7)"
Sep 02 14:55:53 minikube dockerd[400]: time="2022-09-02T14:55:53.979151347Z" level=warning msg="Error (Unable to complete atomic operation, key modified) deleting object [endpoint abfaaecb223a10629a94ddc08730ddfe913bcef9f8703abf1977f670f97b043a e96093130d4a3ca2bb5e9bb130c6916f5a535a0574972075609d9442426805de], retrying...."
Sep 02 14:55:55 minikube dockerd[400]: time="2022-09-02T14:55:55.622589770Z" level=info msg="Removing stale sandbox faa922d1a63b84cfc41d984469adb35cb1a93cf506e32f8b45cfa776af87d4e3 (6dac02c0a6e7e18cfb6a6886c75400bb5628e80242ae613d9ece478efabc328a)"
Sep 02 14:55:56 minikube dockerd[400]: time="2022-09-02T14:55:56.882895651Z" level=warning msg="Error (Unable to complete atomic operation, key modified) deleting object [endpoint 76d91ba2716e9daf1e5350bc6f0f57eb66ebc4f9142666dde872de23f76eeb5c 67f524ada4a514bd8ace7f099409af6a5f272bfe1663ae70d7f47ef81c120ef9], retrying...."
Sep 02 14:55:58 minikube dockerd[400]: time="2022-09-02T14:55:58.198385233Z" level=info msg="Removing stale sandbox 00f3d13f3c1e2d0f690538dbaea4c6a09b8332f509d9f7fa7c836dd259277072 (8e138f55c0f545c9547651a4400ef3876aae4aabe0d43749ab6921b26f768e0c)"
Sep 02 14:55:58 minikube dockerd[400]: time="2022-09-02T14:55:58.498662032Z" level=warning msg="Error (Unable to complete atomic operation, key modified) deleting object [endpoint abfaaecb223a10629a94ddc08730ddfe913bcef9f8703abf1977f670f97b043a e9867de5671f2abc598c69097a800b01685582d262734786c351802552be6850], retrying...."
Sep 02 14:55:59 minikube dockerd[400]: time="2022-09-02T14:55:59.376460302Z" level=info msg="Removing stale sandbox 176fa150effc686728a25827b0aa6632fa35f1385d58945001177ae55295c70e (288235fa5bb92cd5dd22835592d96ca15961892f588422cb5d31ad798dd053cc)"
Sep 02 14:56:00 minikube dockerd[400]: time="2022-09-02T14:56:00.049755650Z" level=warning msg="Error (Unable to complete atomic operation, key modified) deleting object [endpoint 76d91ba2716e9daf1e5350bc6f0f57eb66ebc4f9142666dde872de23f76eeb5c cc1df057131fd1c8c10931afdcdea7bbfa0c50b440ab3a9473bfdee8d5ea549c], retrying...."
Sep 02 14:56:01 minikube dockerd[400]: time="2022-09-02T14:56:01.068706897Z" level=info msg="Removing stale sandbox 4c338cb2b0f60700676a8618f339de7538831316259d12245f15d8bd8d12fbff (3d79846fefd149c77832f12657b20c6628301acc0d55e0c5e9d201e3c78af84c)"
Sep 02 14:56:01 minikube dockerd[400]: time="2022-09-02T14:56:01.774606609Z" level=warning msg="Error (Unable to complete atomic operation, key modified) deleting object [endpoint 76d91ba2716e9daf1e5350bc6f0f57eb66ebc4f9142666dde872de23f76eeb5c 4a427a4110549273afb8dbd67b880a27acda4357c8d0ece83cd0671cde3087be], retrying...."
Sep 02 14:56:03 minikube dockerd[400]: time="2022-09-02T14:56:03.040752648Z" level=info msg="Removing stale sandbox 519c08905628a51cd26b8009fcaf96e9a476f1e7355a47871a10c1c85c65db5b (a3eb960c4fbbdf437b6153e54e125175a23303be73088456a2c54700fbca8585)"
Sep 02 14:56:03 minikube dockerd[400]: time="2022-09-02T14:56:03.456298244Z" level=warning msg="Error (Unable to complete atomic operation, key modified) deleting object [endpoint abfaaecb223a10629a94ddc08730ddfe913bcef9f8703abf1977f670f97b043a c84da474716747a6668218741b046c4fdbbb11245350545540b6075d2fa80b2c], retrying...."
Sep 02 14:56:04 minikube dockerd[400]: time="2022-09-02T14:56:04.397069785Z" level=info msg="Removing stale sandbox ae0df724681602d2eb63df2e482bd30cd9e28f517931848967ec458488a91604 (c3adefbcd9e899007da0436567ed70c22a1900d30ee4614eb3bb6886a5fcb7e7)"
Sep 02 14:56:04 minikube dockerd[400]: time="2022-09-02T14:56:04.642723740Z" level=warning msg="Error (Unable to complete atomic operation, key modified) deleting object [endpoint abfaaecb223a10629a94ddc08730ddfe913bcef9f8703abf1977f670f97b043a ef46d9193b92ea0e0ed6fe4358ab7b39f7d47388c42423359a244af4a050493d], retrying...."
Sep 02 14:56:06 minikube dockerd[400]: time="2022-09-02T14:56:06.824331244Z" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
Sep 02 14:56:07 minikube dockerd[400]: time="2022-09-02T14:56:07.424089989Z" level=info msg="Loading containers: done."
Sep 02 14:56:14 minikube dockerd[400]: time="2022-09-02T14:56:14.911359372Z" level=info msg="Docker daemon" commit=a89b842 graphdriver(s)=overlay2 version=20.10.17
Sep 02 14:56:15 minikube dockerd[400]: time="2022-09-02T14:56:15.327165997Z" level=info msg="Daemon has completed initialization"
Sep 02 14:56:19 minikube dockerd[400]: time="2022-09-02T14:56:19.017208301Z" level=info msg="API listen on [::]:2376"
Sep 02 14:56:19 minikube dockerd[400]: time="2022-09-02T14:56:19.028113584Z" level=info msg="API listen on /var/run/docker.sock"
Sep 02 14:56:19 minikube systemd[1]: Started Docker Application Container Engine.
Sep 02 15:03:50 minikube dockerd[400]: time="2022-09-02T15:03:50.092015501Z" level=info msg="ignoring event" container=b4cb9c339fb6aa871eb919683a189e3188b5341f18597e581c40234130077f8a module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 02 15:04:04 minikube dockerd[400]: time="2022-09-02T15:04:04.360292826Z" level=error msg="Failed to compute size of container rootfs 98f8c28a35b6d456e34464a4f8306e973a14a58d23d511e1d69a532f15d21f82: mount does not exist"
Sep 02 15:06:49 minikube dockerd[400]: time="2022-09-02T15:06:49.515254453Z" level=info msg="ignoring event" container=7dd853ec6fc31d64af1f8ebc646293853113c61ab112f3161ef7c759389ac22c module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 02 15:06:50 minikube dockerd[400]: time="2022-09-02T15:06:50.488113495Z" level=info msg="ignoring event" container=c41fb8a3313e4d7f07637045b05142df403d5ac03fe595c97bbbef8d858a16bd module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 02 15:06:50 minikube dockerd[400]: time="2022-09-02T15:06:50.488660966Z" level=info msg="ignoring event" container=5c16746c5ddd52306c102825509652af01d709bb1691c31ad12edf0b2c46f01c module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 02 15:06:50 minikube dockerd[400]: time="2022-09-02T15:06:50.489161195Z" level=info msg="ignoring event" container=ad38faf9e5956f5ddb103c75fde69363b8d8ed49729094b1b3ff83b5d58b130c module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 02 15:07:56 minikube dockerd[400]: time="2022-09-02T15:07:56.306564129Z" level=error msg="Failed to compute size of container rootfs 7bebaa7f7d566cdf240853f68cddeae85b1d4f0a2c3962724c468c6b7dcd51c3: mount does not exist"
Sep 02 15:08:04 minikube dockerd[400]: time="2022-09-02T15:08:04.912035883Z" level=info msg="ignoring event" container=f87719350d909e6c135e91cd89bb88ce6faeff8487d52f8d3725100f6c4a439b module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 02 15:09:59 minikube dockerd[400]: time="2022-09-02T15:09:59.294672650Z" level=info msg="ignoring event" container=a3362615e0b8df1a40063c00ea2a5c993184d3849020605e49a419640b966da6 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 02 15:10:46 minikube dockerd[400]: time="2022-09-02T15:10:46.157407921Z" level=info msg="ignoring event" container=f7e33258825a4d24e6e99392ed9ce63c5c576741c283e429dd1d154bd5186193 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 02 15:11:23 minikube dockerd[400]: time="2022-09-02T15:11:23.549299787Z" level=info msg="ignoring event" container=ed8fd25d43416275fc3b0462c7f32b5b17c2142ad064a3c9a5bc5dec3d510b27 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 02 15:12:31 minikube dockerd[400]: time="2022-09-02T15:12:31.080397988Z" level=info msg="ignoring event" container=31d3fbc82cd02c2f98388a21cb1cfead98002acf2189555a8f18ed859dd97ae4 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 02 15:13:44 minikube dockerd[400]: time="2022-09-02T15:13:44.481188597Z" level=info msg="ignoring event" container=9375486e7ba788af8cab32f19130627b3eca640712de36f66e77f9b79d3dcb6b module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 02 15:13:52 minikube dockerd[400]: time="2022-09-02T15:13:52.359447238Z" level=error msg="Failed to compute size of container rootfs f87719350d909e6c135e91cd89bb88ce6faeff8487d52f8d3725100f6c4a439b: mount does not exist"
Sep 02 15:14:10 minikube dockerd[400]: time="2022-09-02T15:14:10.078262571Z" level=info msg="ignoring event" container=b8a4ffeabc0990a1b828d8488e0bbffa89ba6499c7cbe1b0fca2d3e317beb73b module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 02 15:14:33 minikube dockerd[400]: time="2022-09-02T15:14:33.186675079Z" level=info msg="ignoring event" container=7f99322db9eda60c8564f55d7250e8246ef87da6020ab4dc488cfa374a9d6e3c module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 02 15:16:12 minikube dockerd[400]: time="2022-09-02T15:16:12.516008548Z" level=info msg="ignoring event" container=a2bc646ec5f2c3eda9dba1491960feabb174581c735cf97b3db5e74ccc334f01 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 02 15:17:12 minikube dockerd[400]: time="2022-09-02T15:17:12.848768179Z" level=info msg="ignoring event" container=9fc065b2c3f9f0a8d91035708c105ce1cc1cbe635c0373d84a87425b0c52337d module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 02 15:17:46 minikube dockerd[400]: time="2022-09-02T15:17:46.449528379Z" level=info msg="ignoring event" container=11a3140432fb489df4151034ea4bfc04e53ab79b5259bfb19c2601dc8dc4ece5 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 02 15:18:25 minikube dockerd[400]: time="2022-09-02T15:18:25.783187701Z" level=info msg="ignoring event" container=8ce7ad06aa3f9f53a513f8d09da6a80378f7120b299848e84e1352382deafd17 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 02 15:20:16 minikube dockerd[400]: time="2022-09-02T15:20:16.338322325Z" level=info msg="ignoring event" container=d269bd020413eeba44e14d066cb282c75d2efbab417e09685c0fe040933d66f6 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 02 15:22:59 minikube dockerd[400]: time="2022-09-02T15:22:59.859323733Z" level=info msg="ignoring event" container=7203dfe31f3a769b234aa110c77bb64d762d910137967672968d2c19969dc3d1 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 02 15:24:11 minikube dockerd[400]: time="2022-09-02T15:24:11.522193568Z" level=info msg="ignoring event" container=274dd494f6d5da8f5b13a01025ab6290d6d129538537e3dd596d428fad08af91 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 02 15:28:31 minikube dockerd[400]: time="2022-09-02T15:28:31.792151762Z" level=info msg="ignoring event" container=fecc50ef0f898a1047e3194923a9f903aebca9adfe1cc921f4e66b77d8464317 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 02 15:31:04 minikube dockerd[400]: time="2022-09-02T15:31:04.364512914Z" level=info msg="ignoring event" container=e661566b00ce8082c68e7f4be8bb1139624e88e6c14ee8efc99c267607e54d12 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"

* 
* ==> container status <==
* CONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID
e661566b00ce8       48c29a351b61e       3 minutes ago       Exited              test-img                  112                 58217005fe6ef
fecc50ef0f898       dc674d685bf04       4 minutes ago       Exited              test-img                  130                 a38e0a32bca7a
263e629bbbe7e       6e38f40d628db       18 minutes ago      Running             storage-provisioner       51                  cc276efa66bf0
20b1a59d4608b       a4ca41631cc7a       25 minutes ago      Running             coredns                   1                   2abdceb00889a
818e5a214c300       2ae1ba6417cbc       28 minutes ago      Running             kube-proxy                1                   692db4697097b
6ff11fb7bd950       586c112956dfc       28 minutes ago      Running             kube-controller-manager   3                   c9fdc417184fb
5b62549995e3a       d521dd763e2e3       31 minutes ago      Running             kube-apiserver            3                   dbe7848b6fe69
eaeba0744d771       3a5aa3a515f5d       31 minutes ago      Running             kube-scheduler            1                   55dbf087f8b3b
b4cb9c339fb6a       586c112956dfc       31 minutes ago      Exited              kube-controller-manager   2                   c9fdc417184fb
7f57b049beb94       aebe758cef4cd       31 minutes ago      Running             etcd                      1                   dcc348d023421
d2c2d1eaffce7       d521dd763e2e3       22 hours ago        Exited              kube-apiserver            2                   15338fd718869
28ea8642eb2f1       aebe758cef4cd       4 days ago          Exited              etcd                      0                   02caf3c5c617b
505b95de9f6d9       3a5aa3a515f5d       4 days ago          Exited              kube-scheduler            0                   8e138f55c0f54

* 
* ==> coredns [20b1a59d4608] <==
* [INFO] plugin/ready: Still waiting on: "kubernetes"
.:53
[INFO] plugin/reload: Running configuration MD5 = cec3c60eb1cc4909fd4579a8d79ea031
CoreDNS-1.8.6
linux/amd64, go1.17.1, 13a9191
[ERROR] plugin/errors: 2 6246077906464000144.6818614929184553504. HINFO: read udp 172.17.0.3:60407->192.168.49.1:53: i/o timeout
[ERROR] plugin/errors: 2 6246077906464000144.6818614929184553504. HINFO: read udp 172.17.0.3:35251->192.168.49.1:53: i/o timeout
[ERROR] plugin/errors: 2 6246077906464000144.6818614929184553504. HINFO: read udp 172.17.0.3:32885->192.168.49.1:53: i/o timeout
[ERROR] plugin/errors: 2 6246077906464000144.6818614929184553504. HINFO: read udp 172.17.0.3:40703->192.168.49.1:53: i/o timeout
[ERROR] plugin/errors: 2 6246077906464000144.6818614929184553504. HINFO: read udp 172.17.0.3:48450->192.168.49.1:53: i/o timeout
[ERROR] plugin/errors: 2 6246077906464000144.6818614929184553504. HINFO: read udp 172.17.0.3:44447->192.168.49.1:53: i/o timeout
[ERROR] plugin/errors: 2 6246077906464000144.6818614929184553504. HINFO: read udp 172.17.0.3:55594->192.168.49.1:53: i/o timeout
[ERROR] plugin/errors: 2 6246077906464000144.6818614929184553504. HINFO: read udp 172.17.0.3:35640->192.168.49.1:53: i/o timeout
[ERROR] plugin/errors: 2 6246077906464000144.6818614929184553504. HINFO: read udp 172.17.0.3:53427->192.168.49.1:53: i/o timeout
[ERROR] plugin/errors: 2 6246077906464000144.6818614929184553504. HINFO: read udp 172.17.0.3:59679->192.168.49.1:53: i/o timeout

* 
* ==> describe nodes <==
* Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=62e108c3dfdec8029a890ad6d8ef96b6461426dc
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2022_08_28T20_33_05_0700
                    minikube.k8s.io/version=v1.26.1
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Sun, 28 Aug 2022 19:32:51 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Fri, 02 Sep 2022 15:33:09 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Fri, 02 Sep 2022 15:29:20 +0000   Mon, 29 Aug 2022 08:13:45 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Fri, 02 Sep 2022 15:29:20 +0000   Mon, 29 Aug 2022 08:13:45 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Fri, 02 Sep 2022 15:29:20 +0000   Mon, 29 Aug 2022 08:13:45 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Fri, 02 Sep 2022 15:29:20 +0000   Mon, 29 Aug 2022 08:13:45 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                2
  ephemeral-storage:  33597096Ki
  hugepages-2Mi:      0
  memory:             4528180Ki
  pods:               110
Allocatable:
  cpu:                2
  ephemeral-storage:  33597096Ki
  hugepages-2Mi:      0
  memory:             4528180Ki
  pods:               110
System Info:
  Machine ID:                 4c192b04687c403f8fbb9bc7975b21b3
  System UUID:                6f63f63d-0982-4927-b149-62ae2ce180dc
  Boot ID:                    212c098e-2a32-41dc-91d5-1fdc67f236c2
  Kernel Version:             5.15.0-46-generic
  OS Image:                   Ubuntu 20.04.4 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://20.10.17
  Kubelet Version:            v1.24.3
  Kube-Proxy Version:         v1.24.3
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (9 in total)
  Namespace                   Name                                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                ------------  ----------  ---------------  -------------  ---
  default                     test-dep-99b7588fb-zd7g4            0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         4d19h
  default                     test-dep1-56bdf49f98-748td          0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         4d18h
  kube-system                 coredns-6d4b75cb6d-kqd2l            100m (5%!)(MISSING)     0 (0%!)(MISSING)      70Mi (1%!)(MISSING)        170Mi (3%!)(MISSING)     4d19h
  kube-system                 etcd-minikube                       100m (5%!)(MISSING)     0 (0%!)(MISSING)      100Mi (2%!)(MISSING)       0 (0%!)(MISSING)         4d20h
  kube-system                 kube-apiserver-minikube             250m (12%!)(MISSING)    0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         4d20h
  kube-system                 kube-controller-manager-minikube    200m (10%!)(MISSING)    0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         4d20h
  kube-system                 kube-proxy-rvfpd                    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         4d19h
  kube-system                 kube-scheduler-minikube             100m (5%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         4d20h
  kube-system                 storage-provisioner                 0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         4d19h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (37%!)(MISSING)  0 (0%!)(MISSING)
  memory             170Mi (3%!)(MISSING)  170Mi (3%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:
  Type    Reason                   Age                 From             Message
  ----    ------                   ----                ----             -------
  Normal  Starting                 25m                 kube-proxy       
  Normal  NodeAllocatableEnforced  32m                 kubelet          Updated Node Allocatable limit across pods
  Normal  NodeHasNoDiskPressure    32m (x8 over 32m)   kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     32m (x7 over 32m)   kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeHasSufficientMemory  29m (x58 over 32m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  RegisteredNode           26m                 node-controller  Node minikube event: Registered Node minikube in Controller

* 
* ==> dmesg <==
* [Sep 2 07:27] unchecked MSR access error: WRMSR to 0x3a (tried to write 0x0000000000000001) at rIP: 0xffffffffbae93f04 (native_write_msr+0x4/0x30)
[  +0.000014] Call Trace:
[  +0.000004]  <TASK>
[  +0.000004]  ? init_ia32_feat_ctl+0xc2/0x440
[  +0.000013]  init_intel+0xf2/0x510
[  +0.000012]  identify_cpu+0x2dd/0x6f0
[  +0.000010]  identify_boot_cpu+0x10/0x9e
[  +0.000011]  check_bugs+0x2a/0xfaa
[  +0.000010]  start_kernel+0x80c/0x845
[  +0.000012]  x86_64_start_reservations+0x24/0x2a
[  +0.000009]  x86_64_start_kernel+0xe4/0xef
[  +0.000009]  secondary_startup_64_no_verify+0xc2/0xcb
[  +0.000015]  </TASK>
[  -0.220489] Call Trace:
[  +0.000000]  <TASK>
[  +0.000000]  ? init_ia32_feat_ctl+0xc0/0x440
[  +0.000000]  init_intel+0xf2/0x510
[  +0.000000]  identify_cpu+0x2dd/0x6f0
[  +0.000000]  identify_secondary_cpu+0x18/0x90
[  +0.000000]  smp_store_cpu_info+0x5a/0x80
[  +0.000000]  start_secondary+0x53/0x180
[  +0.000000]  secondary_startup_64_no_verify+0xc2/0xcb
[  +0.000000]  </TASK>
[  +0.447388] acpi PNP0A03:00: fail to add MMCONFIG information, can't access extended PCI configuration space under this bridge.
[  +0.373759] device-mapper: core: CONFIG_IMA_DISABLE_HTABLE is disabled. Duplicate IMA measurements will not be recorded in the IMA log.
[  +0.000538] platform eisa.0: EISA: Cannot allocate resource for mainboard
[  +0.000005] platform eisa.0: Cannot allocate resource for EISA slot 1
[  +0.000004] platform eisa.0: Cannot allocate resource for EISA slot 2
[  +0.000005] platform eisa.0: Cannot allocate resource for EISA slot 3
[  +0.000004] platform eisa.0: Cannot allocate resource for EISA slot 4
[  +0.000005] platform eisa.0: Cannot allocate resource for EISA slot 5
[  +0.000005] platform eisa.0: Cannot allocate resource for EISA slot 6
[  +0.000004] platform eisa.0: Cannot allocate resource for EISA slot 7
[  +0.000005] platform eisa.0: Cannot allocate resource for EISA slot 8
[ +37.038860] systemd-journald[234]: File /var/log/journal/a7f714663d6c47fea658949d517d98c3/system.journal corrupted or uncleanly shut down, renaming and replacing.
[  +7.204502] [drm:vmw_host_printf [vmwgfx]] *ERROR* Failed to send host log message.
[Sep 2 07:28] kauditd_printk_skb: 30 callbacks suppressed
[ +33.824673] systemd-journald[234]: Failed to read journal file /var/log/journal/a7f714663d6c47fea658949d517d98c3/user-1000.journal for rotation, trying to move it out of the way: Device or resource busy
[Sep 2 07:33] vboxsf: Old binary mount data not supported, remove obsolete mount.vboxsf and/or update your VBoxService.
[  +1.017517] vboxsf: Old binary mount data not supported, remove obsolete mount.vboxsf and/or update your VBoxService.
[Sep 2 10:02] sched: RT throttling activated
[Sep 2 10:06] vboxsf: Old binary mount data not supported, remove obsolete mount.vboxsf and/or update your VBoxService.
[Sep 2 10:07] vboxsf: Old binary mount data not supported, remove obsolete mount.vboxsf and/or update your VBoxService.
[Sep 2 10:12] e1000 0000:00:03.0 enp0s3: Reset adapter
[Sep 2 12:53] usb 1-1: can't set config #1, error -32

* 
* ==> etcd [28ea8642eb2f] <==
* {"level":"warn","ts":"2022-09-01T18:35:02.735Z","caller":"etcdserver/util.go:123","msg":"failed to apply request","took":"65.999¬µs","request":"header:<ID:8128015351599247050 > lease_revoke:<id:70cc82e5f0fe886b>","response":"size:30","error":"lease not found"}
{"level":"warn","ts":"2022-09-01T18:35:02.735Z","caller":"etcdserver/util.go:123","msg":"failed to apply request","took":"123.662¬µs","request":"header:<ID:8128015351599247052 > lease_revoke:<id:70cc82e5f0fe852b>","response":"size:30","error":"lease not found"}
{"level":"warn","ts":"2022-09-01T18:35:02.736Z","caller":"etcdserver/util.go:123","msg":"failed to apply request","took":"66.242¬µs","request":"header:<ID:8128015351599247051 > lease_revoke:<id:70cc82e5f0feae5d>","response":"size:30","error":"lease not found"}
{"level":"warn","ts":"2022-09-01T18:35:02.736Z","caller":"etcdserver/util.go:123","msg":"failed to apply request","took":"13.856¬µs","request":"header:<ID:8128015351599247053 > lease_revoke:<id:70cc82e5f0fe89d9>","response":"size:30","error":"lease not found"}
{"level":"warn","ts":"2022-09-01T18:35:02.736Z","caller":"etcdserver/util.go:123","msg":"failed to apply request","took":"66.225¬µs","request":"header:<ID:8128015351599247054 > lease_revoke:<id:70cc82e5f0fe886b>","response":"size:30","error":"lease not found"}
{"level":"warn","ts":"2022-09-01T18:35:02.736Z","caller":"etcdserver/util.go:123","msg":"failed to apply request","took":"114.303¬µs","request":"header:<ID:8128015351599247055 > lease_revoke:<id:70cc82e5f0feae5d>","response":"size:30","error":"lease not found"}
{"level":"warn","ts":"2022-09-01T18:35:02.736Z","caller":"etcdserver/util.go:123","msg":"failed to apply request","took":"20.449¬µs","request":"header:<ID:8128015351599247056 > lease_revoke:<id:70cc82e5f0fe852b>","response":"size:30","error":"lease not found"}
{"level":"warn","ts":"2022-09-01T18:35:02.736Z","caller":"etcdserver/util.go:123","msg":"failed to apply request","took":"91.234¬µs","request":"header:<ID:8128015351599247057 > lease_revoke:<id:70cc82e5f0fe89d9>","response":"size:30","error":"lease not found"}
{"level":"warn","ts":"2022-09-01T18:35:02.737Z","caller":"etcdserver/util.go:123","msg":"failed to apply request","took":"14.833¬µs","request":"header:<ID:8128015351599247058 > lease_revoke:<id:70cc82e5f0fe886b>","response":"size:30","error":"lease not found"}
{"level":"warn","ts":"2022-09-01T18:35:02.737Z","caller":"etcdserver/util.go:123","msg":"failed to apply request","took":"16.757¬µs","request":"header:<ID:8128015351599247059 > lease_revoke:<id:70cc82e5f0feae5d>","response":"size:30","error":"lease not found"}
{"level":"warn","ts":"2022-09-01T18:35:02.737Z","caller":"etcdserver/util.go:123","msg":"failed to apply request","took":"20.406¬µs","request":"header:<ID:8128015351599247060 > lease_revoke:<id:70cc82e5f0fe852b>","response":"size:30","error":"lease not found"}
{"level":"warn","ts":"2022-09-01T18:35:02.737Z","caller":"etcdserver/util.go:123","msg":"failed to apply request","took":"14.878¬µs","request":"header:<ID:8128015351599247061 > lease_revoke:<id:70cc82e5f0fe89d9>","response":"size:30","error":"lease not found"}
{"level":"warn","ts":"2022-09-01T18:35:02.737Z","caller":"etcdserver/util.go:123","msg":"failed to apply request","took":"14.794¬µs","request":"header:<ID:8128015351599247062 > lease_revoke:<id:70cc82e5f0fe886b>","response":"size:30","error":"lease not found"}
{"level":"warn","ts":"2022-09-01T18:35:02.737Z","caller":"etcdserver/util.go:123","msg":"failed to apply request","took":"13.927¬µs","request":"header:<ID:8128015351599247063 > lease_revoke:<id:70cc82e5f0feae5d>","response":"size:30","error":"lease not found"}
{"level":"warn","ts":"2022-09-01T18:35:02.738Z","caller":"etcdserver/util.go:123","msg":"failed to apply request","took":"13.462¬µs","request":"header:<ID:8128015351599247064 > lease_revoke:<id:70cc82e5f0fe852b>","response":"size:30","error":"lease not found"}
{"level":"warn","ts":"2022-09-01T18:35:02.738Z","caller":"etcdserver/util.go:123","msg":"failed to apply request","took":"14.024¬µs","request":"header:<ID:8128015351599247065 > lease_revoke:<id:70cc82e5f0fe89d9>","response":"size:30","error":"lease not found"}
{"level":"warn","ts":"2022-09-01T18:35:02.738Z","caller":"etcdserver/util.go:123","msg":"failed to apply request","took":"15.469¬µs","request":"header:<ID:8128015351599247066 > lease_revoke:<id:70cc82e5f0fe886b>","response":"size:30","error":"lease not found"}
{"level":"warn","ts":"2022-09-01T18:35:02.738Z","caller":"etcdserver/util.go:123","msg":"failed to apply request","took":"16.041¬µs","request":"header:<ID:8128015351599247067 > lease_revoke:<id:70cc82e5f0feae5d>","response":"size:30","error":"lease not found"}
{"level":"warn","ts":"2022-09-01T18:35:02.738Z","caller":"etcdserver/util.go:123","msg":"failed to apply request","took":"90.845¬µs","request":"header:<ID:8128015351599247068 > lease_revoke:<id:70cc82e5f0fe852b>","response":"size:30","error":"lease not found"}
{"level":"warn","ts":"2022-09-01T18:35:02.738Z","caller":"etcdserver/util.go:123","msg":"failed to apply request","took":"13.793¬µs","request":"header:<ID:8128015351599247069 > lease_revoke:<id:70cc82e5f0fe89d9>","response":"size:30","error":"lease not found"}
{"level":"warn","ts":"2022-09-01T18:35:02.738Z","caller":"etcdserver/util.go:123","msg":"failed to apply request","took":"14.242¬µs","request":"header:<ID:8128015351599247070 > lease_revoke:<id:70cc82e5f0fe886b>","response":"size:30","error":"lease not found"}
{"level":"warn","ts":"2022-09-01T18:35:02.739Z","caller":"etcdserver/util.go:123","msg":"failed to apply request","took":"114.808¬µs","request":"header:<ID:8128015351599247071 > lease_revoke:<id:70cc82e5f0feae5d>","response":"size:30","error":"lease not found"}
{"level":"warn","ts":"2022-09-01T18:35:02.739Z","caller":"etcdserver/server.go:1138","msg":"failed to revoke lease","lease-id":"70cc82e5f0fe852b","error":"lease not found"}
{"level":"warn","ts":"2022-09-01T18:35:02.740Z","caller":"etcdserver/server.go:1138","msg":"failed to revoke lease","lease-id":"70cc82e5f0fe89d9","error":"lease not found"}
{"level":"warn","ts":"2022-09-01T18:35:02.740Z","caller":"etcdserver/server.go:1138","msg":"failed to revoke lease","lease-id":"70cc82e5f0fe8b1a","error":"lease not found"}
{"level":"warn","ts":"2022-09-01T18:35:02.740Z","caller":"etcdserver/server.go:1138","msg":"failed to revoke lease","lease-id":"70cc82e5f0fe886b","error":"lease not found"}
{"level":"info","ts":"2022-09-01T18:35:06.721Z","caller":"traceutil/trace.go:171","msg":"trace[2053444804] linearizableReadLoop","detail":"{readStateIndex:33890; appliedIndex:33889; }","duration":"6m39.463732448s","start":"2022-09-01T18:28:20.653Z","end":"2022-09-01T18:35:00.117Z","steps":["trace[2053444804] 'read index received'  (duration: 861.451¬µs)","trace[2053444804] 'applied index is now lower than readState.Index'  (duration: 6m39.186123039s)"],"step_count":2}
{"level":"warn","ts":"2022-09-01T18:35:14.293Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"6m40.862706769s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/\" range_end:\"/registry/pods0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"warn","ts":"2022-09-01T18:35:14.294Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"2m45.864301605s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/ingress/\" range_end:\"/registry/ingress0\" count_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-09-01T18:35:14.383Z","caller":"traceutil/trace.go:171","msg":"trace[1557664437] range","detail":"{range_begin:/registry/pods/; range_end:/registry/pods0; response_count:0; response_revision:25438; }","duration":"6m43.063963898s","start":"2022-09-01T18:28:31.230Z","end":"2022-09-01T18:35:14.294Z","steps":["trace[1557664437] 'agreement among raft nodes before linearized reading'  (duration: 6m38.890289695s)","trace[1557664437] 'get authentication metadata'  (duration: 1.625107175s)","trace[1557664437] 'count revisions from in-memory index tree'  (duration: 347.278956ms)"],"step_count":3}
{"level":"info","ts":"2022-09-01T18:35:14.383Z","caller":"traceutil/trace.go:171","msg":"trace[218520332] range","detail":"{range_begin:/registry/ingress/; range_end:/registry/ingress0; response_count:0; response_revision:25438; }","duration":"2m48.065909895s","start":"2022-09-01T18:32:26.229Z","end":"2022-09-01T18:35:14.294Z","steps":["trace[218520332] 'agreement among raft nodes before linearized reading'  (duration: 2m43.891772986s)","trace[218520332] 'get authentication metadata'  (duration: 1.625259021s)","trace[218520332] 'count revisions from in-memory index tree'  (duration: 347.276762ms)"],"step_count":3}
{"level":"warn","ts":"2022-09-01T18:35:17.989Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-09-01T18:32:25.312Z","time spent":"2m51.21302876s","remote":"127.0.0.1:57480","response type":"/etcdserverpb.KV/Range","request count":0,"request size":42,"response count":0,"response size":30,"request content":"key:\"/registry/ingress/\" range_end:\"/registry/ingress0\" count_only:true "}
{"level":"warn","ts":"2022-09-01T18:35:17.989Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-09-01T18:28:31.230Z","time spent":"6m45.295024468s","remote":"127.0.0.1:57444","response type":"/etcdserverpb.KV/Range","request count":0,"request size":36,"response count":9,"response size":32,"request content":"key:\"/registry/pods/\" range_end:\"/registry/pods0\" count_only:true "}
{"level":"warn","ts":"2022-09-01T18:36:58.984Z","caller":"etcdserver/server.go:1138","msg":"failed to revoke lease","lease-id":"70cc82e5f0fe8cde","error":"etcdserver: request timed out"}
{"level":"warn","ts":"2022-09-01T18:36:57.921Z","caller":"wal/wal.go:802","msg":"slow fdatasync","took":"1.2185953s","expected-duration":"1s"}
{"level":"warn","ts":"2022-09-01T18:36:59.406Z","caller":"etcdserver/server.go:1138","msg":"failed to revoke lease","lease-id":"70cc82e5f0fe8cde","error":"etcdserver: request timed out"}
{"level":"warn","ts":"2022-09-01T18:37:04.638Z","caller":"etcdserver/server.go:1138","msg":"failed to revoke lease","lease-id":"70cc82e5f0fe8cde","error":"etcdserver: request timed out"}
{"level":"warn","ts":"2022-09-01T18:37:06.218Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"5.518412619s","expected-duration":"100ms","prefix":"","request":"header:<ID:8128015351599247095 > lease_revoke:<id:70cc82e5f0fe8cde>","response":"size:30"}
{"level":"warn","ts":"2022-09-01T18:37:06.218Z","caller":"etcdserver/util.go:123","msg":"failed to apply request","took":"301.375¬µs","request":"header:<ID:8128015351599247096 > lease_revoke:<id:70cc82e5f0fe8cde>","response":"size:30","error":"lease not found"}
{"level":"warn","ts":"2022-09-01T18:37:06.219Z","caller":"etcdserver/util.go:123","msg":"failed to apply request","took":"15.643¬µs","request":"header:<ID:8128015351599247097 > lease_revoke:<id:70cc82e5f0fe8cde>","response":"size:30","error":"lease not found"}
{"level":"warn","ts":"2022-09-01T18:37:32.143Z","caller":"embed/config_logging.go:169","msg":"rejected connection","remote-addr":"127.0.0.1:57686","server-name":"","error":"EOF"}
{"level":"warn","ts":"2022-09-01T18:38:34.814Z","caller":"wal/wal.go:802","msg":"slow fdatasync","took":"1.555777896s","expected-duration":"1s"}
{"level":"warn","ts":"2022-09-01T18:38:38.113Z","caller":"etcdserver/server.go:1138","msg":"failed to revoke lease","lease-id":"70cc82e5f0fe8e70","error":"etcdserver: request timed out"}
{"level":"warn","ts":"2022-09-01T18:38:37.630Z","caller":"etcdserver/server.go:1138","msg":"failed to revoke lease","lease-id":"70cc82e5f0fe8e70","error":"etcdserver: request timed out"}
{"level":"warn","ts":"2022-09-01T18:38:41.118Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"8.003753044s","expected-duration":"100ms","prefix":"","request":"header:<ID:8128015351599247099 > lease_revoke:<id:70cc82e5f0fe8e70>","response":"size:30"}
{"level":"warn","ts":"2022-09-01T18:38:41.118Z","caller":"etcdserver/util.go:123","msg":"failed to apply request","took":"39.134¬µs","request":"header:<ID:8128015351599247098 > lease_revoke:<id:70cc82e5f0fe8e70>","response":"size:30","error":"lease not found"}
{"level":"warn","ts":"2022-09-01T18:40:10.921Z","caller":"wal/wal.go:802","msg":"slow fdatasync","took":"1.887065611s","expected-duration":"1s"}
{"level":"warn","ts":"2022-09-01T18:40:09.918Z","caller":"etcdserver/server.go:1138","msg":"failed to revoke lease","lease-id":"70cc82e5f0fe8f91","error":"etcdserver: request timed out"}
{"level":"warn","ts":"2022-09-01T18:40:11.844Z","caller":"etcdserver/server.go:1138","msg":"failed to revoke lease","lease-id":"70cc82e5f0fe8f91","error":"etcdserver: request timed out"}
{"level":"warn","ts":"2022-09-01T18:40:09.678Z","caller":"etcdserver/server.go:1138","msg":"failed to revoke lease","lease-id":"70cc82e5f0fe8f91","error":"etcdserver: request timed out"}
{"level":"warn","ts":"2022-09-01T18:40:11.664Z","caller":"etcdserver/server.go:1138","msg":"failed to revoke lease","lease-id":"70cc82e5f0fe8f91","error":"etcdserver: request timed out"}
{"level":"warn","ts":"2022-09-01T18:40:18.844Z","caller":"etcdserver/server.go:1138","msg":"failed to revoke lease","lease-id":"70cc82e5f0fe8f91","error":"etcdserver: request timed out"}
{"level":"warn","ts":"2022-09-01T18:40:53.899Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"29.932654007s","expected-duration":"100ms","prefix":"","request":"header:<ID:8128015351599247100 > lease_revoke:<id:70cc82e5f0fe8f91>","response":"size:30"}
{"level":"warn","ts":"2022-09-01T18:41:31.425Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"13.520053429s","expected-duration":"100ms","prefix":"","request":"header:<ID:8128015351599247101 > lease_revoke:<id:70cc82e5f0fe8f91>","response":"size:30","error":"lease not found"}
{"level":"warn","ts":"2022-09-01T18:41:39.024Z","caller":"etcdserver/util.go:123","msg":"failed to apply request","took":"26.796413162s","request":"header:<ID:8128015351599247101 > lease_revoke:<id:70cc82e5f0fe8f91>","response":"size:30","error":"lease not found"}
{"level":"warn","ts":"2022-09-01T18:41:48.087Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"5.345489983s","expected-duration":"100ms","prefix":"","request":"header:<ID:8128015351599247102 > lease_revoke:<id:70cc82e5f0fe8f91>","response":"size:30","error":"lease not found"}
{"level":"warn","ts":"2022-09-01T18:41:48.087Z","caller":"etcdserver/util.go:123","msg":"failed to apply request","took":"5.345629034s","request":"header:<ID:8128015351599247102 > lease_revoke:<id:70cc82e5f0fe8f91>","response":"size:30","error":"lease not found"}
{"level":"warn","ts":"2022-09-01T18:41:48.088Z","caller":"etcdserver/util.go:123","msg":"failed to apply request","took":"288.236¬µs","request":"header:<ID:8128015351599247103 > lease_revoke:<id:70cc82e5f0fe8f91>","response":"size:30","error":"lease not found"}
{"level":"warn","ts":"2022-09-01T18:41:48.088Z","caller":"etcdserver/util.go:123","msg":"failed to apply request","took":"82.146¬µs","request":"header:<ID:8128015351599247104 > lease_revoke:<id:70cc82e5f0fe8f91>","response":"size:30","error":"lease not found"}
{"level":"warn","ts":"2022-09-01T18:41:52.335Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"3.288542272s","expected-duration":"100ms","prefix":"","request":"header:<ID:8128015351599247105 > lease_revoke:<id:70cc82e5f0fe9181>","response":"size:30"}

* 
* ==> etcd [7f57b049beb9] <==
* {"level":"warn","ts":"2022-09-02T15:32:47.132Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-09-02T15:32:46.487Z","time spent":"644.922442ms","remote":"127.0.0.1:52474","response type":"/etcdserverpb.KV/Range","request count":0,"request size":72,"response count":0,"response size":30,"request content":"key:\"/registry/persistentvolumeclaims/\" range_end:\"/registry/persistentvolumeclaims0\" count_only:true "}
{"level":"warn","ts":"2022-09-02T15:32:48.849Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"386.170292ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:1114"}
{"level":"info","ts":"2022-09-02T15:32:48.850Z","caller":"traceutil/trace.go:171","msg":"trace[608103320] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:26727; }","duration":"387.291282ms","start":"2022-09-02T15:32:48.463Z","end":"2022-09-02T15:32:48.850Z","steps":["trace[608103320] 'range keys from in-memory index tree'  (duration: 384.007197ms)"],"step_count":1}
{"level":"warn","ts":"2022-09-02T15:32:48.851Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-09-02T15:32:48.463Z","time spent":"387.458658ms","remote":"127.0.0.1:52480","response type":"/etcdserverpb.KV/Range","request count":0,"request size":67,"response count":1,"response size":1138,"request content":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" "}
{"level":"warn","ts":"2022-09-02T15:32:54.898Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"213.348942ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128015458042514799 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:26725 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:67 lease:8128015458042514797 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >>","response":"size:18"}
{"level":"info","ts":"2022-09-02T15:32:54.899Z","caller":"traceutil/trace.go:171","msg":"trace[394163634] transaction","detail":"{read_only:false; response_revision:26731; number_of_response:1; }","duration":"308.757947ms","start":"2022-09-02T15:32:54.590Z","end":"2022-09-02T15:32:54.899Z","steps":["trace[394163634] 'process raft request'  (duration: 94.865553ms)","trace[394163634] 'compare'  (duration: 213.039045ms)"],"step_count":2}
{"level":"warn","ts":"2022-09-02T15:32:54.899Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-09-02T15:32:54.590Z","time spent":"309.017684ms","remote":"127.0.0.1:52460","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":118,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:26725 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:67 lease:8128015458042514797 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >"}
{"level":"warn","ts":"2022-09-02T15:32:55.252Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"119.227205ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:1114"}
{"level":"info","ts":"2022-09-02T15:32:55.252Z","caller":"traceutil/trace.go:171","msg":"trace[1218847568] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:26731; }","duration":"119.422356ms","start":"2022-09-02T15:32:55.133Z","end":"2022-09-02T15:32:55.252Z","steps":["trace[1218847568] 'range keys from in-memory index tree'  (duration: 119.022676ms)"],"step_count":1}
{"level":"warn","ts":"2022-09-02T15:32:56.875Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"106.191879ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/namespaces/kube-public\" ","response":"range_response_count:1 size:354"}
{"level":"info","ts":"2022-09-02T15:32:56.968Z","caller":"traceutil/trace.go:171","msg":"trace[1089371154] range","detail":"{range_begin:/registry/namespaces/kube-public; range_end:; response_count:1; response_revision:26732; }","duration":"106.70782ms","start":"2022-09-02T15:32:56.769Z","end":"2022-09-02T15:32:56.876Z","steps":["trace[1089371154] 'range keys from bolt db'  (duration: 106.001023ms)"],"step_count":1}
{"level":"warn","ts":"2022-09-02T15:32:57.083Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"111.26845ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/namespaces/kube-node-lease\" ","response":"range_response_count:1 size:366"}
{"level":"info","ts":"2022-09-02T15:32:57.195Z","caller":"traceutil/trace.go:171","msg":"trace[1697497242] range","detail":"{range_begin:/registry/namespaces/kube-node-lease; range_end:; response_count:1; response_revision:26732; }","duration":"111.376936ms","start":"2022-09-02T15:32:56.971Z","end":"2022-09-02T15:32:57.083Z","steps":["trace[1697497242] 'range keys from bolt db'  (duration: 111.072138ms)"],"step_count":1}
{"level":"info","ts":"2022-09-02T15:32:58.236Z","caller":"traceutil/trace.go:171","msg":"trace[749557959] linearizableReadLoop","detail":"{readStateIndex:35704; appliedIndex:35704; }","duration":"379.988094ms","start":"2022-09-02T15:32:57.856Z","end":"2022-09-02T15:32:58.236Z","steps":["trace[749557959] 'read index received'  (duration: 379.976399ms)","trace[749557959] 'applied index is now lower than readState.Index'  (duration: 9.907¬µs)"],"step_count":2}
{"level":"warn","ts":"2022-09-02T15:32:58.311Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"455.405066ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-09-02T15:32:58.312Z","caller":"traceutil/trace.go:171","msg":"trace[1193022353] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:26733; }","duration":"455.637071ms","start":"2022-09-02T15:32:57.856Z","end":"2022-09-02T15:32:58.311Z","steps":["trace[1193022353] 'agreement among raft nodes before linearized reading'  (duration: 380.322968ms)","trace[1193022353] 'range keys from in-memory index tree'  (duration: 75.042326ms)"],"step_count":2}
{"level":"warn","ts":"2022-09-02T15:32:58.312Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-09-02T15:32:57.856Z","time spent":"455.75904ms","remote":"127.0.0.1:52452","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":30,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2022-09-02T15:32:58.312Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"146.925143ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/namespaces/\" range_end:\"/registry/namespaces0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"info","ts":"2022-09-02T15:32:58.312Z","caller":"traceutil/trace.go:171","msg":"trace[1448568491] range","detail":"{range_begin:/registry/namespaces/; range_end:/registry/namespaces0; response_count:0; response_revision:26733; }","duration":"147.066812ms","start":"2022-09-02T15:32:58.165Z","end":"2022-09-02T15:32:58.312Z","steps":["trace[1448568491] 'agreement among raft nodes before linearized reading'  (duration: 70.948031ms)","trace[1448568491] 'count revisions from in-memory index tree'  (duration: 75.911534ms)"],"step_count":2}
{"level":"warn","ts":"2022-09-02T15:32:58.972Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"146.48619ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-09-02T15:32:58.973Z","caller":"traceutil/trace.go:171","msg":"trace[1224972265] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:26734; }","duration":"146.69036ms","start":"2022-09-02T15:32:58.826Z","end":"2022-09-02T15:32:58.972Z","steps":["trace[1224972265] 'range keys from in-memory index tree'  (duration: 146.165184ms)"],"step_count":1}
{"level":"warn","ts":"2022-09-02T15:32:59.156Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-09-02T15:32:58.826Z","time spent":"330.212664ms","remote":"127.0.0.1:52452","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":30,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2022-09-02T15:33:01.014Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"1.177181103s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-09-02T15:33:01.014Z","caller":"traceutil/trace.go:171","msg":"trace[825771042] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:26735; }","duration":"1.177283198s","start":"2022-09-02T15:32:59.837Z","end":"2022-09-02T15:33:01.014Z","steps":["trace[825771042] 'range keys from in-memory index tree'  (duration: 1.176835105s)"],"step_count":1}
{"level":"warn","ts":"2022-09-02T15:33:01.015Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"1.121311295s","expected-duration":"100ms","prefix":"","request":"header:<ID:8128015458042514824 > lease_revoke:<id:70cc82feb9820d49>","response":"size:30"}
{"level":"info","ts":"2022-09-02T15:33:01.015Z","caller":"traceutil/trace.go:171","msg":"trace[842509466] linearizableReadLoop","detail":"{readStateIndex:35707; appliedIndex:35706; }","duration":"898.1668ms","start":"2022-09-02T15:33:00.117Z","end":"2022-09-02T15:33:01.015Z","steps":["trace[842509466] 'read index received'  (duration: 303.644367ms)","trace[842509466] 'applied index is now lower than readState.Index'  (duration: 594.520209ms)"],"step_count":2}
{"level":"warn","ts":"2022-09-02T15:33:01.015Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"898.522196ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/limitranges/\" range_end:\"/registry/limitranges0\" count_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-09-02T15:33:01.015Z","caller":"traceutil/trace.go:171","msg":"trace[266510241] range","detail":"{range_begin:/registry/limitranges/; range_end:/registry/limitranges0; response_count:0; response_revision:26735; }","duration":"898.671547ms","start":"2022-09-02T15:33:00.116Z","end":"2022-09-02T15:33:01.015Z","steps":["trace[266510241] 'agreement among raft nodes before linearized reading'  (duration: 898.502909ms)"],"step_count":1}
{"level":"warn","ts":"2022-09-02T15:33:01.015Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-09-02T15:33:00.116Z","time spent":"898.804313ms","remote":"127.0.0.1:52466","response type":"/etcdserverpb.KV/Range","request count":0,"request size":50,"response count":0,"response size":30,"request content":"key:\"/registry/limitranges/\" range_end:\"/registry/limitranges0\" count_only:true "}
{"level":"warn","ts":"2022-09-02T15:33:01.015Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-09-02T15:32:59.837Z","time spent":"1.177369402s","remote":"127.0.0.1:52452","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":30,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2022-09-02T15:33:01.016Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"189.662927ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-09-02T15:33:01.016Z","caller":"traceutil/trace.go:171","msg":"trace[1613928519] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:26735; }","duration":"189.88703ms","start":"2022-09-02T15:33:00.826Z","end":"2022-09-02T15:33:01.016Z","steps":["trace[1613928519] 'agreement among raft nodes before linearized reading'  (duration: 189.647534ms)"],"step_count":1}
{"level":"warn","ts":"2022-09-02T15:33:02.473Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"1.161730441s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-09-02T15:33:02.473Z","caller":"traceutil/trace.go:171","msg":"trace[405358361] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:26735; }","duration":"1.161836256s","start":"2022-09-02T15:33:01.311Z","end":"2022-09-02T15:33:02.473Z","steps":["trace[405358361] 'range keys from in-memory index tree'  (duration: 1.161590932s)"],"step_count":1}
{"level":"warn","ts":"2022-09-02T15:33:02.473Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-09-02T15:33:01.311Z","time spent":"1.162202631s","remote":"127.0.0.1:52452","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":30,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2022-09-02T15:33:02.474Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"781.841048ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:1114"}
{"level":"info","ts":"2022-09-02T15:33:02.474Z","caller":"traceutil/trace.go:171","msg":"trace[249543287] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:26735; }","duration":"781.894423ms","start":"2022-09-02T15:33:01.692Z","end":"2022-09-02T15:33:02.474Z","steps":["trace[249543287] 'range keys from in-memory index tree'  (duration: 781.599206ms)"],"step_count":1}
{"level":"warn","ts":"2022-09-02T15:33:02.474Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-09-02T15:33:01.692Z","time spent":"781.961612ms","remote":"127.0.0.1:52480","response type":"/etcdserverpb.KV/Range","request count":0,"request size":67,"response count":1,"response size":1138,"request content":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" "}
{"level":"info","ts":"2022-09-02T15:33:04.930Z","caller":"traceutil/trace.go:171","msg":"trace[2138641096] linearizableReadLoop","detail":"{readStateIndex:35709; appliedIndex:35709; }","duration":"343.296014ms","start":"2022-09-02T15:33:04.587Z","end":"2022-09-02T15:33:04.930Z","steps":["trace[2138641096] 'read index received'  (duration: 343.286739ms)","trace[2138641096] 'applied index is now lower than readState.Index'  (duration: 7.899¬µs)"],"step_count":2}
{"level":"warn","ts":"2022-09-02T15:33:05.012Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"179.124015ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"warn","ts":"2022-09-02T15:33:05.012Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"425.732986ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/masterleases/192.168.49.2\" ","response":"range_response_count:1 size:136"}
{"level":"info","ts":"2022-09-02T15:33:05.012Z","caller":"traceutil/trace.go:171","msg":"trace[776427933] range","detail":"{range_begin:/registry/masterleases/192.168.49.2; range_end:; response_count:1; response_revision:26737; }","duration":"425.78791ms","start":"2022-09-02T15:33:04.587Z","end":"2022-09-02T15:33:05.012Z","steps":["trace[776427933] 'agreement among raft nodes before linearized reading'  (duration: 343.386229ms)","trace[776427933] 'range keys from in-memory index tree'  (duration: 82.3045ms)"],"step_count":2}
{"level":"info","ts":"2022-09-02T15:33:05.012Z","caller":"traceutil/trace.go:171","msg":"trace[1222179536] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:26737; }","duration":"179.275375ms","start":"2022-09-02T15:33:04.833Z","end":"2022-09-02T15:33:05.012Z","steps":["trace[1222179536] 'agreement among raft nodes before linearized reading'  (duration: 97.0063ms)","trace[1222179536] 'range keys from in-memory index tree'  (duration: 82.13754ms)"],"step_count":2}
{"level":"warn","ts":"2022-09-02T15:33:05.013Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-09-02T15:33:04.587Z","time spent":"425.956754ms","remote":"127.0.0.1:52460","response type":"/etcdserverpb.KV/Range","request count":0,"request size":37,"response count":1,"response size":160,"request content":"key:\"/registry/masterleases/192.168.49.2\" "}
{"level":"warn","ts":"2022-09-02T15:33:08.989Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"141.88253ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-09-02T15:33:08.990Z","caller":"traceutil/trace.go:171","msg":"trace[718827414] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:26740; }","duration":"142.326255ms","start":"2022-09-02T15:33:08.847Z","end":"2022-09-02T15:33:08.990Z","steps":["trace[718827414] 'agreement among raft nodes before linearized reading'  (duration: 99.087418ms)","trace[718827414] 'range keys from in-memory index tree'  (duration: 42.756783ms)"],"step_count":2}
{"level":"warn","ts":"2022-09-02T15:33:14.767Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"111.688368ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/default/kubernetes\" ","response":"range_response_count:1 size:423"}
{"level":"info","ts":"2022-09-02T15:33:14.767Z","caller":"traceutil/trace.go:171","msg":"trace[2039590959] range","detail":"{range_begin:/registry/services/endpoints/default/kubernetes; range_end:; response_count:1; response_revision:26744; }","duration":"111.790103ms","start":"2022-09-02T15:33:14.655Z","end":"2022-09-02T15:33:14.767Z","steps":["trace[2039590959] 'agreement among raft nodes before linearized reading'  (duration: 67.916437ms)","trace[2039590959] 'range keys from in-memory index tree'  (duration: 43.719194ms)"],"step_count":2}
{"level":"warn","ts":"2022-09-02T15:33:15.532Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"131.997239ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/\" range_end:\"/registry/pods0\" limit:500 ","response":"range_response_count:9 size:42210"}
{"level":"info","ts":"2022-09-02T15:33:15.532Z","caller":"traceutil/trace.go:171","msg":"trace[1437997861] range","detail":"{range_begin:/registry/pods/; range_end:/registry/pods0; response_count:9; response_revision:26745; }","duration":"132.109829ms","start":"2022-09-02T15:33:15.400Z","end":"2022-09-02T15:33:15.532Z","steps":["trace[1437997861] 'range keys from bolt db'  (duration: 131.640681ms)"],"step_count":1}
{"level":"warn","ts":"2022-09-02T15:33:16.837Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"669.068859ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/events/\" range_end:\"/registry/events0\" limit:500 ","response":"range_response_count:85 size:59198"}
{"level":"info","ts":"2022-09-02T15:33:16.837Z","caller":"traceutil/trace.go:171","msg":"trace[1705785648] range","detail":"{range_begin:/registry/events/; range_end:/registry/events0; response_count:85; response_revision:26745; }","duration":"669.186129ms","start":"2022-09-02T15:33:16.168Z","end":"2022-09-02T15:33:16.837Z","steps":["trace[1705785648] 'range keys from bolt db'  (duration: 668.830918ms)"],"step_count":1}
{"level":"warn","ts":"2022-09-02T15:33:16.837Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-09-02T15:33:16.168Z","time spent":"669.266354ms","remote":"127.0.0.1:52464","response type":"/etcdserverpb.KV/Range","request count":0,"request size":41,"response count":85,"response size":59222,"request content":"key:\"/registry/events/\" range_end:\"/registry/events0\" limit:500 "}
{"level":"warn","ts":"2022-09-02T15:33:20.413Z","caller":"wal/wal.go:802","msg":"slow fdatasync","took":"1.17849623s","expected-duration":"1s"}
{"level":"info","ts":"2022-09-02T15:33:20.413Z","caller":"traceutil/trace.go:171","msg":"trace[1364303589] linearizableReadLoop","detail":"{readStateIndex:35722; appliedIndex:35722; }","duration":"314.258933ms","start":"2022-09-02T15:33:20.099Z","end":"2022-09-02T15:33:20.413Z","steps":["trace[1364303589] 'read index received'  (duration: 314.25112ms)","trace[1364303589] 'applied index is now lower than readState.Index'  (duration: 6.546¬µs)"],"step_count":2}
{"level":"warn","ts":"2022-09-02T15:33:20.415Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"195.554828ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/clusterrolebindings/\" range_end:\"/registry/clusterrolebindings0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"info","ts":"2022-09-02T15:33:20.419Z","caller":"traceutil/trace.go:171","msg":"trace[2014791482] range","detail":"{range_begin:/registry/clusterrolebindings/; range_end:/registry/clusterrolebindings0; response_count:0; response_revision:26747; }","duration":"199.885508ms","start":"2022-09-02T15:33:20.219Z","end":"2022-09-02T15:33:20.419Z","steps":["trace[2014791482] 'agreement among raft nodes before linearized reading'  (duration: 194.175177ms)"],"step_count":1}
{"level":"warn","ts":"2022-09-02T15:33:20.420Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"321.013618ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/jobs/\" range_end:\"/registry/jobs0\" count_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-09-02T15:33:20.420Z","caller":"traceutil/trace.go:171","msg":"trace[663531005] range","detail":"{range_begin:/registry/jobs/; range_end:/registry/jobs0; response_count:0; response_revision:26747; }","duration":"321.199069ms","start":"2022-09-02T15:33:20.099Z","end":"2022-09-02T15:33:20.420Z","steps":["trace[663531005] 'agreement among raft nodes before linearized reading'  (duration: 314.474107ms)"],"step_count":1}
{"level":"warn","ts":"2022-09-02T15:33:20.420Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-09-02T15:33:20.099Z","time spent":"321.370121ms","remote":"127.0.0.1:52506","response type":"/etcdserverpb.KV/Range","request count":0,"request size":36,"response count":0,"response size":30,"request content":"key:\"/registry/jobs/\" range_end:\"/registry/jobs0\" count_only:true "}

* 
* ==> kernel <==
*  15:33:21 up  8:06,  0 users,  load average: 8.67, 6.34, 6.57
Linux minikube 5.15.0-46-generic #49~20.04.1-Ubuntu SMP Thu Aug 4 19:15:44 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 20.04.4 LTS"

* 
* ==> kube-apiserver [5b62549995e3] <==
* Trace[1091034900]: [1.327828349s] [1.327828349s] END
I0902 15:30:43.216390       1 trace.go:205] Trace[1147279523]: "Update" url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube,user-agent:kubelet/v1.24.3 (linux/amd64) kubernetes/aef86a9,audit-id:19a8958d-0685-4ed2-861f-35c30b2f65cd,client:192.168.49.2,accept:application/vnd.kubernetes.protobuf,application/json,protocol:HTTP/2.0 (02-Sep-2022 15:30:41.887) (total time: 1328ms):
Trace[1147279523]: ---"Object stored in database" 1327ms (15:30:43.216)
Trace[1147279523]: [1.328349591s] [1.328349591s] END
I0902 15:31:05.183623       1 trace.go:205] Trace[401090807]: "GuaranteedUpdate etcd3" type:*v1.Endpoints (02-Sep-2022 15:31:04.588) (total time: 594ms):
Trace[401090807]: ---"Transaction committed" 585ms (15:31:05.183)
Trace[401090807]: [594.512435ms] [594.512435ms] END
I0902 15:31:10.133610       1 trace.go:205] Trace[192730527]: "Get" url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,audit-id:7923ca2d-64c4-455b-97f5-9f0769666aa0,client:192.168.49.2,accept:application/json, */*,protocol:HTTP/2.0 (02-Sep-2022 15:31:09.288) (total time: 844ms):
Trace[192730527]: ---"About to write a response" 844ms (15:31:10.133)
Trace[192730527]: [844.738906ms] [844.738906ms] END
I0902 15:31:10.652288       1 trace.go:205] Trace[938204778]: "GuaranteedUpdate etcd3" type:*core.Pod (02-Sep-2022 15:31:09.269) (total time: 1342ms):
Trace[938204778]: ---"Transaction prepared" 1339ms (15:31:10.608)
Trace[938204778]: [1.342561821s] [1.342561821s] END
I0902 15:31:10.652817       1 trace.go:205] Trace[1182591264]: "Patch" url:/api/v1/namespaces/default/pods/test-dep1-56bdf49f98-748td/status,user-agent:kubelet/v1.24.3 (linux/amd64) kubernetes/aef86a9,audit-id:e9d8893c-3911-4b23-a476-c382974fb67a,client:192.168.49.2,accept:application/vnd.kubernetes.protobuf,application/json,protocol:HTTP/2.0 (02-Sep-2022 15:31:09.185) (total time: 1466ms):
Trace[1182591264]: ---"About to apply patch" 82ms (15:31:09.269)
Trace[1182591264]: ---"About to check admission control" 1258ms (15:31:10.528)
Trace[1182591264]: ---"Object stored in database" 124ms (15:31:10.652)
Trace[1182591264]: [1.46695437s] [1.46695437s] END
I0902 15:32:06.952362       1 trace.go:205] Trace[1051230710]: "GuaranteedUpdate etcd3" type:*coordination.Lease (02-Sep-2022 15:32:05.834) (total time: 1118ms):
Trace[1051230710]: ---"Transaction committed" 1117ms (15:32:06.952)
Trace[1051230710]: [1.118267614s] [1.118267614s] END
I0902 15:32:06.952718       1 trace.go:205] Trace[994127145]: "Update" url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube,user-agent:kubelet/v1.24.3 (linux/amd64) kubernetes/aef86a9,audit-id:e240582c-d98e-4105-ba40-2ba4e120d647,client:192.168.49.2,accept:application/vnd.kubernetes.protobuf,application/json,protocol:HTTP/2.0 (02-Sep-2022 15:32:05.833) (total time: 1118ms):
Trace[994127145]: ---"Object stored in database" 1118ms (15:32:06.952)
Trace[994127145]: [1.11878403s] [1.11878403s] END
I0902 15:32:19.340899       1 trace.go:205] Trace[401140936]: "GuaranteedUpdate etcd3" type:*core.Endpoints (02-Sep-2022 15:32:18.674) (total time: 666ms):
Trace[401140936]: ---"Transaction prepared" 534ms (15:32:19.208)
Trace[401140936]: ---"Transaction committed" 131ms (15:32:19.340)
Trace[401140936]: [666.023449ms] [666.023449ms] END
I0902 15:32:19.342204       1 trace.go:205] Trace[494293335]: "Update" url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,audit-id:c6e3d71b-c7c1-4086-b2fc-3c71e19cdc29,client:192.168.49.2,accept:application/json, */*,protocol:HTTP/2.0 (02-Sep-2022 15:32:18.674) (total time: 668ms):
Trace[494293335]: ---"Object stored in database" 666ms (15:32:19.340)
Trace[494293335]: [668.040811ms] [668.040811ms] END
I0902 15:32:25.120369       1 trace.go:205] Trace[925962917]: "Get" url:/api/v1/namespaces/default,user-agent:kube-apiserver/v1.24.3 (linux/amd64) kubernetes/aef86a9,audit-id:5d4f768d-20dc-4e41-b77a-765e0cd4e403,client:127.0.0.1,accept:application/vnd.kubernetes.protobuf, */*,protocol:HTTP/2.0 (02-Sep-2022 15:32:24.576) (total time: 543ms):
Trace[925962917]: ---"About to write a response" 543ms (15:32:25.120)
Trace[925962917]: [543.569053ms] [543.569053ms] END
I0902 15:32:26.219482       1 trace.go:205] Trace[1403237987]: "GuaranteedUpdate etcd3" type:*v1.Endpoints (02-Sep-2022 15:32:25.128) (total time: 1091ms):
Trace[1403237987]: ---"Transaction committed" 1084ms (15:32:26.219)
Trace[1403237987]: [1.091170716s] [1.091170716s] END
I0902 15:32:26.426991       1 trace.go:205] Trace[1302188609]: "Get" url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,audit-id:345aeff2-a66c-448e-adae-17bafd1fbb41,client:192.168.49.2,accept:application/json, */*,protocol:HTTP/2.0 (02-Sep-2022 15:32:25.594) (total time: 832ms):
Trace[1302188609]: ---"About to write a response" 832ms (15:32:26.426)
Trace[1302188609]: [832.53091ms] [832.53091ms] END
I0902 15:32:35.097076       1 trace.go:205] Trace[787157087]: "GuaranteedUpdate etcd3" type:*v1.Endpoints (02-Sep-2022 15:32:34.580) (total time: 516ms):
Trace[787157087]: ---"Transaction committed" 512ms (15:32:35.097)
Trace[787157087]: [516.293899ms] [516.293899ms] END
I0902 15:32:45.308755       1 trace.go:205] Trace[1326920704]: "GuaranteedUpdate etcd3" type:*v1.Endpoints (02-Sep-2022 15:32:44.723) (total time: 585ms):
Trace[1326920704]: ---"Transaction committed" 537ms (15:32:45.308)
Trace[1326920704]: [585.115074ms] [585.115074ms] END
I0902 15:33:02.475205       1 trace.go:205] Trace[1015461247]: "Get" url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,audit-id:a1f7b1fe-165b-450b-b25e-1b84fa4a541b,client:192.168.49.2,accept:application/json, */*,protocol:HTTP/2.0 (02-Sep-2022 15:33:01.691) (total time: 783ms):
Trace[1015461247]: ---"About to write a response" 783ms (15:33:02.475)
Trace[1015461247]: [783.870395ms] [783.870395ms] END
I0902 15:33:15.989365       1 trace.go:205] Trace[1804800527]: "List(recursive=true) etcd3" key:/pods,resourceVersion:,resourceVersionMatch:,limit:500,continue: (02-Sep-2022 15:33:15.399) (total time: 589ms):
Trace[1804800527]: [589.461788ms] [589.461788ms] END
I0902 15:33:16.127688       1 trace.go:205] Trace[1319160237]: "List" url:/api/v1/pods,user-agent:kubectl/v1.24.3 (linux/amd64) kubernetes/aef86a9,audit-id:cc718644-6d12-4f9a-a9e5-ce010a42214d,client:127.0.0.1,accept:application/json, */*,protocol:HTTP/2.0 (02-Sep-2022 15:33:15.399) (total time: 727ms):
Trace[1319160237]: ---"Listing from storage done" 589ms (15:33:15.989)
Trace[1319160237]: ---"Writing http response done" count:9 138ms (15:33:16.127)
Trace[1319160237]: [727.845304ms] [727.845304ms] END
I0902 15:33:16.865828       1 trace.go:205] Trace[642958536]: "List(recursive=true) etcd3" key:/events,resourceVersion:,resourceVersionMatch:,limit:500,continue: (02-Sep-2022 15:33:16.167) (total time: 697ms):
Trace[642958536]: [697.905719ms] [697.905719ms] END
I0902 15:33:16.866387       1 trace.go:205] Trace[2017122364]: "List" url:/api/v1/events,user-agent:kubectl/v1.24.3 (linux/amd64) kubernetes/aef86a9,audit-id:4497639f-9b41-4e37-8ce2-e43cfb728ee4,client:127.0.0.1,accept:application/json, */*,protocol:HTTP/2.0 (02-Sep-2022 15:33:16.167) (total time: 698ms):
Trace[2017122364]: ---"Listing from storage done" 698ms (15:33:16.866)
Trace[2017122364]: [698.533826ms] [698.533826ms] END

* 
* ==> kube-apiserver [d2c2d1eaffce] <==
* I0901 18:27:26.468547       1 trace.go:205] Trace[240193103]: "Update" url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,audit-id:103d14ab-2b6a-4cf3-a8f3-2f652c877c82,client:192.168.49.2,accept:application/json, */*,protocol:HTTP/2.0 (01-Sep-2022 18:27:25.440) (total time: 1027ms):
Trace[240193103]: ---"Object stored in database" 1026ms (18:27:26.468)
Trace[240193103]: [1.027782183s] [1.027782183s] END
I0901 18:27:29.172265       1 trace.go:205] Trace[1065221672]: "GuaranteedUpdate etcd3" type:*coordination.Lease (01-Sep-2022 18:27:27.947) (total time: 1224ms):
Trace[1065221672]: ---"Transaction committed" 1223ms (18:27:29.172)
Trace[1065221672]: [1.224707809s] [1.224707809s] END
I0901 18:27:29.172886       1 trace.go:205] Trace[1463382095]: "Update" url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube,user-agent:kubelet/v1.24.3 (linux/amd64) kubernetes/aef86a9,audit-id:4834bba3-bf9a-4fe0-92db-98c77d1c83d5,client:192.168.49.2,accept:application/vnd.kubernetes.protobuf,application/json,protocol:HTTP/2.0 (01-Sep-2022 18:27:27.947) (total time: 1225ms):
Trace[1463382095]: ---"Object stored in database" 1225ms (18:27:29.172)
Trace[1463382095]: [1.225572045s] [1.225572045s] END
I0901 18:27:30.179038       1 trace.go:205] Trace[1702386286]: "Get" url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,audit-id:dbf99bf8-14dc-4411-bfcc-cfbe3ca105f9,client:192.168.49.2,accept:application/json, */*,protocol:HTTP/2.0 (01-Sep-2022 18:27:28.578) (total time: 1600ms):
Trace[1702386286]: ---"About to write a response" 1600ms (18:27:30.178)
Trace[1702386286]: [1.600326485s] [1.600326485s] END
I0901 18:27:34.232794       1 trace.go:205] Trace[1215185139]: "GuaranteedUpdate etcd3" type:*v1.Endpoints (01-Sep-2022 18:27:33.049) (total time: 1182ms):
Trace[1215185139]: ---"Transaction prepared" 1067ms (18:27:34.216)
Trace[1215185139]: [1.182736338s] [1.182736338s] END
I0901 18:27:36.272722       1 trace.go:205] Trace[638981503]: "Get" url:/api/v1/namespaces/default/endpoints/kubernetes,user-agent:kube-apiserver/v1.24.3 (linux/amd64) kubernetes/aef86a9,audit-id:f2d8ceaf-06b5-4975-a46d-20f1395b49f7,client:127.0.0.1,accept:application/vnd.kubernetes.protobuf, */*,protocol:HTTP/2.0 (01-Sep-2022 18:27:34.236) (total time: 2036ms):
Trace[638981503]: ---"About to write a response" 2036ms (18:27:36.272)
Trace[638981503]: [2.036187428s] [2.036187428s] END
I0901 18:27:44.194897       1 trace.go:205] Trace[1006152872]: "GuaranteedUpdate etcd3" type:*v1.Endpoints (01-Sep-2022 18:27:42.890) (total time: 1304ms):
Trace[1006152872]: ---"initial value restored" 361ms (18:27:43.252)
Trace[1006152872]: ---"Transaction committed" 941ms (18:27:44.194)
Trace[1006152872]: [1.304350989s] [1.304350989s] END
I0901 18:27:53.681635       1 trace.go:205] Trace[1481995431]: "GuaranteedUpdate etcd3" type:*v1.Endpoints (01-Sep-2022 18:27:52.798) (total time: 882ms):
Trace[1481995431]: ---"Transaction committed" 880ms (18:27:53.681)
Trace[1481995431]: [882.867641ms] [882.867641ms] END
I0901 18:28:01.564742       1 trace.go:205] Trace[311227669]: "GuaranteedUpdate etcd3" type:*coordination.Lease (01-Sep-2022 18:28:00.532) (total time: 1031ms):
Trace[311227669]: ---"Transaction prepared" 210ms (18:28:00.743)
Trace[311227669]: ---"Transaction committed" 820ms (18:28:01.564)
Trace[311227669]: [1.031653041s] [1.031653041s] END
I0901 18:28:01.565038       1 trace.go:205] Trace[843239197]: "Update" url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube,user-agent:kubelet/v1.24.3 (linux/amd64) kubernetes/aef86a9,audit-id:df750154-80bb-4297-80b3-b9433f727602,client:192.168.49.2,accept:application/vnd.kubernetes.protobuf,application/json,protocol:HTTP/2.0 (01-Sep-2022 18:28:00.532) (total time: 1032ms):
Trace[843239197]: ---"Object stored in database" 1031ms (18:28:01.564)
Trace[843239197]: [1.032240024s] [1.032240024s] END
I0901 18:28:03.904653       1 trace.go:205] Trace[754272777]: "GuaranteedUpdate etcd3" type:*v1.Endpoints (01-Sep-2022 18:28:02.830) (total time: 1073ms):
Trace[754272777]: ---"Transaction committed" 1059ms (18:28:03.904)
Trace[754272777]: [1.073705485s] [1.073705485s] END
I0901 18:28:05.266783       1 trace.go:205] Trace[111750278]: "Get" url:/api/v1/namespaces/default/endpoints/kubernetes,user-agent:kube-apiserver/v1.24.3 (linux/amd64) kubernetes/aef86a9,audit-id:ff9fa643-a1ff-401f-9bfd-1d382fcab22c,client:127.0.0.1,accept:application/vnd.kubernetes.protobuf, */*,protocol:HTTP/2.0 (01-Sep-2022 18:28:03.911) (total time: 1355ms):
Trace[111750278]: ---"About to write a response" 1355ms (18:28:05.266)
Trace[111750278]: [1.355354745s] [1.355354745s] END
I0901 18:28:08.054285       1 trace.go:205] Trace[173159249]: "List(recursive=true) etcd3" key:/masterleases/,resourceVersion:0,resourceVersionMatch:NotOlderThan,limit:0,continue: (01-Sep-2022 18:28:05.578) (total time: 963ms):
Trace[173159249]: [963.671196ms] [963.671196ms] END
I0901 18:28:11.718788       1 trace.go:205] Trace[1194113370]: "Get" url:/apis/discovery.k8s.io/v1/namespaces/default/endpointslices/kubernetes,user-agent:kube-apiserver/v1.24.3 (linux/amd64) kubernetes/aef86a9,audit-id:266a6b9c-9374-411f-a9ed-f73cb165e69a,client:127.0.0.1,accept:application/vnd.kubernetes.protobuf, */*,protocol:HTTP/2.0 (01-Sep-2022 18:28:10.676) (total time: 1041ms):
Trace[1194113370]: ---"About to write a response" 1041ms (18:28:11.718)
Trace[1194113370]: [1.041869249s] [1.041869249s] END
I0901 18:28:14.178557       1 trace.go:205] Trace[47339454]: "GuaranteedUpdate etcd3" type:*coordination.Lease (01-Sep-2022 18:28:13.678) (total time: 500ms):
Trace[47339454]: ---"Transaction committed" 498ms (18:28:14.178)
Trace[47339454]: [500.115202ms] [500.115202ms] END
I0901 18:28:14.179068       1 trace.go:205] Trace[1531476337]: "Update" url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube,user-agent:kubelet/v1.24.3 (linux/amd64) kubernetes/aef86a9,audit-id:70c54c27-26c7-48f3-94d4-5f0f1fa6b94d,client:192.168.49.2,accept:application/vnd.kubernetes.protobuf,application/json,protocol:HTTP/2.0 (01-Sep-2022 18:28:13.237) (total time: 941ms):
Trace[1531476337]: ---"About to store object in database" 440ms (18:28:13.678)
Trace[1531476337]: ---"Object stored in database" 500ms (18:28:14.178)
Trace[1531476337]: [941.327848ms] [941.327848ms] END
I0901 18:28:22.394986       1 trace.go:205] Trace[2125801689]: "Get" url:/api/v1/namespaces/default,user-agent:kube-apiserver/v1.24.3 (linux/amd64) kubernetes/aef86a9,audit-id:8108e750-13c1-4ee4-8225-8c789052fee9,client:127.0.0.1,accept:application/vnd.kubernetes.protobuf, */*,protocol:HTTP/2.0 (01-Sep-2022 18:28:13.678) (total time: 8716ms):
Trace[2125801689]: ---"About to write a response" 8716ms (18:28:22.394)
Trace[2125801689]: [8.716435048s] [8.716435048s] END
{"level":"warn","ts":"2022-09-01T18:28:20.824Z","logger":"etcd-client","caller":"v3/retry_interceptor.go:62","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc0006f7880/127.0.0.1:2379","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = context deadline exceeded"}
{"level":"warn","ts":"2022-09-01T18:28:20.551Z","logger":"etcd-client","caller":"v3/retry_interceptor.go:62","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc0006f7880/127.0.0.1:2379","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = context deadline exceeded"}
I0901 18:28:31.238480       1 trace.go:205] Trace[2020386768]: "DeltaFIFO Pop Process" ID:v1.authorization.k8s.io,Depth:27,Reason:slow event handlers blocking the queue (01-Sep-2022 18:28:29.050) (total time: 258ms):
Trace[2020386768]: [258.588416ms] [258.588416ms] END
W0901 18:33:04.904091       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout". Reconnecting...
W0901 18:33:05.354526       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout". Reconnecting...
W0901 18:33:05.354621       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: i/o timeout". Reconnecting...

* 
* ==> kube-controller-manager [6ff11fb7bd95] <==
* I0902 15:06:33.276318       1 trace.go:205] Trace[84142186]: "Reflector ListAndWatch" name:vendor/k8s.io/client-go/informers/factory.go:134 (02-Sep-2022 15:06:22.683) (total time: 10592ms):
Trace[84142186]: ---"Objects listed" error:<nil> 10592ms (15:06:33.276)
Trace[84142186]: [10.592805885s] [10.592805885s] END
I0902 15:06:33.281552       1 trace.go:205] Trace[1220418571]: "Reflector ListAndWatch" name:vendor/k8s.io/client-go/informers/factory.go:134 (02-Sep-2022 15:06:22.971) (total time: 10309ms):
Trace[1220418571]: ---"Objects listed" error:<nil> 10307ms (15:06:33.278)
Trace[1220418571]: [10.309985813s] [10.309985813s] END
I0902 15:06:33.886540       1 shared_informer.go:262] Caches are synced for TTL after finished
I0902 15:06:33.886627       1 shared_informer.go:262] Caches are synced for certificate-csrsigning-kubelet-serving
I0902 15:06:33.886687       1 shared_informer.go:262] Caches are synced for certificate-csrsigning-kubelet-client
I0902 15:06:34.816099       1 trace.go:205] Trace[2070424372]: "Reflector ListAndWatch" name:vendor/k8s.io/client-go/informers/factory.go:134 (02-Sep-2022 15:06:23.404) (total time: 11411ms):
Trace[2070424372]: ---"Objects listed" error:<nil> 11411ms (15:06:34.815)
Trace[2070424372]: [11.411789737s] [11.411789737s] END
I0902 15:06:34.816933       1 trace.go:205] Trace[422826453]: "Reflector ListAndWatch" name:vendor/k8s.io/client-go/informers/factory.go:134 (02-Sep-2022 15:06:22.686) (total time: 12130ms):
Trace[422826453]: ---"Objects listed" error:<nil> 12130ms (15:06:34.816)
Trace[422826453]: [12.130886073s] [12.130886073s] END
I0902 15:06:34.817474       1 trace.go:205] Trace[1280510450]: "Reflector ListAndWatch" name:vendor/k8s.io/client-go/informers/factory.go:134 (02-Sep-2022 15:06:22.681) (total time: 12135ms):
Trace[1280510450]: ---"Objects listed" error:<nil> 12135ms (15:06:34.817)
Trace[1280510450]: [12.135527976s] [12.135527976s] END
I0902 15:06:34.817690       1 shared_informer.go:262] Caches are synced for PV protection
I0902 15:06:34.818859       1 shared_informer.go:262] Caches are synced for cronjob
I0902 15:06:34.818946       1 shared_informer.go:262] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0902 15:06:34.819002       1 shared_informer.go:262] Caches are synced for certificate-csrsigning-legacy-unknown
I0902 15:06:34.819085       1 shared_informer.go:262] Caches are synced for certificate-csrapproving
I0902 15:06:34.819121       1 shared_informer.go:262] Caches are synced for expand
I0902 15:06:35.482914       1 shared_informer.go:262] Caches are synced for PVC protection
I0902 15:06:35.483406       1 shared_informer.go:262] Caches are synced for HPA
I0902 15:06:35.483854       1 shared_informer.go:262] Caches are synced for GC
I0902 15:06:35.484131       1 shared_informer.go:262] Caches are synced for taint
I0902 15:06:35.484359       1 node_lifecycle_controller.go:1399] Initializing eviction metric for zone: 
I0902 15:06:36.135250       1 shared_informer.go:262] Caches are synced for persistent volume
I0902 15:06:37.260516       1 trace.go:205] Trace[636149005]: "Reflector ListAndWatch" name:vendor/k8s.io/client-go/informers/factory.go:134 (02-Sep-2022 15:06:22.687) (total time: 14572ms):
Trace[636149005]: ---"Objects listed" error:<nil> 14572ms (15:06:37.260)
Trace[636149005]: [14.572533216s] [14.572533216s] END
I0902 15:06:37.262197       1 shared_informer.go:262] Caches are synced for ReplicationController
I0902 15:06:37.262239       1 shared_informer.go:262] Caches are synced for endpoint_slice
I0902 15:06:37.290640       1 shared_informer.go:262] Caches are synced for attach detach
I0902 15:06:37.744579       1 shared_informer.go:262] Caches are synced for daemon sets
I0902 15:06:37.745251       1 shared_informer.go:262] Caches are synced for job
I0902 15:06:37.745435       1 shared_informer.go:262] Caches are synced for stateful set
I0902 15:06:37.745601       1 shared_informer.go:262] Caches are synced for ephemeral
I0902 15:06:37.832407       1 taint_manager.go:187] "Starting NoExecuteTaintManager"
W0902 15:06:38.276603       1 node_lifecycle_controller.go:1014] Missing timestamp for Node minikube. Assuming now as a timestamp.
I0902 15:06:39.208791       1 node_lifecycle_controller.go:1215] Controller detected that zone  is now in state Normal.
I0902 15:06:39.209902       1 shared_informer.go:262] Caches are synced for namespace
I0902 15:06:39.210279       1 event.go:294] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube event: Registered Node minikube in Controller"
I0902 15:06:41.047289       1 shared_informer.go:262] Caches are synced for endpoint
I0902 15:06:41.047595       1 shared_informer.go:262] Caches are synced for service account
I0902 15:06:41.048372       1 shared_informer.go:262] Caches are synced for disruption
I0902 15:06:41.048426       1 disruption.go:371] Sending events to api server.
I0902 15:06:41.048746       1 shared_informer.go:262] Caches are synced for endpoint_slice_mirroring
I0902 15:06:41.051908       1 shared_informer.go:262] Caches are synced for ReplicaSet
I0902 15:06:41.390522       1 shared_informer.go:262] Caches are synced for deployment
I0902 15:06:43.266518       1 shared_informer.go:262] Caches are synced for crt configmap
I0902 15:06:43.267770       1 shared_informer.go:262] Caches are synced for resource quota
I0902 15:06:43.268321       1 shared_informer.go:262] Caches are synced for bootstrap_signer
I0902 15:06:44.025285       1 shared_informer.go:262] Caches are synced for resource quota
I0902 15:06:45.543690       1 shared_informer.go:255] Waiting for caches to sync for garbage collector
I0902 15:06:48.803576       1 shared_informer.go:262] Caches are synced for garbage collector
I0902 15:06:48.803707       1 garbagecollector.go:158] Garbage collector: all resource monitors have synced. Proceeding to collect garbage
I0902 15:06:48.844731       1 shared_informer.go:262] Caches are synced for garbage collector

* 
* ==> kube-controller-manager [b4cb9c339fb6] <==
* internal/poll.(*FD).Accept(0xc0001f6500)
	/usr/local/go/src/internal/poll/fd_unix.go:614 +0x22c
net.(*netFD).accept(0xc0001f6500)
	/usr/local/go/src/net/fd_unix.go:172 +0x35
net.(*TCPListener).accept(0xc00000d278)
	/usr/local/go/src/net/tcpsock_posix.go:139 +0x28
net.(*TCPListener).Accept(0xc00000d278)
	/usr/local/go/src/net/tcpsock.go:288 +0x3d
k8s.io/kubernetes/vendor/k8s.io/apiserver/pkg/server.tcpKeepAliveListener.Accept({{0x4d20438?, 0xc00000d278?}})
	vendor/k8s.io/apiserver/pkg/server/secure_serving.go:275 +0x2b
crypto/tls.(*listener).Accept(0xc0004321f8)
	/usr/local/go/src/crypto/tls/tls.go:66 +0x2d
net/http.(*Server).Serve(0xc000176e00, {0x4d0ec78, 0xc0004321f8})
	/usr/local/go/src/net/http/server.go:3039 +0x385
k8s.io/kubernetes/vendor/k8s.io/apiserver/pkg/server.RunServer.func2()
	vendor/k8s.io/apiserver/pkg/server/secure_serving.go:250 +0x177
created by k8s.io/kubernetes/vendor/k8s.io/apiserver/pkg/server.RunServer
	vendor/k8s.io/apiserver/pkg/server/secure_serving.go:240 +0x18a

goroutine 196 [IO wait]:
internal/poll.runtime_pollWait(0x7f985dcacff8, 0x72)
	/usr/local/go/src/runtime/netpoll.go:302 +0x89
internal/poll.(*pollDesc).wait(0xc0007d9180?, 0xc000847500?, 0x0)
	/usr/local/go/src/internal/poll/fd_poll_runtime.go:83 +0x32
internal/poll.(*pollDesc).waitRead(...)
	/usr/local/go/src/internal/poll/fd_poll_runtime.go:88
internal/poll.(*FD).Read(0xc0007d9180, {0xc000847500, 0x931, 0x931})
	/usr/local/go/src/internal/poll/fd_unix.go:167 +0x25a
net.(*netFD).Read(0xc0007d9180, {0xc000847500?, 0xc00044c3c0?, 0xc0008475df?})
	/usr/local/go/src/net/fd_posix.go:55 +0x29
net.(*conn).Read(0xc00068c048, {0xc000847500?, 0xc000adc810?, 0x417a73?})
	/usr/local/go/src/net/net.go:183 +0x45
crypto/tls.(*atLeastReader).Read(0xc0004333e0, {0xc000847500?, 0x0?, 0x34?})
	/usr/local/go/src/crypto/tls/conn.go:785 +0x3d
bytes.(*Buffer).ReadFrom(0xc000738278, {0x4cf6c20, 0xc0004333e0})
	/usr/local/go/src/bytes/buffer.go:204 +0x98
crypto/tls.(*Conn).readFromUntil(0xc000738000, {0x4cff140?, 0xc00068c048}, 0x857?)
	/usr/local/go/src/crypto/tls/conn.go:807 +0xe5
crypto/tls.(*Conn).readRecordOrCCS(0xc000738000, 0x0)
	/usr/local/go/src/crypto/tls/conn.go:614 +0x116
crypto/tls.(*Conn).readRecord(...)
	/usr/local/go/src/crypto/tls/conn.go:582
crypto/tls.(*Conn).Read(0xc000738000, {0xc00003c000, 0x1000, 0x9197e0?})
	/usr/local/go/src/crypto/tls/conn.go:1285 +0x16f
bufio.(*Reader).Read(0xc00038f020, {0xc000e262e0, 0x9, 0x9361a2?})
	/usr/local/go/src/bufio/bufio.go:236 +0x1b4
io.ReadAtLeast({0x4cf6a40, 0xc00038f020}, {0xc000e262e0, 0x9, 0x9}, 0x9)
	/usr/local/go/src/io/io.go:331 +0x9a
io.ReadFull(...)
	/usr/local/go/src/io/io.go:350
k8s.io/kubernetes/vendor/golang.org/x/net/http2.readFrameHeader({0xc000e262e0?, 0x9?, 0xc001acda40?}, {0x4cf6a40?, 0xc00038f020?})
	vendor/golang.org/x/net/http2/frame.go:237 +0x6e
k8s.io/kubernetes/vendor/golang.org/x/net/http2.(*Framer).ReadFrame(0xc000e262a0)
	vendor/golang.org/x/net/http2/frame.go:498 +0x95
k8s.io/kubernetes/vendor/golang.org/x/net/http2.(*clientConnReadLoop).run(0xc000adcf98)
	vendor/golang.org/x/net/http2/transport.go:2101 +0x130
k8s.io/kubernetes/vendor/golang.org/x/net/http2.(*ClientConn).readLoop(0xc00049e300)
	vendor/golang.org/x/net/http2/transport.go:1997 +0x6f
created by k8s.io/kubernetes/vendor/golang.org/x/net/http2.(*Transport).newClientConn
	vendor/golang.org/x/net/http2/transport.go:725 +0xa65

* 
* ==> kube-proxy [818e5a214c30] <==
* I0902 15:07:48.452117       1 node.go:163] Successfully retrieved node IP: 192.168.49.2
I0902 15:07:48.453790       1 server_others.go:138] "Detected node IP" address="192.168.49.2"
I0902 15:07:48.455010       1 server_others.go:578] "Unknown proxy mode, assuming iptables proxy" proxyMode=""
I0902 15:07:58.857669       1 server_others.go:206] "Using iptables Proxier"
I0902 15:07:58.857885       1 server_others.go:213] "kube-proxy running in dual-stack mode" ipFamily=IPv4
I0902 15:07:58.858033       1 server_others.go:214] "Creating dualStackProxier for iptables"
I0902 15:07:58.858207       1 server_others.go:501] "Detect-local-mode set to ClusterCIDR, but no IPv6 cluster CIDR defined, , defaulting to no-op detect-local for IPv6"
I0902 15:07:58.858281       1 proxier.go:259] "Setting route_localnet=1, use nodePortAddresses to filter loopback addresses for NodePorts to skip it https://issues.k8s.io/90259"
I0902 15:07:58.858857       1 proxier.go:259] "Setting route_localnet=1, use nodePortAddresses to filter loopback addresses for NodePorts to skip it https://issues.k8s.io/90259"
I0902 15:07:58.978226       1 server.go:661] "Version info" version="v1.24.3"
I0902 15:07:58.978274       1 server.go:663] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0902 15:07:59.415700       1 config.go:317] "Starting service config controller"
I0902 15:07:59.415809       1 shared_informer.go:255] Waiting for caches to sync for service config
I0902 15:07:59.416120       1 config.go:226] "Starting endpoint slice config controller"
I0902 15:07:59.417619       1 config.go:444] "Starting node config controller"
I0902 15:07:59.417702       1 shared_informer.go:255] Waiting for caches to sync for node config
I0902 15:07:59.419638       1 shared_informer.go:255] Waiting for caches to sync for endpoint slice config
I0902 15:08:00.127890       1 shared_informer.go:262] Caches are synced for node config
I0902 15:08:00.261569       1 shared_informer.go:262] Caches are synced for endpoint slice config
I0902 15:08:00.261834       1 shared_informer.go:262] Caches are synced for service config

* 
* ==> kube-scheduler [505b95de9f6d] <==
* W0901 18:40:58.849095       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.ReplicationController: Get "https://192.168.49.2:8443/api/v1/replicationcontrollers?resourceVersion=25363": net/http: TLS handshake timeout
I0901 18:41:02.565938       1 trace.go:205] Trace[760741505]: "Reflector ListAndWatch" name:vendor/k8s.io/client-go/informers/factory.go:134 (01-Sep-2022 18:39:36.337) (total time: 86228ms):
Trace[760741505]: ---"Objects listed" error:Get "https://192.168.49.2:8443/api/v1/replicationcontrollers?resourceVersion=25363": net/http: TLS handshake timeout 82511ms (18:40:58.849)
Trace[760741505]: [1m26.228294462s] [1m26.228294462s] END
I0901 18:41:02.566295       1 trace.go:205] Trace[1271230428]: "Reflector ListAndWatch" name:vendor/k8s.io/client-go/informers/factory.go:134 (01-Sep-2022 18:39:30.850) (total time: 89030ms):
Trace[1271230428]: ---"Objects listed" error:Get "https://192.168.49.2:8443/api/v1/namespaces?resourceVersion=25367": net/http: TLS handshake timeout 86269ms (18:40:57.120)
Trace[1271230428]: [1m29.030327681s] [1m29.030327681s] END
W0901 18:40:59.196431       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.PodDisruptionBudget: Get "https://192.168.49.2:8443/apis/policy/v1/poddisruptionbudgets?resourceVersion=25350": net/http: TLS handshake timeout
I0901 18:41:02.823837       1 trace.go:205] Trace[66287716]: "Reflector ListAndWatch" name:vendor/k8s.io/client-go/informers/factory.go:134 (01-Sep-2022 18:38:53.819) (total time: 129004ms):
Trace[66287716]: ---"Objects listed" error:Get "https://192.168.49.2:8443/apis/policy/v1/poddisruptionbudgets?resourceVersion=25350": net/http: TLS handshake timeout 125377ms (18:40:59.196)
Trace[66287716]: [2m9.004755875s] [2m9.004755875s] END
W0901 18:41:00.271466       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.PersistentVolume: Get "https://192.168.49.2:8443/api/v1/persistentvolumes?resourceVersion=25363": net/http: TLS handshake timeout
I0901 18:41:02.824128       1 trace.go:205] Trace[528982190]: "Reflector ListAndWatch" name:vendor/k8s.io/client-go/informers/factory.go:134 (01-Sep-2022 18:39:30.630) (total time: 92193ms):
Trace[528982190]: ---"Objects listed" error:Get "https://192.168.49.2:8443/api/v1/persistentvolumes?resourceVersion=25363": net/http: TLS handshake timeout 84190ms (18:40:54.821)
Trace[528982190]: [1m32.193240162s] [1m32.193240162s] END
I0901 18:41:03.165421       1 trace.go:205] Trace[614572610]: "Reflector ListAndWatch" name:vendor/k8s.io/client-go/informers/factory.go:134 (01-Sep-2022 18:39:30.089) (total time: 89332ms):
Trace[614572610]: ---"Objects listed" error:Get "https://192.168.49.2:8443/api/v1/nodes?resourceVersion=25369": net/http: TLS handshake timeout 85679ms (18:40:55.769)
Trace[614572610]: [1m29.332953376s] [1m29.332953376s] END
I0901 18:41:03.165653       1 trace.go:205] Trace[1235363631]: "Reflector ListAndWatch" name:vendor/k8s.io/client-go/informers/factory.go:134 (01-Sep-2022 18:39:36.589) (total time: 83840ms):
Trace[1235363631]: ---"Objects listed" error:Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csidrivers?resourceVersion=25364": net/http: TLS handshake timeout 80531ms (18:40:57.120)
Trace[1235363631]: [1m23.840166549s] [1m23.840166549s] END
I0901 18:41:03.942129       1 trace.go:205] Trace[2108238709]: "Reflector ListAndWatch" name:vendor/k8s.io/client-go/informers/factory.go:134 (01-Sep-2022 18:39:02.058) (total time: 117364ms):
Trace[2108238709]: ---"Objects listed" error:Get "https://192.168.49.2:8443/api/v1/persistentvolumeclaims?resourceVersion=25364": net/http: TLS handshake timeout 117364ms (18:40:59.422)
Trace[2108238709]: [1m57.364516838s] [1m57.364516838s] END
I0901 18:41:05.705773       1 trace.go:205] Trace[831833274]: "Reflector ListAndWatch" name:vendor/k8s.io/client-go/informers/factory.go:134 (01-Sep-2022 18:39:30.660) (total time: 90318ms):
Trace[831833274]: ---"Objects listed" error:Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/storageclasses?resourceVersion=25359": net/http: TLS handshake timeout 84654ms (18:40:55.315)
Trace[831833274]: [1m30.31877101s] [1m30.31877101s] END
E0901 18:41:05.706050       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/storageclasses?resourceVersion=25359": net/http: TLS handshake timeout
I0901 18:41:05.706493       1 trace.go:205] Trace[1197598206]: "Reflector ListAndWatch" name:vendor/k8s.io/client-go/informers/factory.go:134 (01-Sep-2022 18:39:37.066) (total time: 82438ms):
Trace[1197598206]: ---"Objects listed" error:Get "https://192.168.49.2:8443/api/v1/services?resourceVersion=25362": net/http: TLS handshake timeout 79245ms (18:40:56.311)
Trace[1197598206]: [1m22.438243572s] [1m22.438243572s] END
E0901 18:41:05.706754       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://192.168.49.2:8443/api/v1/services?resourceVersion=25362": net/http: TLS handshake timeout
I0901 18:41:05.706973       1 trace.go:205] Trace[736418181]: "Reflector ListAndWatch" name:vendor/k8s.io/client-go/informers/factory.go:134 (01-Sep-2022 18:39:25.135) (total time: 94369ms):
Trace[736418181]: ---"Objects listed" error:Get "https://192.168.49.2:8443/api/v1/pods?fieldSelector=status.phase%3DSucceeded%!C(MISSING)status.phase%3DFailed&resourceVersion=25370": net/http: TLS handshake timeout 94369ms (18:40:59.504)
Trace[736418181]: [1m34.369456174s] [1m34.369456174s] END
E0901 18:41:05.707065       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Pod: failed to list *v1.Pod: Get "https://192.168.49.2:8443/api/v1/pods?fieldSelector=status.phase%3DSucceeded%!C(MISSING)status.phase%3DFailed&resourceVersion=25370": net/http: TLS handshake timeout
I0901 18:41:05.973859       1 trace.go:205] Trace[424406126]: "Reflector ListAndWatch" name:vendor/k8s.io/client-go/informers/factory.go:134 (01-Sep-2022 18:39:19.785) (total time: 101721ms):
Trace[424406126]: ---"Objects listed" error:Get "https://192.168.49.2:8443/apis/apps/v1/replicasets?resourceVersion=25331": net/http: TLS handshake timeout 97510ms (18:40:57.296)
Trace[424406126]: [1m41.721924593s] [1m41.721924593s] END
E0901 18:41:05.974057       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: Get "https://192.168.49.2:8443/apis/apps/v1/replicasets?resourceVersion=25331": net/http: TLS handshake timeout
E0901 18:41:06.078706       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: Get "https://192.168.49.2:8443/apis/policy/v1/poddisruptionbudgets?resourceVersion=25350": net/http: TLS handshake timeout
E0901 18:41:06.078890       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: Get "https://192.168.49.2:8443/api/v1/persistentvolumes?resourceVersion=25363": net/http: TLS handshake timeout
E0901 18:41:06.079122       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Node: failed to list *v1.Node: Get "https://192.168.49.2:8443/api/v1/nodes?resourceVersion=25369": net/http: TLS handshake timeout
E0901 18:41:06.079438       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csidrivers?resourceVersion=25364": net/http: TLS handshake timeout
E0901 18:41:06.079707       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: Get "https://192.168.49.2:8443/api/v1/persistentvolumeclaims?resourceVersion=25364": net/http: TLS handshake timeout
I0901 18:41:03.942506       1 trace.go:205] Trace[633294531]: "Reflector ListAndWatch" name:vendor/k8s.io/client-go/informers/factory.go:134 (01-Sep-2022 18:39:41.062) (total time: 79769ms):
Trace[633294531]: ---"Objects listed" error:Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?resourceVersion=25375": net/http: TLS handshake timeout 73759ms (18:40:54.821)
Trace[633294531]: [1m19.769593879s] [1m19.769593879s] END
E0901 18:41:06.080003       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?resourceVersion=25375": net/http: TLS handshake timeout
I0901 18:41:06.455885       1 trace.go:205] Trace[546204769]: "Reflector ListAndWatch" name:vendor/k8s.io/client-go/informers/factory.go:134 (01-Sep-2022 18:39:29.581) (total time: 92213ms):
Trace[546204769]: ---"Objects listed" error:Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csinodes?resourceVersion=25350": net/http: TLS handshake timeout 88241ms (18:40:57.823)
Trace[546204769]: [1m32.21336053s] [1m32.21336053s] END
E0901 18:41:06.455956       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSINode: failed to list *v1.CSINode: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csinodes?resourceVersion=25350": net/http: TLS handshake timeout
I0901 18:41:06.455980       1 trace.go:205] Trace[1843136947]: "Reflector ListAndWatch" name:vendor/k8s.io/client-go/informers/factory.go:134 (01-Sep-2022 18:39:28.699) (total time: 93575ms):
Trace[1843136947]: ---"Objects listed" error:Get "https://192.168.49.2:8443/apis/apps/v1/statefulsets?resourceVersion=25363": net/http: TLS handshake timeout 91182ms (18:40:59.881)
Trace[1843136947]: [1m33.575524084s] [1m33.575524084s] END
E0901 18:41:06.456002       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: Get "https://192.168.49.2:8443/apis/apps/v1/statefulsets?resourceVersion=25363": net/http: TLS handshake timeout
E0901 18:41:06.456024       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: Get "https://192.168.49.2:8443/api/v1/replicationcontrollers?resourceVersion=25363": net/http: TLS handshake timeout
E0901 18:41:06.456046       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://192.168.49.2:8443/api/v1/namespaces?resourceVersion=25367": net/http: TLS handshake timeout
I0901 18:42:00.873202       1 request.go:601] Waited for 1.067101018s due to client-side throttling, not priority and fairness, request: GET:https://192.168.49.2:8443/api/v1/replicationcontrollers?resourceVersion=25363

* 
* ==> kube-scheduler [eaeba0744d77] <==
* E0902 15:02:58.552181       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://192.168.49.2:8443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0902 15:02:59.985066       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.CSINode: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0902 15:02:59.986599       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSINode: failed to list *v1.CSINode: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0902 15:03:00.032953       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Pod: Get "https://192.168.49.2:8443/api/v1/pods?fieldSelector=status.phase%3DSucceeded%!C(MISSING)status.phase%3DFailed&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0902 15:03:00.033503       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Pod: failed to list *v1.Pod: Get "https://192.168.49.2:8443/api/v1/pods?fieldSelector=status.phase%3DSucceeded%!C(MISSING)status.phase%3DFailed&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0902 15:03:00.059260       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.ReplicaSet: Get "https://192.168.49.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0902 15:03:00.059715       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: Get "https://192.168.49.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0902 15:03:00.886905       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.CSIDriver: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0902 15:03:00.887103       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0902 15:03:01.393655       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.StorageClass: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0902 15:03:01.393815       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0902 15:03:02.088453       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Node: Get "https://192.168.49.2:8443/api/v1/nodes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0902 15:03:02.088676       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Node: failed to list *v1.Node: Get "https://192.168.49.2:8443/api/v1/nodes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0902 15:03:04.031218       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.PodDisruptionBudget: Get "https://192.168.49.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0902 15:03:04.031316       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: Get "https://192.168.49.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0902 15:03:07.114090       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.ReplicationController: Get "https://192.168.49.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0902 15:03:07.114268       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: Get "https://192.168.49.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0902 15:03:35.562439       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Service: Get "https://192.168.49.2:8443/api/v1/services?limit=500&resourceVersion=0": net/http: TLS handshake timeout
I0902 15:03:35.987077       1 trace.go:205] Trace[722753187]: "Reflector ListAndWatch" name:vendor/k8s.io/client-go/informers/factory.go:134 (02-Sep-2022 15:03:25.561) (total time: 10001ms):
Trace[722753187]: ---"Objects listed" error:Get "https://192.168.49.2:8443/api/v1/services?limit=500&resourceVersion=0": net/http: TLS handshake timeout 10001ms (15:03:35.562)
Trace[722753187]: [10.001642583s] [10.001642583s] END
E0902 15:03:35.987374       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://192.168.49.2:8443/api/v1/services?limit=500&resourceVersion=0": net/http: TLS handshake timeout
W0902 15:03:36.024308       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.PersistentVolumeClaim: Get "https://192.168.49.2:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0": net/http: TLS handshake timeout
I0902 15:03:36.024564       1 trace.go:205] Trace[390786308]: "Reflector ListAndWatch" name:vendor/k8s.io/client-go/informers/factory.go:134 (02-Sep-2022 15:03:26.022) (total time: 10001ms):
Trace[390786308]: ---"Objects listed" error:Get "https://192.168.49.2:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0": net/http: TLS handshake timeout 10001ms (15:03:36.024)
Trace[390786308]: [10.001711428s] [10.001711428s] END
E0902 15:03:36.024597       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: Get "https://192.168.49.2:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0": net/http: TLS handshake timeout
W0902 15:03:40.988162       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Pod: Get "https://192.168.49.2:8443/api/v1/pods?fieldSelector=status.phase%3DSucceeded%!C(MISSING)status.phase%3DFailed&limit=500&resourceVersion=0": net/http: TLS handshake timeout
I0902 15:03:40.988463       1 trace.go:205] Trace[1525464793]: "Reflector ListAndWatch" name:vendor/k8s.io/client-go/informers/factory.go:134 (02-Sep-2022 15:03:30.987) (total time: 10001ms):
Trace[1525464793]: ---"Objects listed" error:Get "https://192.168.49.2:8443/api/v1/pods?fieldSelector=status.phase%3DSucceeded%!C(MISSING)status.phase%3DFailed&limit=500&resourceVersion=0": net/http: TLS handshake timeout 10001ms (15:03:40.988)
Trace[1525464793]: [10.001188203s] [10.001188203s] END
E0902 15:03:40.988676       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Pod: failed to list *v1.Pod: Get "https://192.168.49.2:8443/api/v1/pods?fieldSelector=status.phase%3DSucceeded%!C(MISSING)status.phase%3DFailed&limit=500&resourceVersion=0": net/http: TLS handshake timeout
W0902 15:03:41.494881       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.ReplicaSet: Get "https://192.168.49.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0": net/http: TLS handshake timeout
I0902 15:03:41.495059       1 trace.go:205] Trace[487688207]: "Reflector ListAndWatch" name:vendor/k8s.io/client-go/informers/factory.go:134 (02-Sep-2022 15:03:31.492) (total time: 10002ms):
Trace[487688207]: ---"Objects listed" error:Get "https://192.168.49.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0": net/http: TLS handshake timeout 10001ms (15:03:41.494)
Trace[487688207]: [10.002115924s] [10.002115924s] END
E0902 15:03:41.495128       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: Get "https://192.168.49.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0": net/http: TLS handshake timeout
W0902 15:03:42.363160       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.CSIStorageCapacity: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0": net/http: TLS handshake timeout
I0902 15:03:42.363335       1 trace.go:205] Trace[1364599878]: "Reflector ListAndWatch" name:vendor/k8s.io/client-go/informers/factory.go:134 (02-Sep-2022 15:03:32.361) (total time: 10001ms):
Trace[1364599878]: ---"Objects listed" error:Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0": net/http: TLS handshake timeout 10001ms (15:03:42.363)
Trace[1364599878]: [10.001594025s] [10.001594025s] END
E0902 15:03:42.363369       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0": net/http: TLS handshake timeout
W0902 15:03:42.854624       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.ReplicationController: Get "https://192.168.49.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0": net/http: TLS handshake timeout
I0902 15:03:42.854801       1 trace.go:205] Trace[668813323]: "Reflector ListAndWatch" name:vendor/k8s.io/client-go/informers/factory.go:134 (02-Sep-2022 15:03:32.852) (total time: 10002ms):
Trace[668813323]: ---"Objects listed" error:Get "https://192.168.49.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0": net/http: TLS handshake timeout 10001ms (15:03:42.854)
Trace[668813323]: [10.002099177s] [10.002099177s] END
E0902 15:03:42.854895       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: Get "https://192.168.49.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0": net/http: TLS handshake timeout
W0902 15:03:48.219503       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0902 15:03:48.219619       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W0902 15:03:48.219698       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0902 15:03:48.219732       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W0902 15:03:48.221543       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0902 15:03:48.221616       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W0902 15:03:48.221852       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0902 15:03:48.221885       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W0902 15:03:48.222078       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0902 15:03:48.222212       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W0902 15:03:48.222666       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0902 15:03:48.222700       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
I0902 15:03:50.171299       1 shared_informer.go:262] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file

* 
* ==> kubelet <==
* -- Logs begin at Fri 2022-09-02 14:53:37 UTC, end at Fri 2022-09-02 15:33:32 UTC. --
Sep 02 15:29:50 minikube kubelet[1033]: I0902 15:29:50.605980    1033 scope.go:110] "RemoveContainer" containerID="fecc50ef0f898a1047e3194923a9f903aebca9adfe1cc921f4e66b77d8464317"
Sep 02 15:29:50 minikube kubelet[1033]: E0902 15:29:50.606581    1033 pod_workers.go:951] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"test-img\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=test-img pod=test-dep-99b7588fb-zd7g4_default(fab52af1-8d9a-49ad-a766-36f20674d159)\"" pod="default/test-dep-99b7588fb-zd7g4" podUID=fab52af1-8d9a-49ad-a766-36f20674d159
Sep 02 15:30:05 minikube kubelet[1033]: I0902 15:30:05.601384    1033 scope.go:110] "RemoveContainer" containerID="fecc50ef0f898a1047e3194923a9f903aebca9adfe1cc921f4e66b77d8464317"
Sep 02 15:30:05 minikube kubelet[1033]: E0902 15:30:05.603051    1033 pod_workers.go:951] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"test-img\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=test-img pod=test-dep-99b7588fb-zd7g4_default(fab52af1-8d9a-49ad-a766-36f20674d159)\"" pod="default/test-dep-99b7588fb-zd7g4" podUID=fab52af1-8d9a-49ad-a766-36f20674d159
Sep 02 15:30:17 minikube kubelet[1033]: I0902 15:30:17.601120    1033 scope.go:110] "RemoveContainer" containerID="fecc50ef0f898a1047e3194923a9f903aebca9adfe1cc921f4e66b77d8464317"
Sep 02 15:30:17 minikube kubelet[1033]: E0902 15:30:17.601541    1033 pod_workers.go:951] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"test-img\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=test-img pod=test-dep-99b7588fb-zd7g4_default(fab52af1-8d9a-49ad-a766-36f20674d159)\"" pod="default/test-dep-99b7588fb-zd7g4" podUID=fab52af1-8d9a-49ad-a766-36f20674d159
Sep 02 15:30:31 minikube kubelet[1033]: I0902 15:30:31.601595    1033 scope.go:110] "RemoveContainer" containerID="fecc50ef0f898a1047e3194923a9f903aebca9adfe1cc921f4e66b77d8464317"
Sep 02 15:30:31 minikube kubelet[1033]: E0902 15:30:31.609035    1033 pod_workers.go:951] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"test-img\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=test-img pod=test-dep-99b7588fb-zd7g4_default(fab52af1-8d9a-49ad-a766-36f20674d159)\"" pod="default/test-dep-99b7588fb-zd7g4" podUID=fab52af1-8d9a-49ad-a766-36f20674d159
Sep 02 15:30:46 minikube kubelet[1033]: I0902 15:30:46.603403    1033 scope.go:110] "RemoveContainer" containerID="fecc50ef0f898a1047e3194923a9f903aebca9adfe1cc921f4e66b77d8464317"
Sep 02 15:30:46 minikube kubelet[1033]: E0902 15:30:46.610068    1033 pod_workers.go:951] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"test-img\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=test-img pod=test-dep-99b7588fb-zd7g4_default(fab52af1-8d9a-49ad-a766-36f20674d159)\"" pod="default/test-dep-99b7588fb-zd7g4" podUID=fab52af1-8d9a-49ad-a766-36f20674d159
Sep 02 15:30:59 minikube kubelet[1033]: I0902 15:30:59.603545    1033 scope.go:110] "RemoveContainer" containerID="fecc50ef0f898a1047e3194923a9f903aebca9adfe1cc921f4e66b77d8464317"
Sep 02 15:30:59 minikube kubelet[1033]: E0902 15:30:59.605536    1033 pod_workers.go:951] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"test-img\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=test-img pod=test-dep-99b7588fb-zd7g4_default(fab52af1-8d9a-49ad-a766-36f20674d159)\"" pod="default/test-dep-99b7588fb-zd7g4" podUID=fab52af1-8d9a-49ad-a766-36f20674d159
Sep 02 15:31:09 minikube kubelet[1033]: I0902 15:31:09.059340    1033 scope.go:110] "RemoveContainer" containerID="274dd494f6d5da8f5b13a01025ab6290d6d129538537e3dd596d428fad08af91"
Sep 02 15:31:09 minikube kubelet[1033]: I0902 15:31:09.060689    1033 scope.go:110] "RemoveContainer" containerID="e661566b00ce8082c68e7f4be8bb1139624e88e6c14ee8efc99c267607e54d12"
Sep 02 15:31:09 minikube kubelet[1033]: E0902 15:31:09.061808    1033 pod_workers.go:951] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"test-img\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=test-img pod=test-dep1-56bdf49f98-748td_default(dad109ac-8182-484c-b4fe-b49232677704)\"" pod="default/test-dep1-56bdf49f98-748td" podUID=dad109ac-8182-484c-b4fe-b49232677704
Sep 02 15:31:12 minikube kubelet[1033]: I0902 15:31:12.165656    1033 pod_container_deletor.go:79] "Container not found in pod's containers" containerID="274dd494f6d5da8f5b13a01025ab6290d6d129538537e3dd596d428fad08af91"
Sep 02 15:31:12 minikube kubelet[1033]: I0902 15:31:12.166199    1033 scope.go:110] "RemoveContainer" containerID="e661566b00ce8082c68e7f4be8bb1139624e88e6c14ee8efc99c267607e54d12"
Sep 02 15:31:12 minikube kubelet[1033]: E0902 15:31:12.166625    1033 pod_workers.go:951] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"test-img\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=test-img pod=test-dep1-56bdf49f98-748td_default(dad109ac-8182-484c-b4fe-b49232677704)\"" pod="default/test-dep1-56bdf49f98-748td" podUID=dad109ac-8182-484c-b4fe-b49232677704
Sep 02 15:31:12 minikube kubelet[1033]: I0902 15:31:12.601429    1033 scope.go:110] "RemoveContainer" containerID="fecc50ef0f898a1047e3194923a9f903aebca9adfe1cc921f4e66b77d8464317"
Sep 02 15:31:12 minikube kubelet[1033]: E0902 15:31:12.602043    1033 pod_workers.go:951] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"test-img\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=test-img pod=test-dep-99b7588fb-zd7g4_default(fab52af1-8d9a-49ad-a766-36f20674d159)\"" pod="default/test-dep-99b7588fb-zd7g4" podUID=fab52af1-8d9a-49ad-a766-36f20674d159
Sep 02 15:31:24 minikube kubelet[1033]: I0902 15:31:24.602653    1033 scope.go:110] "RemoveContainer" containerID="e661566b00ce8082c68e7f4be8bb1139624e88e6c14ee8efc99c267607e54d12"
Sep 02 15:31:24 minikube kubelet[1033]: E0902 15:31:24.603110    1033 pod_workers.go:951] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"test-img\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=test-img pod=test-dep1-56bdf49f98-748td_default(dad109ac-8182-484c-b4fe-b49232677704)\"" pod="default/test-dep1-56bdf49f98-748td" podUID=dad109ac-8182-484c-b4fe-b49232677704
Sep 02 15:31:27 minikube kubelet[1033]: I0902 15:31:27.602879    1033 scope.go:110] "RemoveContainer" containerID="fecc50ef0f898a1047e3194923a9f903aebca9adfe1cc921f4e66b77d8464317"
Sep 02 15:31:27 minikube kubelet[1033]: E0902 15:31:27.604162    1033 pod_workers.go:951] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"test-img\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=test-img pod=test-dep-99b7588fb-zd7g4_default(fab52af1-8d9a-49ad-a766-36f20674d159)\"" pod="default/test-dep-99b7588fb-zd7g4" podUID=fab52af1-8d9a-49ad-a766-36f20674d159
Sep 02 15:31:38 minikube kubelet[1033]: I0902 15:31:38.602013    1033 scope.go:110] "RemoveContainer" containerID="e661566b00ce8082c68e7f4be8bb1139624e88e6c14ee8efc99c267607e54d12"
Sep 02 15:31:38 minikube kubelet[1033]: E0902 15:31:38.609114    1033 pod_workers.go:951] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"test-img\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=test-img pod=test-dep1-56bdf49f98-748td_default(dad109ac-8182-484c-b4fe-b49232677704)\"" pod="default/test-dep1-56bdf49f98-748td" podUID=dad109ac-8182-484c-b4fe-b49232677704
Sep 02 15:31:40 minikube kubelet[1033]: I0902 15:31:40.601645    1033 scope.go:110] "RemoveContainer" containerID="fecc50ef0f898a1047e3194923a9f903aebca9adfe1cc921f4e66b77d8464317"
Sep 02 15:31:40 minikube kubelet[1033]: E0902 15:31:40.602143    1033 pod_workers.go:951] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"test-img\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=test-img pod=test-dep-99b7588fb-zd7g4_default(fab52af1-8d9a-49ad-a766-36f20674d159)\"" pod="default/test-dep-99b7588fb-zd7g4" podUID=fab52af1-8d9a-49ad-a766-36f20674d159
Sep 02 15:31:50 minikube kubelet[1033]: I0902 15:31:50.602282    1033 scope.go:110] "RemoveContainer" containerID="e661566b00ce8082c68e7f4be8bb1139624e88e6c14ee8efc99c267607e54d12"
Sep 02 15:31:50 minikube kubelet[1033]: E0902 15:31:50.605067    1033 pod_workers.go:951] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"test-img\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=test-img pod=test-dep1-56bdf49f98-748td_default(dad109ac-8182-484c-b4fe-b49232677704)\"" pod="default/test-dep1-56bdf49f98-748td" podUID=dad109ac-8182-484c-b4fe-b49232677704
Sep 02 15:31:54 minikube kubelet[1033]: I0902 15:31:54.606461    1033 scope.go:110] "RemoveContainer" containerID="fecc50ef0f898a1047e3194923a9f903aebca9adfe1cc921f4e66b77d8464317"
Sep 02 15:31:54 minikube kubelet[1033]: E0902 15:31:54.606935    1033 pod_workers.go:951] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"test-img\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=test-img pod=test-dep-99b7588fb-zd7g4_default(fab52af1-8d9a-49ad-a766-36f20674d159)\"" pod="default/test-dep-99b7588fb-zd7g4" podUID=fab52af1-8d9a-49ad-a766-36f20674d159
Sep 02 15:32:04 minikube kubelet[1033]: I0902 15:32:04.606039    1033 scope.go:110] "RemoveContainer" containerID="e661566b00ce8082c68e7f4be8bb1139624e88e6c14ee8efc99c267607e54d12"
Sep 02 15:32:04 minikube kubelet[1033]: E0902 15:32:04.606498    1033 pod_workers.go:951] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"test-img\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=test-img pod=test-dep1-56bdf49f98-748td_default(dad109ac-8182-484c-b4fe-b49232677704)\"" pod="default/test-dep1-56bdf49f98-748td" podUID=dad109ac-8182-484c-b4fe-b49232677704
Sep 02 15:32:06 minikube kubelet[1033]: I0902 15:32:06.605798    1033 scope.go:110] "RemoveContainer" containerID="fecc50ef0f898a1047e3194923a9f903aebca9adfe1cc921f4e66b77d8464317"
Sep 02 15:32:06 minikube kubelet[1033]: E0902 15:32:06.607469    1033 pod_workers.go:951] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"test-img\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=test-img pod=test-dep-99b7588fb-zd7g4_default(fab52af1-8d9a-49ad-a766-36f20674d159)\"" pod="default/test-dep-99b7588fb-zd7g4" podUID=fab52af1-8d9a-49ad-a766-36f20674d159
Sep 02 15:32:16 minikube kubelet[1033]: I0902 15:32:16.602214    1033 scope.go:110] "RemoveContainer" containerID="e661566b00ce8082c68e7f4be8bb1139624e88e6c14ee8efc99c267607e54d12"
Sep 02 15:32:16 minikube kubelet[1033]: E0902 15:32:16.603111    1033 pod_workers.go:951] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"test-img\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=test-img pod=test-dep1-56bdf49f98-748td_default(dad109ac-8182-484c-b4fe-b49232677704)\"" pod="default/test-dep1-56bdf49f98-748td" podUID=dad109ac-8182-484c-b4fe-b49232677704
Sep 02 15:32:20 minikube kubelet[1033]: I0902 15:32:20.603205    1033 scope.go:110] "RemoveContainer" containerID="fecc50ef0f898a1047e3194923a9f903aebca9adfe1cc921f4e66b77d8464317"
Sep 02 15:32:20 minikube kubelet[1033]: E0902 15:32:20.603753    1033 pod_workers.go:951] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"test-img\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=test-img pod=test-dep-99b7588fb-zd7g4_default(fab52af1-8d9a-49ad-a766-36f20674d159)\"" pod="default/test-dep-99b7588fb-zd7g4" podUID=fab52af1-8d9a-49ad-a766-36f20674d159
Sep 02 15:32:31 minikube kubelet[1033]: I0902 15:32:31.692115    1033 scope.go:110] "RemoveContainer" containerID="e661566b00ce8082c68e7f4be8bb1139624e88e6c14ee8efc99c267607e54d12"
Sep 02 15:32:31 minikube kubelet[1033]: E0902 15:32:31.694584    1033 pod_workers.go:951] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"test-img\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=test-img pod=test-dep1-56bdf49f98-748td_default(dad109ac-8182-484c-b4fe-b49232677704)\"" pod="default/test-dep1-56bdf49f98-748td" podUID=dad109ac-8182-484c-b4fe-b49232677704
Sep 02 15:32:35 minikube kubelet[1033]: I0902 15:32:35.745949    1033 scope.go:110] "RemoveContainer" containerID="fecc50ef0f898a1047e3194923a9f903aebca9adfe1cc921f4e66b77d8464317"
Sep 02 15:32:35 minikube kubelet[1033]: E0902 15:32:35.747075    1033 pod_workers.go:951] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"test-img\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=test-img pod=test-dep-99b7588fb-zd7g4_default(fab52af1-8d9a-49ad-a766-36f20674d159)\"" pod="default/test-dep-99b7588fb-zd7g4" podUID=fab52af1-8d9a-49ad-a766-36f20674d159
Sep 02 15:32:44 minikube kubelet[1033]: I0902 15:32:44.228653    1033 scope.go:110] "RemoveContainer" containerID="e661566b00ce8082c68e7f4be8bb1139624e88e6c14ee8efc99c267607e54d12"
Sep 02 15:32:44 minikube kubelet[1033]: E0902 15:32:44.231265    1033 pod_workers.go:951] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"test-img\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=test-img pod=test-dep1-56bdf49f98-748td_default(dad109ac-8182-484c-b4fe-b49232677704)\"" pod="default/test-dep1-56bdf49f98-748td" podUID=dad109ac-8182-484c-b4fe-b49232677704
Sep 02 15:32:46 minikube kubelet[1033]: I0902 15:32:46.600770    1033 scope.go:110] "RemoveContainer" containerID="fecc50ef0f898a1047e3194923a9f903aebca9adfe1cc921f4e66b77d8464317"
Sep 02 15:32:46 minikube kubelet[1033]: E0902 15:32:46.602380    1033 pod_workers.go:951] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"test-img\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=test-img pod=test-dep-99b7588fb-zd7g4_default(fab52af1-8d9a-49ad-a766-36f20674d159)\"" pod="default/test-dep-99b7588fb-zd7g4" podUID=fab52af1-8d9a-49ad-a766-36f20674d159
Sep 02 15:32:57 minikube kubelet[1033]: I0902 15:32:57.601819    1033 scope.go:110] "RemoveContainer" containerID="e661566b00ce8082c68e7f4be8bb1139624e88e6c14ee8efc99c267607e54d12"
Sep 02 15:32:57 minikube kubelet[1033]: E0902 15:32:57.603773    1033 pod_workers.go:951] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"test-img\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=test-img pod=test-dep1-56bdf49f98-748td_default(dad109ac-8182-484c-b4fe-b49232677704)\"" pod="default/test-dep1-56bdf49f98-748td" podUID=dad109ac-8182-484c-b4fe-b49232677704
Sep 02 15:33:01 minikube kubelet[1033]: I0902 15:33:01.602569    1033 scope.go:110] "RemoveContainer" containerID="fecc50ef0f898a1047e3194923a9f903aebca9adfe1cc921f4e66b77d8464317"
Sep 02 15:33:01 minikube kubelet[1033]: E0902 15:33:01.608517    1033 pod_workers.go:951] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"test-img\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=test-img pod=test-dep-99b7588fb-zd7g4_default(fab52af1-8d9a-49ad-a766-36f20674d159)\"" pod="default/test-dep-99b7588fb-zd7g4" podUID=fab52af1-8d9a-49ad-a766-36f20674d159
Sep 02 15:33:11 minikube kubelet[1033]: I0902 15:33:11.601402    1033 scope.go:110] "RemoveContainer" containerID="e661566b00ce8082c68e7f4be8bb1139624e88e6c14ee8efc99c267607e54d12"
Sep 02 15:33:11 minikube kubelet[1033]: E0902 15:33:11.601892    1033 pod_workers.go:951] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"test-img\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=test-img pod=test-dep1-56bdf49f98-748td_default(dad109ac-8182-484c-b4fe-b49232677704)\"" pod="default/test-dep1-56bdf49f98-748td" podUID=dad109ac-8182-484c-b4fe-b49232677704
Sep 02 15:33:15 minikube kubelet[1033]: I0902 15:33:15.600897    1033 scope.go:110] "RemoveContainer" containerID="fecc50ef0f898a1047e3194923a9f903aebca9adfe1cc921f4e66b77d8464317"
Sep 02 15:33:15 minikube kubelet[1033]: E0902 15:33:15.604280    1033 pod_workers.go:951] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"test-img\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=test-img pod=test-dep-99b7588fb-zd7g4_default(fab52af1-8d9a-49ad-a766-36f20674d159)\"" pod="default/test-dep-99b7588fb-zd7g4" podUID=fab52af1-8d9a-49ad-a766-36f20674d159
Sep 02 15:33:23 minikube kubelet[1033]: I0902 15:33:23.616433    1033 scope.go:110] "RemoveContainer" containerID="e661566b00ce8082c68e7f4be8bb1139624e88e6c14ee8efc99c267607e54d12"
Sep 02 15:33:23 minikube kubelet[1033]: E0902 15:33:23.625704    1033 pod_workers.go:951] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"test-img\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=test-img pod=test-dep1-56bdf49f98-748td_default(dad109ac-8182-484c-b4fe-b49232677704)\"" pod="default/test-dep1-56bdf49f98-748td" podUID=dad109ac-8182-484c-b4fe-b49232677704
Sep 02 15:33:29 minikube kubelet[1033]: I0902 15:33:29.602733    1033 scope.go:110] "RemoveContainer" containerID="fecc50ef0f898a1047e3194923a9f903aebca9adfe1cc921f4e66b77d8464317"
Sep 02 15:33:29 minikube kubelet[1033]: E0902 15:33:29.604763    1033 pod_workers.go:951] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"test-img\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=test-img pod=test-dep-99b7588fb-zd7g4_default(fab52af1-8d9a-49ad-a766-36f20674d159)\"" pod="default/test-dep-99b7588fb-zd7g4" podUID=fab52af1-8d9a-49ad-a766-36f20674d159

* 
* ==> storage-provisioner [263e629bbbe7] <==
* I0902 15:14:52.684974       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0902 15:14:54.061261       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0902 15:14:54.134076       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0902 15:15:17.087437       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0902 15:15:17.224725       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"667db358-5f05-4c74-aeb8-6146074928c1", APIVersion:"v1", ResourceVersion:"25952", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_c243781f-3d0c-4352-a075-9389673ab0f5 became leader
I0902 15:15:17.225524       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_c243781f-3d0c-4352-a075-9389673ab0f5!
I0902 15:15:17.890477       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_c243781f-3d0c-4352-a075-9389673ab0f5!
I0902 15:16:55.344262       1 request.go:655] Throttling request took 1.223370266s, request: GET:https://10.96.0.1:443/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath

